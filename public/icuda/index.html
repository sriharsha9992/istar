<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="author" content="Hongjian Li">
  <meta name="description" content="hands-on introduction to CUDA programming">
  <title>Hands-on Introduction to CUDA Programming</title>
  <link rel="stylesheet" href="/bootstrap.min.css">
  <link rel="stylesheet" href="/jquery.snippet.min.css">
  <link rel="stylesheet" href="/index.css">
  <link rel="stylesheet" href="index.css">
  <link rel="shortcut icon" href="/favicon.ico">
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="apple-touch-icon-precomposed" sizes="114x114" href="/apple-touch-icon-114-precomposed.png">
  <link rel="apple-touch-icon-precomposed" sizes="72x72" href="/apple-touch-icon-72-precomposed.png">
  <link rel="apple-touch-icon-precomposed" href="/apple-touch-icon-57-precomposed.png">
  <script>
    var _gaq=[['_setAccount','UA-20604862-1'],['_trackPageview']];
    (function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];
    g.src=('https:'==location.protocol?'//ssl':'//www')+'.google-analytics.com/ga.js';
    s.parentNode.insertBefore(g,s)}(document,'script'));
  </script>
</head>
<body>
  <a class="sr-only" href="#content">Skip navigation</a>
  <a href="//github.com/HongjianLi/istar" class="ribbon"></a>
  <header class="navbar navbar-inverse navbar-static-top" role="banner">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
      </div>
      <nav class="collapse navbar-collapse" role="navigation">
        <ul class="nav navbar-nav">
          <li>
            <a href="/"><img src="/logo.png" alt="istar logo">istar: software as a service</a>
          </li>
          <li>
            <a href="/idock"><img src="/idock/logo.png" alt="idock logo">idock: protein-ligand docking</a>
          </li>
          <li>
            <a href="/igrep"><img src="/igrep/logo.png" alt="igrep logo">igrep: DNA sequence matching</a>
          </li>
          <li>
            <a href="/iview"><img src="/iview/logo.png" alt="iview logo">iview: interactive WebGL visualizer</a>
          </li>
          <li class="active">
            <a href="/icuda"><img src="/icuda/logo.png" alt="icuda logo">icuda: introduction to CUDA</a>
          </li>
        </ul>
      </nav>
    </div>
  </header>
  <div class="jumbotron" id="content" role="main">
    <div class="container">
      <h1><img src="logo.png" alt="logo" class="logo">icuda</h1>
      <p>hands-on introduction to CUDA programming</p>
    </div>
  </div>
  <div class="container section">
    <section>
      <div class="page-header">
        <h1>Outline</h1>
      </div>
      <div class="row">
        <div class="col-md-12">
          <p>This is a hands-on seminar series on pragmatic CUDA programming. It emphasizes <b>examples</b>, <b>tools</b> and <b>libraries</b>.
          <h2>Schedule</h2>
          <ul>
            <li> 1 Dec 2013, 10:00am - 11:30am, SHB 904</li>
            <li> 8 Dec 2013, 10:00am - 11:30am, SHB 904</li>
            <li>15 Dec 2013, 10:00am - 11:30am, SHB 904</li>
          </ul>
          <h2>References</h2>
          <p>CUDA toolkit documentation. <a href="http://docs.nvidia.com/">http://docs.nvidia.com/</a></p>
          <p>CUDA education & training. <a href="https://developer.nvidia.com/cuda-education-training/">https://developer.nvidia.com/cuda-education-training/</a></p>
          <p>University courses. <a href="http://ece408.hwu.crhc.illinois.edu">ECE408/CS483 @ UIUC</a> and <a href="http://stanford-cs193g-sp2010.googlecode.com/svn/trunk/">CS193G @ Stanford University</a></p>
          <h2>Contents</h2>
          <p>Seminar 1: <a href="#vectorAdd">vectorAdd</a>, <a href="#zeroCopy">zeroCopy</a> and <a href="bandwidthTest">bandwidthTest</a></p>
          <p>Seminar 2: <a href="#matrixMul">matrixMul</a>, <a href="#atomicAdd">atomicAdd</a>, <a href="#hyperQ">hyperQ</a></p>
          <p>Seminar 3: <a href="#deviceQuery">deviceQuery</a>, multiDevice, OpenMP, MPI, driver API, OpenCL</p>
        </div>
      </div>
    </section>
    <section>
      <div class="page-header">
        <h1>Samples</h1>
      </div>
      <div class="row">
        <div class="col-md-12">
          <h2>Makefile</h2>
          <p>A sample Makefile looks like this. Instead of <code>gcc</code>, it uses <code>nvcc</code> as CUDA compiler. The option <code>arch</code> specifies the name of the class of nVidia GPU architectures for which the cuda input files must be compiled. Our K20m GPU architecture is <code>sm_35</code>.</p>
<pre class="makefile">
CC=nvcc -arch=sm_35

vectorAdd: vectorAdd.o
	$(CC) -o $@ $^

vectorAdd.o: vectorAdd.cu
	$(CC) -o $@ $< -c

clean:
	rm -f vectorAdd vectorAdd.o
</pre>
          <h2 id="vectorAdd">vectorAdd</h2>
          <p>This sample adds two vectors of floats.</p>
<pre class="cpp">
#include &lt;stdio.h&gt;

__global__ void vectorAdd(const float *a, const float *b, float *c, int numElements)
{
	int i = blockDim.x * blockIdx.x + threadIdx.x;
	if (i < numElements)
	{
		c[i] = a[i] + b[i];
	}
}

int main(int argc, char *argv[])
{
	int numElements = 50000;

	// Allocate vectors a, b and c in host memory.
	size_t size = sizeof(float) * numElements;
	float *h_a = (float *)malloc(size);
	float *h_b = (float *)malloc(size);
	float *h_c = (float *)malloc(size);

	// Initialize vectors a and b.
	for (int i = 0; i < numElements; ++i)
	{
		h_a[i] = rand() / (float)RAND_MAX;
		h_b[i] = rand() / (float)RAND_MAX;
	}

	// Allocate vectors a, b and c in device memory.
	float *d_a;
	float *d_b;
	float *d_c;
	cudaMalloc((void **)&d_a, size);
	cudaMalloc((void **)&d_b, size);
	cudaMalloc((void **)&d_c, size);

	// Copy vectors a and b from host memory to device memory synchronously.
	cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);
	cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);

	// Determine the number of threads per block and the number of blocks per grid.
	int numThreadsPerBlock = 256;
	int numBlocksPerGrid = (numElements + numThreadsPerBlock - 1) / numThreadsPerBlock;

	// Invoke the kernel on device asynchronously.
	vectorAdd&lt;&lt;&lt;numBlocksPerGrid, numThreadsPerBlock&gt;&gt;&gt;(d_a, d_b, d_c, numElements);

	// Copy vector c from device memory to host memory synchronously.
	cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);

	// Validate the result.
	for (int i = 0; i < numElements; ++i)
	{
		float actual = h_c[i];
		float expected = h_a[i] + h_b[i];
		if (fabs(actual - expected) > 1e-7)
		{
			printf("h_c[%d] = %f, expected = %f\n", i, actual, expected);
			break;
		}
	}

	// Cleanup.
	cudaFree(d_c);
	cudaFree(d_b);
	cudaFree(d_a);
	cudaDeviceReset();
	free(h_c);
	free(h_b);
	free(h_a);
}
</pre>
          <p>Try different values of <code>numElements</code> and <code>numThreadsPerBlock</code>.</p>
          <h2 id="zeroCopy">zeroCopy</h2>
          <p>This sample maps device pointers to pinned host memory so that kernels can directly read from and write to pinned host memory.</p>
          <p>On <a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#data-transfer-between-host-and-device">integrated systems</a> where device memory and host memory are physically the same, the mapping mechanism saves superfluous copies from host to device and from device to host.</p>
          <p>On discrete systems where device memory and host memory are physically different, the mapping mechanism saves explicit copies from host to device and from device to host.</p>
<pre class="cpp">
#include &lt;stdio.h&gt;

__global__ void vectorAdd(const float* a, const float* b, float* c)
{
	int i = blockDim.x * blockIdx.x + threadIdx.x;
	c[i] = a[i] + b[i];
}

int main(int argc, char *argv[])
{
	// Initialize the number of threads per block and the number of blocks per grid.
	const unsigned int numThreadsPerBlock = 256;
	const unsigned int numBlocksPerGrid = 1024;
	const unsigned int numThreadsPerGrid = numThreadsPerBlock * numBlocksPerGrid;

	// Set the flag in order to allocate pinned host memory that is accessible to the device.
	cudaSetDeviceFlags(cudaDeviceMapHost);

	// Allocate pinned vectors a, b and c in host memory with the cudaHostAllocMapped flag so that they can be accessed by the device.
	float* h_a;
	float* h_b;
	float* h_c;
	cudaHostAlloc((void**)&h_a, sizeof(float) * numThreadsPerGrid, cudaHostAllocMapped);
	cudaHostAlloc((void**)&h_b, sizeof(float) * numThreadsPerGrid, cudaHostAllocMapped);
	cudaHostAlloc((void**)&h_c, sizeof(float) * numThreadsPerGrid, cudaHostAllocMapped);

	// Initialize vectors a and b.
	for (int i = 0; i < numThreadsPerGrid; ++i)
	{
		h_a[i] = rand() / (float)RAND_MAX;
		h_b[i] = rand() / (float)RAND_MAX;
	}

	// Get the mapped pointers for the device.
	float* d_a;
	float* d_b;
	float* d_c;
	cudaHostGetDevicePointer(&d_a, h_a, 0);
	cudaHostGetDevicePointer(&d_b, h_b, 0);
	cudaHostGetDevicePointer(&d_c, h_c, 0);

	// Invoke the kernel on device asynchronously.
	vectorAdd&lt;&lt;&lt;numBlocksPerGrid, numThreadsPerBlock&gt;&gt;&gt;(d_a, d_b, d_c);

	// Wait for the device to finish.
	cudaDeviceSynchronize();

	// Validate the result.
	for (int i = 0; i < numThreadsPerGrid; ++i)
	{
		float actual = h_c[i];
		float expected = h_a[i] + h_b[i];
		if (fabs(actual - expected) > 1e-7)
		{
			printf("h_c[%d] = %f, expected = %f\n", i, actual, expected);
			break;
		}
	}

	// Cleanup.
	cudaFreeHost(h_c);
	cudaFreeHost(h_b);
	cudaFreeHost(h_a);
	cudaDeviceReset();
}
</pre>
          <p>Try passing the flag <code>cudaDeviceScheduleAuto</code> to <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g18074e885b4d89f5a0fe1beab589e0c8">cudaSetDeviceFlags</a>.</p>
          <p>Try passing the flag <code>cudaHostAllocDefault</code> to <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g15a3871f15f8c38f5b7190946845758c">cudaHostAlloc</a> to allocate the memory as non-mapped.</p>
          <h2 id="bandwidthTest">bandwidthTest</h2>
          <p>This sample measures host-to-device and device-to-host bandwidth via PCIe for pageable and pinned memory of four transfer sizes of 3KB, 15KB, 15MB and 100MB, and outputs them in CSV format.</p>
<pre class="cpp">
#include &lt;stdio.h&gt;
#include &lt;sys/time.h&gt;

// This function returns the current timestamp in seconds.
double shrDeltaT()
{
	static struct timeval old_time;
	struct timeval new_time;
	gettimeofday(&new_time, NULL);
	const double DeltaT = ((double)new_time.tv_sec + 1.0e-6 * (double)new_time.tv_usec) - ((double)old_time.tv_sec + 1.0e-6 * (double)old_time.tv_usec);
	old_time.tv_sec  = new_time.tv_sec;
	old_time.tv_usec = new_time.tv_usec;
	return DeltaT;
}

int main(int argc, char* argv[])
{
	// Initialize 4 transfer sizes, i.e. 3KB, 15KB, 15MB and 100MB.
	const int n = 4;
	const size_t sizes[n] = { 3 << 10, 15 << 10, 15 << 20, 100 << 20 };

	// Initialize the number of transfer iterations, i.e. 60K, 60K, 300 and 30 iterations, respectively.
	const int iterations[n] = { 60000, 60000, 300, 30 };

	// Print header in CSV format.
	printf("size (B),memory,direction,bandwidth (MB/s)\n");

	// Loop through the 4 transfer sizes.
	for (int s = 0; s < n; ++s)
	{
		// Calculate the total transfer size.
		const size_t size = sizes[s];
		const int iteration = iterations[s];
		const double totalSizeInMB = (double)size * iteration / (1 << 20);
		double time;

		// Allocate d_p in device memory.
		void* h_p;
		void* d_p;
		cudaMalloc(&d_p, size);

		// Allocate pageable h_p in host memory.
		h_p = malloc(size);

		// Test host-to-device bandwidth.
		shrDeltaT();
		for (int i = 0; i < iteration; ++i)
		{
			cudaMemcpy(d_p, h_p, size, cudaMemcpyHostToDevice);
		}
		time = shrDeltaT();
		printf("%lu,%s,%s,%.0f\n", size, "pageable", "HtoD", totalSizeInMB / time);

		// Test device-to-host bandwidth.
		shrDeltaT();
		for (int i = 0; i < iteration; ++i)
		{
			cudaMemcpy(h_p, d_p, size, cudaMemcpyDeviceToHost);
		}
		time = shrDeltaT();
		printf("%lu,%s,%s,%.0f\n", size, "pageable", "DtoH", totalSizeInMB / time);

		// Deallocate pageable h_p in host memory.
		free(h_p);

		// Allocate pinned h_p in host memory.
        cudaHostAlloc(&h_p, size, cudaHostAllocDefault);

		// Test host-to-device bandwidth.
		shrDeltaT();
		for (int i = 0; i < iteration; ++i)
		{
			cudaMemcpyAsync(d_p, h_p, size, cudaMemcpyHostToDevice);
		}
		cudaDeviceSynchronize();
		time = shrDeltaT();
		printf("%lu,%s,%s,%.0f\n", size, "pinned", "HtoD", totalSizeInMB / time);

		// Test device-to-host bandwidth.
		shrDeltaT();
		for (int i = 0; i < iteration; ++i)
		{
			cudaMemcpyAsync(h_p, d_p, size, cudaMemcpyDeviceToHost);
		}
		cudaDeviceSynchronize();
		time = shrDeltaT();
		printf("%lu,%s,%s,%.0f\n", size, "pinned", "DtoH", totalSizeInMB / time);

		// Deallocate pinned h_p in host memory.
		cudaFreeHost(h_p);

		// Deallocate d_p in device memory.
		cudaFree(d_p);
	}

	// Cleanup.
	cudaDeviceReset();
}
</pre>
          <p>Try different values of <code>sizes</code>.</p>
          <p>Try <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g48efa06b81cc031b2aa6fdc2e9930741">cudaMemcpy</a> instead of <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1gf2810a94ec08fe3c7ff9d93a33af7255">cudaMemcpyAsync</a> on pinned memory.</p>
          <p>Try <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g36b9fe28f547f28d23742e8c7cd18141">cudaHostRegister</a> to register an existing host memory range as pinned memory, and <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1gc07b1312c60ca36c118e2ed71b192afe">cudaHostUnregister</a> to unregister it after use.</p>
          <p>Try passing the flag <code>cudaHostAllocWriteCombined</code> to <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g15a3871f15f8c38f5b7190946845758c">cudaHostAlloc</a> to allocate the memory as write-combined (WC).</p>
          <h2 id="checkError">checkError</h2>
          <p>This sample checks the return value of every runtime API. It requires <code>-I /usr/local/cuda/samples/common/inc</code> in the Makefile.</p>
<pre class="cpp">
#include &lt;helper_cuda.h&gt;

__global__ void iota(float *a)
{
	int i = blockDim.x * blockIdx.x + threadIdx.x;
	a[i] = i;
}

int main(int argc, char *argv[])
{
	int numElements = 1e+8;
	size_t size = sizeof(float) * numElements;

	// Allocate vector a in device memory.
	float *d_a;
	checkCudaErrors(cudaMalloc((void **)&d_a, size));

	// Determine the number of threads per block and the number of blocks per grid.
	int numThreadsPerBlock = 256;
	int numBlocksPerGrid = (numElements + numThreadsPerBlock - 1) / numThreadsPerBlock;

	// Invoke the kernel on device asynchronously.
	iota&lt;&lt;&lt;numBlocksPerGrid, numThreadsPerBlock&gt;&gt;&gt;(d_a);

	// Cleanup.
	checkCudaErrors(cudaFree(d_a));
	checkCudaErrors(cudaDeviceReset());
}
</pre>
          <p>Try setting <code>numElements</code> to 1e+10, and observe <code>cudaErrorMemoryAllocation</code>.</p>
          <p>Try setting <code>numThreadsPerBlock</code> to 2560, and observe <code>cudaErrorInvalidConfiguration</code>.</p>
          <p>Try accessing <code>a[i*2]</code> in the kernel, and observe <code>cudaErrorLaunchFailure</code>. Note the error line number at line 26 because kernel execution is asynchronous.</p>
          <p>The <code>checkCudaErrors</code> macro, as listed below, is defined in <code>/usr/local/cuda/samples/common/inc/helper_cuda.h</code>. In case a CUDA host call returns an error, it will output the proper CUDA error string to stderr, calls <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1gdcc2c6f914eb9461565b12648faa5e28">cudaDeviceReset</a> to destroy all allocations and reset all state on the current device in the current process, and exits the program. Try to define your own <code>checkCudaErrors</code> macro for your particular applications.</p>
<pre class="cpp">
#ifdef __DRIVER_TYPES_H__
#ifndef DEVICE_RESET
#define DEVICE_RESET cudaDeviceReset();
#endif
#else
#ifndef DEVICE_RESET
#define DEVICE_RESET 
#endif
#endif

template< typename T >
void check(T result, char const *const func, const char *const file, int const line)
{
    if (result)
    {
        fprintf(stderr, "CUDA error at %s:%d code=%d(%s) \"%s\" \n",
                file, line, static_cast<unsigned int>(result), _cudaGetErrorEnum(result), func);
		DEVICE_RESET
        // Make sure we call CUDA Device Reset before exiting
        exit(EXIT_FAILURE);
    }
}

#ifdef __DRIVER_TYPES_H__
// This will output the proper CUDA error strings in the event that a CUDA host call returns an error
#define checkCudaErrors(val)           check ( (val), #val, __FILE__, __LINE__ )
</pre>
          <h2 id="matrixMul">matrixMul</h2>
          <p>This sample uses shared memory to accelerate matrix multiplication.</p>
<pre class="cpp">
</pre>
          <p>Try using <code>double</code> instead of <code>float</code>.</p>
          <p>Try using <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1gac27b566beee1aa9175373bb9e29b8d1">cudaDeviceSetCacheConfig</a> or <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EXECUTION.html#group__CUDART__EXECUTION_1g4f35d04be20a41c5df96613a748eecc1">cudaFuncSetCacheConfig</a> to set the sizes of L1 cache and shared memory out of 64KB.</p>
          <p>Try using <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g1a4789fb687cc36dccc98f25c96f0cd8">cudaDeviceSetSharedMemConfig</a> or <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EXECUTION.html#group__CUDART__EXECUTION_1gcb2e02dc6c75e4f705dfd2252748fe99">cudaFuncSetSharedMemConfig</a> to set shared memory bank width to be eight bytes natively.</p>
          <p>Try using <a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared">dynamic shared memory</a> which can have a dynamic size at run time.</p>
          <p>Try using <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g80d689bc903792f906e49be4a0b6d8db">cudaMallocPitch</a> to allocate a 2D array and <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g17f3a55e8c9aef5f90b67cdf22851375">cudaMemcpy2D</a> to copy a 2D array.</p>
          <p>Try rewriting the previous <a href="#bandwidthTest">bandwidthTest</a> sample using <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html#group__CUDART__EVENT_1ge31b1b1558db52579c9e23c5782af93e">cudaEventRecord</a> and <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html#group__CUDART__EVENT_1g14c387cc57ce2e328f6669854e6020a5">cudaEventElapsedTime</a> instead of <code>shrDeltaT</code>.</p>
          <h2 id="atomicAdd">atomicAdd</h2>
          <p>This sample uses <a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#atomic-functions">atomic functions</a>, <a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#assertion">assertions</a> and <a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#formatted-output">printf</a>.</p>
<pre class="cpp">
#include &lt;stdio.h&gt;
#include &lt;assert.h&gt;

__constant__ int inc;
__device__ int sum;

__global__ void atomicAdd()
{
	int s = atomicAdd(&sum, inc);
	assert((s - 1) % inc == 0);
	if (threadIdx.x == 0)
	{
		printf("blockIdx.x = %d, sum = %d\n", blockIdx.x, s);
	}
}

int main(int argc, char *argv[])
{
	// Initialize inc and sum.
	int h_inc = 3;
	int h_sum = 1;

	// Copy inc and sum from host memory to device memory synchronously.
	cudaMemcpyToSymbol(inc, &h_inc, sizeof(int));
	cudaMemcpyToSymbol(sum, &h_sum, sizeof(int));

	// Invoke the kernel on device asynchronously.
	atomicAdd&lt;&lt;&lt;2, 2&gt;&gt;&gt;();

	// Wait for the device to finish.
	cudaDeviceSynchronize();

	// Copy sum from device memory to host memory synchronously.
	cudaMemcpyFromSymbol(&h_sum, sum, sizeof(int));

	// Print the result.
	printf("sum = %d\n", h_sum);

	// Cleanup.
	cudaDeviceReset();
}
</pre>
          <p>Try different numbers of threads per block and different numbers of blocks per grid.</p>
          <p>Try atomic functions on shared memory, and compare the performance to atomic functions on global memory.</p>
          <h2 id="hyperQ">hyperQ</h2>
          <p>This sample uses multiple streams to exploit the <a href="http://docs.nvidia.com/cuda/kepler-tuning-guide/index.html#hyperq">HyperQ</a> technology.</p>
<pre class="cpp">
#include &lt;stdio.h&gt;

__global__ void spin(clock_t numClocks)
{
	for (const clock_t threshold = clock() + numClocks; clock() < threshold;);
}

int main(int argc, char *argv[])
{
	// Initialize constants.
	const int numMilliseconds = 10;
	const int numKernels = 2;
	const int numStreams = 32;

	// Get the major and minor compute capability version numbers.
	int major, minor;
	cudaDeviceGetAttribute(&major, cudaDevAttrComputeCapabilityMajor, 0);
	cudaDeviceGetAttribute(&minor, cudaDevAttrComputeCapabilityMinor, 0);

	// Get the peak clock frequency in KHz of device 0.
	int clockRate;
	cudaDeviceGetAttribute(&clockRate, cudaDevAttrClockRate, 0);

	// Calculate the number of clocks within a certain period.
	clock_t numClocks = clockRate * numMilliseconds;

	// Create streams to enqueue kernels.
	cudaStream_t *streams = (cudaStream_t*)malloc(sizeof(cudaStream_t) * numStreams);
	for (int s = 0; s < numStreams; ++s)
	{
		cudaStreamCreate(&streams[s]);
	}

	// Create events to record timing data.
	cudaEvent_t beg, end;
	cudaEventCreate(&beg);
	cudaEventCreate(&end);

	// Record an event in stream 0 before kernel invocations.
	cudaEventRecord(beg, 0);

	// Enqueue kernels to streams.
	for (int s = 0; s < numStreams; ++s)
	{
		for (int k = 0; k < numKernels; ++k)
		{
			// The 1st argument is of type dim3 and specifies the number of blocks per grid.
			// The 2nd argument is of type dim3 and specifies the number of threads per block.
			// The 3rd argument is of type size_t and specifiesthe number of bytes in shared memory that is dynamically allocated per block for this call in addition to the statically allocated memory. It is an optional argument which defaults to 0.
			// The 4th argument is of type cudaStream_t and specifies the associated stream. It is an optional argument which defaults to 0.
			spin&lt;&lt;&lt;1, 1, 0, streams[s]&gt;&gt;&gt;(numClocks);
		}
	}

	// Record an event in stream 0 after kernel invocations.
	cudaEventRecord(end, 0);

	// Wait for the event to complete.
	cudaEventSynchronize(end);

	// Compute the elapsed time between two events.
	float elapsed;
	cudaEventElapsedTime(&elapsed, beg, end);

	// Print the result.
	printf("%d streams, each %d kernels, each %d ms\n", numStreams, numKernels, numMilliseconds);
	printf("       SM <= 1.3:%4d ms\n", numMilliseconds * numKernels * numStreams);
	printf("2.0 <= SM <= 3.0:%4d ms\n", numMilliseconds * (1 + (numKernels - 1) * numStreams));
	printf("3.5 <= SM       :%4d ms\n", numMilliseconds * numKernels);
	printf("       SM == %d.%d:%4d ms\n", major, minor, (int)elapsed);

	// Cleanup.
	cudaEventDestroy(end);
	cudaEventDestroy(beg);
	for (int s = 0; s < numStreams; ++s)
	{
		cudaStreamDestroy(streams[s]);
	}
	free(streams);
	cudaDeviceReset();
}
</pre>
          <p>Try different values of <code>numMilliseconds</code>, <code>numKernels</code> and <code>numStreams</code>.</p>
          <p>Try different numbers of threads per block, different numbers of blocks per grid, and different sizes of dynamic shared memory.</p>
          <p>Try double buffering.</p>
          <h2 id="deviceQuery">deviceQuery</h2>
          <p>This sample enumerates some properties of the CUDA devices present in the system.</p>
<pre class="cpp">
#include &lt;stdio.h&gt;

int main(int argc, char *argv[])
{
	// Get the number of devices.
	int deviceCount;
	cudaGetDeviceCount(&deviceCount);
	printf("deviceCount: %d\n", deviceCount);

	// Get the properties of device 0.
	cudaDeviceProp deviceProp;
	cudaGetDeviceProperties(&deviceProp, 0);

	// Print some properties.
	printf("name: %s\n", deviceProp.name);
	printf("major: %d\n", deviceProp.major);
	printf("minor: %d\n", deviceProp.minor);
	printf("multiProcessorCount: %d\n", deviceProp.multiProcessorCount);
	printf("totalGlobalMem: %lu B = %lu MB\n", deviceProp.totalGlobalMem, deviceProp.totalGlobalMem / 1048576);
	printf("sharedMemPerBlock: %lu B = %lu KB\n", deviceProp.sharedMemPerBlock, deviceProp.sharedMemPerBlock / 1024);
	printf("totalConstMem: %lu B = %lu KB\n", deviceProp.totalConstMem, deviceProp.totalConstMem / 1024);
	printf("regsPerBlock: %d\n", deviceProp.regsPerBlock);
	printf("ECCEnabled: %d\n", deviceProp.ECCEnabled);
	printf("kernelExecTimeoutEnabled: %d\n", deviceProp.kernelExecTimeoutEnabled);
	printf("clockRate: %d KHz = %d MHz\n", deviceProp.clockRate, deviceProp.clockRate / 1000);
	printf("memoryClockRate: %d KHz = %d MHz\n", deviceProp.memoryClockRate, deviceProp.memoryClockRate / 1000);
	printf("memoryBusWidth: %d bits\n", deviceProp.memoryBusWidth);
	printf("l2CacheSize: %d B = %d KB\n", deviceProp.l2CacheSize, deviceProp.l2CacheSize / 1024);
	printf("warpSize: %d\n", deviceProp.warpSize);
	printf("maxThreadsPerMultiProcessor: %d\n", deviceProp.maxThreadsPerMultiProcessor);
	printf("maxThreadsPerBlock: %d\n", deviceProp.maxThreadsPerBlock);
	printf("maxThreadsDim[0]: %d\n", deviceProp.maxThreadsDim[0]);
	printf("maxThreadsDim[1]: %d\n", deviceProp.maxThreadsDim[1]);
	printf("maxThreadsDim[2]: %d\n", deviceProp.maxThreadsDim[2]);
	printf("maxGridSize[0]: %d\n", deviceProp.maxGridSize[0]);
	printf("maxGridSize[1]: %d\n", deviceProp.maxGridSize[1]);
	printf("maxGridSize[2]: %d\n", deviceProp.maxGridSize[2]);
	printf("deviceOverlap: %d\n", deviceProp.deviceOverlap);
	printf("asyncEngineCount: %d\n", deviceProp.asyncEngineCount);
	printf("integrated: %d\n", deviceProp.integrated);
	printf("canMapHostMemory: %d\n", deviceProp.canMapHostMemory);
	printf("concurrentKernels: %d\n", deviceProp.concurrentKernels);
	printf("tccDriver: %d\n", deviceProp.tccDriver);
	printf("unifiedAddressing: %d\n", deviceProp.unifiedAddressing);
	printf("pciBusID: %d\n", deviceProp.pciBusID);
	printf("pciDeviceID: %d\n", deviceProp.pciDeviceID);
	printf("computeMode: %d\n", deviceProp.computeMode);
	if (deviceProp.computeMode == cudaComputeModeDefault) printf("computeMode: %s\n", "Default (multiple host threads can use ::cudaSetDevice() with device simultaneously)");
	if (deviceProp.computeMode == cudaComputeModeExclusive) printf("computeMode: %s\n", "Exclusive (only one host thread in one process is able to use ::cudaSetDevice() with this device)");
	if (deviceProp.computeMode == cudaComputeModeProhibited) printf("computeMode: %s\n", "Prohibited (no host thread can use ::cudaSetDevice() with this device)");
	if (deviceProp.computeMode == cudaComputeModeExclusiveProcess) printf("computeMode: %s\n", "Exclusive Process (many threads in one process is able to use ::cudaSetDevice() with this device)");
}
</pre>
          <p>Try enumerating all the properties returned by <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g929dcd9a191e17b7498e7ccaa3d16350">cudaGetDeviceProperties</a> of all the devices.</p>
          <h2 id="openmp">openmp</h2>
          <p>This sample uses OpenMP to create multiple CPU threads to utilize multiple GPUs.</p>
<pre class="cpp">
#include &lt;stdio.h&gt;
#include &lt;omp.h&gt;

__global__ void scalarAdd(int *data, int inc)
{
	int i = blockIdx.x * blockDim.x + threadIdx.x;
	data[i] += inc;
}

int main(int argc, char *argv[])
{
	// Get the number of CUDA devices.
	int numDevices;
	cudaGetDeviceCount(&numDevices);

	// Allocate and initialize data.
	size_t numElements = 8192 * numDevices;
	size_t numBytes = sizeof(int) * numElements;
	int *data = (int *)malloc(numBytes);
	memset(data, 0, numBytes);
	int inc = 7;

	// Create as many CPU threads as there are CUDA devices. Each CPU thread controls a different device, processing its portion of the data.
	omp_set_num_threads(numDevices);

	// All variables declared inside an "omp parallel" scope are local to each CPU thread.
	#pragma omp parallel
	{
		// Get the number of CPU threads and the thread number of the current CPU thread.
		// 0 <= threadNum <= numThreads - 1
		int numThreads = omp_get_num_threads();
		int threadNum = omp_get_thread_num();

		// Set device to be used for GPU executions.
		int deviceNum = threadNum % numDevices;
		cudaSetDevice(deviceNum);

		// Calculate the number of elements per CPU thread and the number of bytes per CPU thread.
		size_t numElementsPerThread = numElements / numThreads;
		size_t numBytesPerThread = sizeof(int) * numElementsPerThread;

		// Calculate the offset to the original data for the current CPU thread.
		int *h = data + numElementsPerThread * threadNum;

		// Allocate device memory to temporarily hold the portion of the data of the current CPU thread.
		int *d;
		cudaMalloc((void **)&d, numBytesPerThread);

		// Copy the portion of the data of the current CPU thread from host memory to device memory.
		cudaMemcpy(d, h, numBytesPerThread, cudaMemcpyHostToDevice);

		// Invoke the kernel for the current portion of the data.
		scalarAdd&lt;&lt;&lt;numElementsPerThread / 128, 128&gt;&gt;&gt;(d, inc);

		// Copy the portion of the data of the current CPU thread from device memory to host memory.
		cudaMemcpy(h, d, numBytesPerThread, cudaMemcpyDeviceToHost);

		// Deallocate the temporary device memory.
		cudaFree(d);
	}

	for (int i = 0; i < numElements; ++i)
	{
		int actual = data[i];
		int expected = 0 + inc;
		if (actual != expected)
		{
			printf("data[%d] = %d, expected = %d\n", i, actual, expected);
			break;
		}
	}

	// Cleanup.
	for (int i = 0; i < numDevices; ++i)
	{
		cudaSetDevice(i);
		cudaDeviceReset();
	}
	free(data);
}
</pre>
          <p>In the Makefile, pass the compiler option <code>-Xcompiler -fopenmp</code> to <code>nvcc</code> in both compilation and linking.</p>
<pre class="makefile">
CC=nvcc -arch=sm_35 -Xcompiler -fopenmp

openmp: openmp.o
	$(CC) -o $@ $^

openmp.o: openmp.cu
	$(CC) -o $@ $< -c -I ~/cuda-5.5/samples/common/inc

clean:
	rm -f openmp openmp.o
</pre>
          <p>Try <code>omp_set_num_threads(numDevices*2);</code> to create twice as many CPU threads as there are CUDA devices. In this case two threads will be allocating resources and launching kernels on the same device.</p>
          <p>Try using <code>valgrind</code> to detect memory leak in the following 3 cases of cleanup.</p>
<pre class="sh">
<b class="text-info">hjli@P32-1:/data/hjli/cudart/openmp></b> valgrind ./openmp
</pre>
          <p>Case 1: <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1gdcc2c6f914eb9461565b12648faa5e28">cudaDeviceReset</a> is called on every device.</p>
<pre class="cpp">
	for (int i = 0; i < numDevices; ++i)
	{
		cudaSetDevice(i);
		cudaDeviceReset();
	}
</pre>
<pre class="sh">
==30241== HEAP SUMMARY:
==30241==     in use at exit: 347,025 bytes in 133 blocks
==30241==   total heap usage: 21,117 allocs, 20,984 frees, 15,888,183 bytes allocated
==30241== 
==30241== LEAK SUMMARY:
==30241==    definitely lost: 0 bytes in 0 blocks
==30241==    indirectly lost: 0 bytes in 0 blocks
==30241==      possibly lost: 3,600 bytes in 24 blocks
==30241==    still reachable: 343,425 bytes in 109 blocks
==30241==         suppressed: 0 bytes in 0 blocks
</pre>
          <p>Case 2: <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1gdcc2c6f914eb9461565b12648faa5e28">cudaDeviceReset</a> is called on one device.</p>
<pre class="cpp">
//	for (int i = 0; i < numDevices; ++i)
//	{
//		cudaSetDevice(i);
		cudaDeviceReset();
//	}
</pre>
<pre class="sh">
==30316== HEAP SUMMARY:
==30316==     in use at exit: 7,035,515 bytes in 11,801 blocks
==30316==   total heap usage: 21,098 allocs, 9,297 frees, 15,887,463 bytes allocated
==30316== 
==30316== LEAK SUMMARY:
==30316==    definitely lost: 0 bytes in 0 blocks
==30316==    indirectly lost: 0 bytes in 0 blocks
==30316==      possibly lost: 56,704 bytes in 412 blocks
==30316==    still reachable: 6,978,811 bytes in 11,389 blocks
==30316==         suppressed: 0 bytes in 0 blocks
</pre>
          <p>Case 3: <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1gdcc2c6f914eb9461565b12648faa5e28">cudaDeviceReset</a> is not called at all.</p>
<pre class="cpp">
//	for (int i = 0; i < numDevices; ++i)
//	{
//		cudaSetDevice(i);
//		cudaDeviceReset();
//	}
</pre>
<pre class="sh">
==30373== HEAP SUMMARY:
==30373==     in use at exit: 10,397,784 bytes in 17,776 blocks
==30373==   total heap usage: 21,095 allocs, 3,319 frees, 15,888,015 bytes allocated
==30373== 
==30373== LEAK SUMMARY:
==30373==    definitely lost: 0 bytes in 0 blocks
==30373==    indirectly lost: 0 bytes in 0 blocks
==30373==      possibly lost: 84,888 bytes in 618 blocks
==30373==    still reachable: 10,312,896 bytes in 17,158 blocks
==30373==         suppressed: 0 bytes in 0 blocks
</pre>
          <h2 id="mpi">mpi</h2>
          <p>This sample uses MPI to create multiple CPU processes to utilize multiple GPUs.</p>
<pre class="cpp">
#include &lt;stdio.h&gt;
#include &lt;mpi.h&gt;

__global__ void square(int *d)
{
	int i = blockDim.x * blockIdx.x + threadIdx.x;
	d[i] = d[i] * d[i];
}

int main(int argc, char *argv[])
{
	// Initialize MPI.
	MPI_Init(&argc, &argv);

	// Get the node count and node rank.
	int commSize, commRank;
	MPI_Comm_size(MPI_COMM_WORLD, &commSize);
	MPI_Comm_rank(MPI_COMM_WORLD, &commRank);

	// Get the number of CUDA devices.
	int numDevices;
	cudaGetDeviceCount(&numDevices);

	// Initialize constants.
	int numThreadsPerBlock = 256;
	int numBlocksPerGrid = 10000;
	int dataSizePerNode = numThreadsPerBlock * numBlocksPerGrid;
	int dataSize = dataSizePerNode * commSize;

	// Generate some random numbers on the root node.
	int *data;
	if (commRank == 0)
	{
		data = new int[dataSize];
		for (int i = 0; i < dataSize; ++i)
		{
			data[i] = rand() % 10;
		}
	}

	// Allocate a buffer on the current node.
	int *dataPerNode = new int[dataSizePerNode];

	// Dispatch a portion of the input data to each node.
	MPI_Scatter(data, dataSizePerNode, MPI_INT, dataPerNode, dataSizePerNode, MPI_INT, 0, MPI_COMM_WORLD);

	// Compute the square of each element on device.
	int *d;
	cudaSetDevice(commRank % numDevices);
	cudaMalloc((void **)&d, sizeof(int) * dataSizePerNode);
	cudaMemcpy(d, dataPerNode, sizeof(int) * dataSizePerNode, cudaMemcpyHostToDevice);
	square&lt;&lt;&lt;numBlocksPerGrid, numThreadsPerBlock&gt;&gt;&gt;(d);
	cudaMemcpy(dataPerNode, d, sizeof(int) * dataSizePerNode, cudaMemcpyDeviceToHost);
	cudaFree(d);

	// Compute the sum of the current node.
	int sum = 0;
	for (int i = 0; i < dataSizePerNode; ++i)
	{
		sum += dataPerNode[i];
	}

	// Compute the sum of all nodes.
	int actual;
	MPI_Reduce(&sum, &actual, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);

	// Validate the result.
	if (commRank == 0)
	{
		int expected = 0;
		for (int i = 0; i < dataSize; ++i)
		{
			expected += data[i] * data[i];
		}
		if (actual != expected)
		{
			printf("actual = %d, expected = %d\n", actual, expected);
		}
		delete[] data;
	}

	// Cleanup.
	delete[] dataPerNode;
	cudaDeviceReset();
	MPI_Finalize();
}
</pre>
          <p>In the Makefile, pass the compiler option <code>-L /usr/lib/openmpi -lmpi -lmpi_cxx</code> to <code>nvcc</code> in linking.</p>
<pre class="makefile">
CC=nvcc -arch=sm_11

mpi: mpi.o
	$(CC) -o $@ $^ -L /usr/lib/openmpi -lmpi -lmpi_cxx

mpi.o: mpi.cu
	$(CC) -o $@ $< -c

clean:
	rm -f mpi mpi.o
</pre>
          <p>Run the executable with <code>mpirun</code>. Specify the number of processes to run with the option <code>-n</code>.</p>
<pre class="sh">
<b class="text-info">hjli@P32-1:/data/hjli/cudart/mpi></b> mpirun -n 4 ./mpi
</pre>
          <p>Try using <code>float</code> instead of <code>int</code>.</p>
          <h2 id="openmp">openmp</h2>
          <p>This sample .</p>
<pre class="cpp">
</pre>
          <p>Try .</p>
        </div>
      </div>
    </section>
    <section>
      <div class="page-header">
        <h1>Tools</h1>
      </div>
      <div class="row">
        <div class="col-md-12">
<pre class="sh">
<b class="text-info">hjli@P32-1:~></b> ls -l /usr/local/cuda-5.5/bin
total 32112
-rwxr-xr-x. 1 root root   63000 Sep  4 12:48 bin2c
lrwxrwxrwx. 1 root root       4 Sep  4 12:48 computeprof -> nvvp
drwxr-xr-x. 2 root root    4096 Sep  4 12:48 crt
-rwxr-xr-x. 1 root root 3415240 Sep  4 12:48 cudafe
-rwxr-xr-x. 1 root root 3081848 Sep  4 12:48 cudafe++
-rwxr-xr-x. 1 root root 6093856 Sep  4 12:48 cuda-gdb
-rwxr-xr-x. 1 root root  342809 Sep  4 12:48 cuda-gdbserver
-rwxr-xr-x. 1 root root     665 Sep  4 12:49 cuda-install-samples-5.5.sh
-rwxr-xr-x. 1 root root  348832 Sep  4 12:48 cuda-memcheck
-rwxr-xr-x. 1 root root  230744 Sep  4 12:48 cuobjdump
-rwxr-xr-x. 1 root root  163200 Sep  4 12:48 fatbin
-rwxr-xr-x. 1 root root  125584 Sep  4 12:48 fatbinary
-rwxr-xr-x. 1 root root   54584 Sep  4 12:48 filehash
-rwxr-xr-x. 1 root root     219 Sep  4 12:48 nsight
-rwxr-xr-x. 1 root root  164608 Sep  4 12:48 nvcc
-rw-r--r--. 1 root root     377 Sep  4 12:48 nvcc.profile
-rwxr-xr-x. 1 root root 3790843 Sep  4 12:48 nvdisasm
-rwxr-xr-x. 1 root root 5895464 Sep  4 12:48 nvlink
-rwxr-xr-x. 1 root root 3300720 Sep  4 12:48 nvprof
-rwxr-xr-x. 1 root root     215 Sep  4 12:48 nvvp
-rwxr-xr-x. 1 root root 5759328 Sep  4 12:48 ptxas
</pre>
          <h2>Compiler: nvcc & ptxas</h2>
          <p>Analogue to GNU <code>cc</code>.</p>
          <p><img class="img-responsive" src="http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/graphics/cuda-compilation-from-cu-to-cu-cpp-ii.png" alt="nvcc"></p>
          <p>The <code>-ptx</code> option generates ptx. The <code>-cubin</code> option generates cubin.</p>
          <h2>Debugger: cuda-gdb & cuda-gdbserver</h2>
          <p>debugger tools analogue to GNU <code>gdb</code> and <code>gdbserver</code>.</p>
          <h2>Profiler: nvprof & nvvp</h2>
          <p>analogue to GNU <code>gprof</code>.</p>
<pre class="sh">
<b class="text-info">hjli@P32-1:~></b> nvprof -o %h-%p.nvprof ~/idock/bin/idock --config idock.conf
<b class="text-info">hjli@P32-1:~></b> nvprof -o %h-%p.nvprof --analysis-metrics ~/idock/bin/idock --config idock.conf
<b class="text-info">hjli@P32-1:~></b> nvprof -i pc90124-26367.nvprof --print-gpu-trace
<b class="text-info">hjli@P32-1:~></b> nvprof -i pc90124-26367.nvprof --print-api-trace
</pre>
          <p>File -> Import -> Nvprof</p>
          <h2>cuda-memcheck</h2>
          <p>cuda-memcheck is a tool analogue to <code>valgrind</code>.</p>
<pre class="sh">
<b class="text-info">hjli@P32-1:~></b> cuda-memcheck --leak-check full ~/idock/bin/idock --config idock.cfg
<b class="text-info">hjli@P32-1:~></b> cuda-memcheck --tool racecheck ~/idock/bin/idock --config idock.cfg
</pre>
          <h2>IDE: Nsight</h2>
          <p>Visual Studio and Eclipse</p>
          <h2>Occupancy.xls</h2>
          <p>There are a few easy ways to quickly get started with Bootstrap, each one appealing to a different skill level and use case. Read through to see what suits your particular needs.</p>
          <h2>Monitor: nvidia-smi</h2>
          <p>analogue to <code>top</code>.</p>
<pre class="sh">
<b class="text-info">hjli@P32-1:~></b> nvidia-smi
Tue Sep 10 10:39:25 2013       
+------------------------------------------------------+                       
| NVIDIA-SMI 5.319.37   Driver Version: 319.37         |                       
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K20m          Off  | 0000:08:00.0     Off |                    0 |
| N/A   35C    P0    42W / 225W |       11MB /  4799MB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla K20m          Off  | 0000:24:00.0     Off |                    0 |
| N/A   41C    P0    44W / 225W |       11MB /  4799MB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla K20m          Off  | 0000:27:00.0     Off |                    0 |
| N/A   35C    P0    39W / 225W |       11MB /  4799MB |     78%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Compute processes:                                               GPU Memory |
|  GPU       PID  Process name                                     Usage      |
|=============================================================================|
|  No running compute processes found                                         |
+-----------------------------------------------------------------------------+
</pre>
        </div>
      </div>
    </section>
    <section>
      <div class="page-header">
        <h1>Libraries</h1>
      </div>
      <div class="row">
        <div class="col-md-12">
          <h2>thrust</h2>
          <h2>CUBLAS</h2>
          <p>Analogue to Intel <code>MKL</code></p>
          <h2>CUFFT</h2>
          <h2>CURAND</h2>
        </div>
      </div>
    </section>
  </div>
  <footer role="contentinfo">
    <div class="container">
      <p><a href="http://www.cuhk.edu.hk"><img src="/cuhk.jpg" alt="CUHK logo"></a>&copy; 2012-2013 Chinese University of Hong Kong. Platform designed by <a href="http://www.cse.cuhk.edu.hk/~hjli">Hongjian Li</a>. Code licensed under <a href="http://www.apache.org/licenses/LICENSE-2.0">Apache License 2.0</a>. Documentation licensed under <a href="http://creativecommons.org/licenses/by/3.0">CC BY 3.0</a>.<a href="http://validator.w3.org/check?uri=referer"><img src="/HTML5_Badge_512.png" alt="HTML5 logo"></a></p>
    </div>
  </footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
  <script src="/bootstrap.min.js"></script>
  <script src="/jquery.lazyload.min.js"></script>
  <script src="/jquery.snippet.min.js"></script>
  <script src="/sh_sh.min.js"></script>
  <script src="sh_makefile.min.js"></script>
  <script src="index.js"></script>
</body>
</html>
