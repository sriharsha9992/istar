<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="author" content="Hongjian Li">
  <meta name="description" content="hands-on introduction to CUDA programming">
  <title>Hands-on Introduction to CUDA Programming</title>
  <link rel="stylesheet" href="/bootstrap.min.css">
  <link rel="stylesheet" href="/index.css">
  <link rel="stylesheet" href="jquery.snippet.min.css">
  <link rel="stylesheet" href="index.css">
  <link rel="shortcut icon" href="/favicon.ico">
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="apple-touch-icon-precomposed" sizes="114x114" href="/apple-touch-icon-114-precomposed.png">
  <link rel="apple-touch-icon-precomposed" sizes="72x72" href="/apple-touch-icon-72-precomposed.png">
  <link rel="apple-touch-icon-precomposed" href="/apple-touch-icon-57-precomposed.png">
  <script>
    var _gaq=[['_setAccount','UA-20604862-1'],['_trackPageview']];
    (function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];
    g.src=('https:'==location.protocol?'//ssl':'//www')+'.google-analytics.com/ga.js';
    s.parentNode.insertBefore(g,s)}(document,'script'));
  </script>
</head>
<body>
  <a class="sr-only" href="#content">Skip navigation</a>
  <a href="//github.com/HongjianLi/istar" class="ribbon"></a>
  <header class="navbar navbar-inverse navbar-static-top" role="banner">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
      </div>
      <nav class="collapse navbar-collapse" role="navigation">
        <ul class="nav navbar-nav">
          <li>
            <a href="/"><img src="/logo.png" alt="istar logo">istar: software as a service</a>
          </li>
          <li>
            <a href="/idock"><img src="/idock/logo.png" alt="idock logo">idock: protein-ligand docking</a>
          </li>
          <li>
            <a href="/iview"><img src="/iview/logo.png" alt="iview logo">iview: interactive WebGL visualizer</a>
          </li>
          <li>
            <a href="/igrep"><img src="/igrep/logo.png" alt="igrep logo">igrep: DNA sequence matching</a>
          </li>
          <li class="active">
            <a href="/icuda"><img src="/icuda/logo.png" alt="icuda logo">icuda: introduction to CUDA</a>
          </li>
        </ul>
      </nav>
    </div>
  </header>
  <div class="jumbotron" id="content" role="main">
    <div class="container">
      <h1><img src="logo.png" alt="logo" class="logo">icuda</h1>
      <p>hands-on introduction to CUDA programming</p>
    </div>
  </div>
  <div class="container section">
    <section>
      <div class="page-header">
        <h1>Outline</h1>
      </div>
      <div class="row">
        <div class="col-md-12">
          <p>This is a hands-on seminar series on pragmatic CUDA programming. It emphasizes <b>samples</b>, <b>libraries</b> and <b>tools</b>. This seminar series consists of 3 seminars.</p>
          <h2>Contents</h2>
          <p>Seminar 1: This seminar gives a high-level overview of GPU hardware and CUDA programming philosophy. It covers four samples including <a href="#vectorAdd">vectorAdd</a>, <a href="#zeroCopy">zeroCopy</a>, <a href="bandwidthTest">bandwidthTest</a> and <a href="#checkError">checkError</a>, and two tools including <a href="#nvcc">nvcc</a> and <a href="#cuda-gdb">cuda-gdb</a>.</p>
          <p>Seminar 2: This seminar illustrates how to exploit single-device parallelism and GPU-specific hardware components. It covers four samples including <a href="#matrixMul">matrixMul</a>, <a href="#atomicAdd">atomicAdd</a>, <a href="#asyncEngine">asyncEngine</a> and <a href="#hyperQ">hyperQ</a>, and three tools including <a href="#nvprof">nvprof</a>, <a href="#nsight">nsight</a> and <a href="#CUDA_Occupancy_Calculator.xls">CUDA_Occupancy_Calculator.xls</a>.</p>
          <p>Seminar 3: This seminar illustrates how to exploit multi-device parallelism and how to utilize libraries. It covers six samples including <a href="#deviceQuery">deviceQuery</a>, <a href="#multiDevice">multiDevice</a>, <a href="#openmp">openmp</a>, <a href="#mpi">mpi</a>, <a href="#cublas">cublas</a> and <a href="#thrust">thrust</a>, and four tools including <a href="#nvidia-smi">nvidia-smi</a>, <a href="#cuda-memcheck">cuda-memcheck</a>, <a href="#nvdisasm">cuobjdump & nvdisasm</a>.</p>
          <h2>Time & venue</h2>
          <p>Seminar 1: 08 Nov 2013, 2:00pm - 4:00pm, SHB 123</p>
          <p>Seminar 2: 14 Nov 2013, 2:00pm - 4:00pm, SHB 123</p>
          <p>Seminar 3: 21 Nov 2013, 2:00pm - 4:00pm, SHB 123</p>
          <h2>Forms</h2>
          <p><a href="GPGPU-Account.pdf">GPGPU System ACCOUNT REQUEST FORM</a></p>
          <h2>References</h2>
          <p><a href="https://developer.nvidia.com/category/zone/cuda-zone">CUDA zone</a></p>
          <p><a href="http://docs.nvidia.com/cuda">CUDA toolkit documentation</a></p>
          <p><a href="http://appsrv.cse.cuhk.edu.hk/~hjli/pptx/icuda.pptx">Seminar slides</a></p>
          <p><a href="seminar1.m2ts">Seminar 1 video</a>, <a href="seminar2.m2ts">Seminar 2 video</a>, <a seminar3.m2ts">Seminar 3 video</a></p>
          <p><a href="http://courses.engr.illinois.edu/ece408">UIUC course ECE408/CS483: Applied Parallel Programming</a></p>
        </div>
      </div>
    </section>
    <section>
      <div class="page-header">
        <h1>Samples</h1>
      </div>
      <div class="row">
        <div class="col-md-12">
          <p>All the samples are free and open source under Apache License 2.0. They are available at <a href="https://github.com/HongjianLi/cudart">https://github.com/HongjianLi/cudart</a>.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~></b> git clone https://github.com/HongjianLi/cudart.git
</pre>
          <p>All the samples use the <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/">CUDA Runtime API</a>.</p>
          <h2>Makefile</h2>
          <p>A sample Makefile looks like this. Instead of <code>gcc</code>, it uses <code>nvcc</code> as CUDA compiler. The option <code>-arch</code> specifies the name of the class of nVidia GPU architectures for which the cuda input files must be compiled. Our K20m GPU architecture is <code>sm_35</code>. Refer to <a href="http://on-demand.gputechconf.com/gtc-express/2012/presentations/inside-tesla-kepler-k20-family.pdf">Inside the Kepler Tesla K20 Family</a> for more detail.</p>
          <pre id="vectorAdd/Makefile"></pre>
          <h2 id="vectorAdd">vectorAdd</h2>
          <p>This sample adds two vectors of floats.</p>
          <pre id="vectorAdd/vectorAdd.cu"></pre>
          <p>Try different values of <code>numElements</code> and <code>numThreadsPerBlock</code>.</p>
          <h2 id="zeroCopy">zeroCopy</h2>
          <p>This sample maps device pointers to pinned host memory so that kernels can directly read from and write to pinned host memory.</p>
          <p>On <a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#data-transfer-between-host-and-device">integrated systems</a> where device memory and host memory are physically the same, the mapping mechanism saves superfluous copies from host to device and from device to host.</p>
          <p>On discrete systems where device memory and host memory are physically different, the mapping mechanism saves explicit copies from host to device and from device to host.</p>
          <pre id="zeroCopy/zeroCopy.cu"></pre>
          <p>Try passing the flag <code>cudaDeviceScheduleAuto</code> to <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g18074e885b4d89f5a0fe1beab589e0c8">cudaSetDeviceFlags</a>.</p>
          <p>Try passing the flag <code>cudaHostAllocDefault</code> to <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g15a3871f15f8c38f5b7190946845758c">cudaHostAlloc</a> to allocate the memory as non-mapped.</p>
          <h2 id="bandwidthTest">bandwidthTest</h2>
          <p>This sample measures host-to-device and device-to-host bandwidth via PCIe for pageable and pinned memory of four transfer sizes of 3KB, 15KB, 15MB and 100MB, and outputs them in CSV format.</p>
          <pre id="bandwidthTest/bandwidthTest.cu"></pre>
          <p>Try different values of <code>sizes</code>.</p>
          <p>Try <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g48efa06b81cc031b2aa6fdc2e9930741">cudaMemcpy</a> instead of <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1gf2810a94ec08fe3c7ff9d93a33af7255">cudaMemcpyAsync</a> on pinned memory.</p>
          <p>Try <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g36b9fe28f547f28d23742e8c7cd18141">cudaHostRegister</a> to register an existing host memory range as pinned memory, and <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1gc07b1312c60ca36c118e2ed71b192afe">cudaHostUnregister</a> to unregister it after use.</p>
          <p>Try passing the flag <code>cudaHostAllocWriteCombined</code> to <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g15a3871f15f8c38f5b7190946845758c">cudaHostAlloc</a> to allocate the memory as write-combined (WC).</p>
          <h2 id="checkError">checkError</h2>
          <p>This sample checks the return value of every runtime API. It requires <code>-I /usr/local/cuda/samples/common/inc</code> in the Makefile.</p>
          <pre id="checkError/checkError.cu"></pre>
          <p>Try setting <code>numElements</code> to 1e+10, and observe <code>cudaErrorMemoryAllocation</code>.</p>
          <p>Try setting <code>numThreadsPerBlock</code> to 2560, and observe <code>cudaErrorInvalidConfiguration</code>.</p>
          <p>Try accessing <code>a[i*2]</code> in the kernel, and observe <code>cudaErrorLaunchFailure</code>. Note the error line number at line 26 because kernel execution is asynchronous.</p>
          <p>The <code>checkCudaErrors</code> macro, as listed below, is defined in <code>/usr/local/cuda/samples/common/inc/helper_cuda.h</code>. In case a CUDA host call returns an error, it will output the proper CUDA error string to stderr, calls <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1gdcc2c6f914eb9461565b12648faa5e28">cudaDeviceReset</a> to destroy all allocations and reset all state on the current device in the current process, and exits the program. Try to define your own <code>checkCudaErrors</code> macro for your particular applications.</p>
          <pre id="checkError/checkCudaErrors.h"></pre>
          <h2 id="matrixMul">matrixMul</h2>
          <p>This sample uses shared memory to accelerate matrix multiplication.</p>
          <pre id="matrixMul/matrixMul.cu"></pre>
          <p>Try using <code>double</code> instead of <code>float</code>.</p>
          <p>Try using <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1gac27b566beee1aa9175373bb9e29b8d1">cudaDeviceSetCacheConfig</a> or <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EXECUTION.html#group__CUDART__EXECUTION_1g4f35d04be20a41c5df96613a748eecc1">cudaFuncSetCacheConfig</a> to set the sizes of L1 cache and shared memory out of 64KB.</p>
          <p>Try using <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g1a4789fb687cc36dccc98f25c96f0cd8">cudaDeviceSetSharedMemConfig</a> or <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EXECUTION.html#group__CUDART__EXECUTION_1gcb2e02dc6c75e4f705dfd2252748fe99">cudaFuncSetSharedMemConfig</a> to set shared memory bank width to be eight bytes natively.</p>
          <p>Try using <a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared">dynamic shared memory</a> which can have a dynamic size at run time.</p>
          <p>Try using <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g80d689bc903792f906e49be4a0b6d8db">cudaMallocPitch</a> to allocate a 2D array and <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g17f3a55e8c9aef5f90b67cdf22851375">cudaMemcpy2D</a> to copy a 2D array.</p>
          <p>Try using <a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#execution-configuration">memory fence functions</a> instead of <a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#synchronization-functions">synchronization functions</a>.</p>
          <p>Try rewriting the previous <a href="#bandwidthTest">bandwidthTest</a> sample using <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html#group__CUDART__EVENT_1ge31b1b1558db52579c9e23c5782af93e">cudaEventRecord</a> and <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html#group__CUDART__EVENT_1g14c387cc57ce2e328f6669854e6020a5">cudaEventElapsedTime</a> instead of <code>shrDeltaT</code>.</p>
          <h2 id="atomicAdd">atomicAdd</h2>
          <p>This sample uses <a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#atomic-functions">atomic functions</a>, <a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#assertion">assertions</a> and <a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#formatted-output">printf</a>.</p>
          <pre id="atomicAdd/atomicAdd.cu"></pre>
          <p>Here are four possible output.</p>
<pre class="sh">
blockIdx.x = 0, sum = 1
blockIdx.x = 1, sum = 4
sum = 13
</pre>
<pre class="sh">
blockIdx.x = 1, sum = 1
blockIdx.x = 0, sum = 4
sum = 13
</pre>
<pre class="sh">
blockIdx.x = 0, sum = 4
blockIdx.x = 1, sum = 1
sum = 13
</pre>
<pre class="sh">
blockIdx.x = 1, sum = 1
blockIdx.x = 0, sum = 7
sum = 13
</pre>
          <p>Try different numbers of threads per block and different numbers of blocks per grid.</p>
          <p>Try atomic functions on shared memory, and compare the performance to atomic functions on global memory.</p>
          <p>Refer to <a href="http://on-demand.gputechconf.com/gtc/2013/presentations/S3101-Atomic-Memory-Operations.pdf">Understanding and Using Atomic Memory Operations</a> for more detail.</p>
          <h2 id="asyncEngine">asyncEngine</h2>
          <p>This sample uses asynchronous engines to overlap data transfer and kernel execution.</p>
          <pre id="asyncEngine/asyncEngine.cu"></pre>
          <h2 id="hyperQ">hyperQ</h2>
          <p>This sample uses multiple streams to overlap multiple kernel execution, known as the <a href="http://docs.nvidia.com/cuda/samples/6_Advanced/simpleHyperQ/doc/HyperQ.pdf">HyperQ</a> technology.</p>
          <pre id="hyperQ/hyperQ.cu"></pre>
          <p>Try different values of <code>numMilliseconds</code>, <code>numKernels</code> and <code>numStreams</code>.</p>
          <p>Try different numbers of threads per block, different numbers of blocks per grid, and different sizes of dynamic shared memory.</p>
          <p>Try double buffering.</p>
          <h2 id="deviceQuery">deviceQuery</h2>
          <p>This sample enumerates some properties of the CUDA devices present in the system.</p>
          <pre id="deviceQuery/deviceQuery.cu"></pre>
          <p>Try enumerating all the properties returned by <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g929dcd9a191e17b7498e7ccaa3d16350">cudaGetDeviceProperties</a> of all the devices.</p>
          <h2 id="multiDevice">multiDevice</h2>
          <p>This sample uses <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g418c299b069c4803bfb7cab4943da383">cudaSetDevice</a> within a single thread to utilize multiple GPUs.</p>
          <pre id="multiDevice/multiDevice.cu"></pre>
          <p>Try using <code>valgrind</code> to detect memory leak in the following 3 cases of cleanup.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~/cudart/multiDevice></b> valgrind ./multiDevice
</pre>
          <p>Case 1: <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1gdcc2c6f914eb9461565b12648faa5e28">cudaDeviceReset</a> is called on every device.</p>
<pre class="cpp">
	for (int i = 0; i &lt; numDevices; ++i)
	{
		cudaSetDevice(i);
		cudaDeviceReset();
	}
</pre>
<pre class="sh">
==53804== HEAP SUMMARY:
==53804==     in use at exit: 343,139 bytes in 119 blocks
==53804==   total heap usage: 21,298 allocs, 21,179 frees, 15,813,174 bytes allocated
==53804== 
==53804== LEAK SUMMARY:
==53804==    definitely lost: 0 bytes in 0 blocks
==53804==    indirectly lost: 0 bytes in 0 blocks
==53804==      possibly lost: 2,992 bytes in 22 blocks
==53804==    still reachable: 340,147 bytes in 97 blocks
==53804==         suppressed: 0 bytes in 0 blocks
</pre>
          <p>Case 2: <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1gdcc2c6f914eb9461565b12648faa5e28">cudaDeviceReset</a> is called on one device.</p>
<pre class="cpp">
//	for (int i = 0; i &lt; numDevices; ++i)
//	{
//		cudaSetDevice(i);
		cudaDeviceReset();
//	}
</pre>
<pre class="sh">
==53860== HEAP SUMMARY:
==53860==     in use at exit: 7,031,081 bytes in 11,771 blocks
==53860==   total heap usage: 21,282 allocs, 9,511 frees, 15,812,934 bytes allocated
==53860== 
==53860== LEAK SUMMARY:
==53860==    definitely lost: 0 bytes in 0 blocks
==53860==    indirectly lost: 0 bytes in 0 blocks
==53860==      possibly lost: 51,984 bytes in 380 blocks
==53860==    still reachable: 6,979,097 bytes in 11,391 blocks
==53860==         suppressed: 0 bytes in 0 blocks
</pre>
          <p>Case 3: <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1gdcc2c6f914eb9461565b12648faa5e28">cudaDeviceReset</a> is not called at all.</p>
<pre class="cpp">
//	for (int i = 0; i &lt; numDevices; ++i)
//	{
//		cudaSetDevice(i);
//		cudaDeviceReset();
//	}
</pre>
<pre class="sh">
==53911== HEAP SUMMARY:
==53911==     in use at exit: 10,393,620 bytes in 17,742 blocks
==53911==   total heap usage: 21,274 allocs, 3,532 frees, 15,812,814 bytes allocated
==53911== 
==53911== LEAK SUMMARY:
==53911==    definitely lost: 0 bytes in 0 blocks
==53911==    indirectly lost: 0 bytes in 0 blocks
==53911==      possibly lost: 78,656 bytes in 575 blocks
==53911==    still reachable: 10,314,964 bytes in 17,167 blocks
==53911==         suppressed: 0 bytes in 0 blocks
</pre>
          <p>Refer to <a href="http://on-demand.gputechconf.com/gtc/2013/presentations/S3465-Multi-GPU-Programming.pdf">Multi-GPU Programming</a> for more detail.</p>
          <h2 id="openmp">openmp</h2>
          <p>This sample uses OpenMP to create multiple CPU threads to utilize multiple GPUs.</p>
          <pre id="openmp/openmp.cu"></pre>
          <p>In the Makefile, pass the compiler option <code>-Xcompiler -fopenmp</code> to <code>nvcc</code> in both compilation and linking.</p>
          <pre id="openmp/Makefile"></pre>
          <p>Try <code>omp_set_num_threads(numDevices*2);</code> to create twice as many CPU threads as there are CUDA devices. In this case two threads will be allocating resources and launching kernels on the same device.</p>
          <p>Refer to <a href="http://on-demand.gputechconf.com/gtc/2013/webinar/gtc-express-webinar-openacc-vs-openmp.pdf">OpenACC 2.0 vs OpenMP 4.0 Programming Comparison</a> for more detail.</p>
          <h2 id="mpi">mpi</h2>
          <p>This sample uses MPI to create multiple CPU processes to utilize multiple GPUs.</p>
          <pre id="mpi/mpi.cu"></pre>
          <p>In the Makefile, pass the compiler option <code>-I /opt/mvapich2-2.0b/include</code> to <code>nvcc</code> in compilation and <code>-L /opt/mvapich2-2.0b/lib -lmpich</code> to <code>nvcc</code> in linking.</p>
          <pre id="mpi/Makefile"></pre>
          <p>Specify the hosts to use in a host file.</p>
<pre id="mpi/hosts">
<b class="text-info">hjli@hpc1:~/cudart/mpi></b> cat hosts
</pre>
          <p>Run the executable with <code>mpirun_rsh</code>. Specify the host file and the number of processes to run with the option <code>-n</code>.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~/cudart/mpi></b> mpirun_rsh -hostfile hosts -n 4 ./mpi
</pre>
          <p>Try using <code>float</code> instead of <code>int</code>.</p>
          <p>Try writing a reduce kernel to compute the sum of the current node.</p>
          <p>Refer to <a href="http://on-demand.gputechconf.com/gtc/2013/presentations/S3047-Intro-CUDA-Aware-MPI-NVIDIA-GPUDirect.pdf">Introduction to CUDA-aware MPI and NVIDIA GPUDirect</a>, <a href="http://on-demand.gputechconf.com/gtc/2013/presentations/S3266-GPUDirect-RDMA-Green-Multi-GPU-Architectures.pdf">GPUDirect RDMA and Green Multi-GPU Architectures</a>, <a href="http://on-demand.gputechconf.com/gtc/2013/presentations/S3316-MVAPICH2-High-Performance-MPI-Library.pdf">MVAPICH2: A High Performance MPI Library for NVIDIA GPU Clusters with InfiniBand </a> and <a href="http://on-demand.gputechconf.com/gtc/2013/webinar/gtc-express-gpudirect-rdma.pdf">Accelerating High Performance Computing with GPUDirect RDMA</a> for more detail.</p>
          <h2 id="cublas">cublas</h2>
          <p>This sample uses <a href="http://docs.nvidia.com/cuda/cublas/">CUBLAS</a>, a CUDA implementation of BLAS (Basic Linear Algebra Subprograms), for matrix multiplication.</p>
          <pre id="cublas/cublas.cu"></pre>
          <p><a href="http://docs.nvidia.com/cuda/cufft/">CUFFT</a> is a CUDA implementation of FFT (Fast Fourier Transform).</p>
          <p><a href="http://docs.nvidia.com/cuda/curand/">CURAND</a> is a CUDA implementation of PRNG (PseudoRandom Number Generator) and QRNG (QuasiRandom Number Generator).</p>
          <p><a href="http://docs.nvidia.com/cuda/cusparse/">CUSPARSE</a> is a CUDA implementation of basic linear algebra subroutines for sparse matrices.</p>
          <p><a href="https://developer.nvidia.com/magma">MAGMA</a> is a third-party CUDA implementation of next generation LA (Linear Algebra) by the team that developed LAPACK and ScaLAPACK.</p>
          <p><a href="https://developer.nvidia.com/paralution">PARALUTION</a> is a third-party CUDA implementation of sparse iterative methods.</p>
          <p><a href="http://viennacl.sourceforge.net">ViennaCL</a> is a free open-source linear algebra library for computations on many-core architectures (GPUs, MIC) and multi-core CPUs.</p>
          <p>More GPU-accelerated libraries can be found at <a href="https://developer.nvidia.com/gpu-accelerated-libraries">https://developer.nvidia.com/gpu-accelerated-libraries</a>.</p>
          <p>Refer to <a href="http://on-demand.gputechconf.com/gtc/2013/presentations/S3461-CUDA-Accelerated-Compute-Libraries.pdf">CUDA Accelerated Compute Libraries</a> and <a href="http://on-demand.gputechconf.com/gtc-express/2013/presentations/cuda--5.0-math-libraries-performance.pdf">CUDA 5 Math Library Performance Overview</a> for more detail.</p>
          <h2 id="thrust">thrust</h2>
          <p>This sample uses <a href="http://docs.nvidia.com/cuda/thrust/">thrust</a>, a CUDA implementation of STL (Standard Template Library), for vector reduction.</p>
          <pre id="thrust/thrust.cu"></pre>
          <p><a href="http://www.accelereyes.com/products/arrayfire">ArrayFire</a> is a fast software library for GPU computing with an easy-to-use API. Refer to <a href="http://on-demand.gputechconf.com/gtc/2013/webinar/gtc-express-arrayfire-for-defense-intelligence-applications.pdf">ArrayFire: A Productive GPU Software Library for Defense and Intelligence Applications</a> for more detail.</p>
          <p><a href="https://github.com/ddemidov/vexcl">VexCL</a> is a vector expression template library for OpenCL/CUDA.</p>
          <p><a href="https://github.com/kylelutz/compute">Boost.Compute</a> is a GPU/parallel-computing library for C++ based on OpenCL.</p>
        </div>
      </div>
    </section>
    <section>
      <div class="page-header">
        <h1>Tools</h1>
      </div>
      <div class="row">
        <div class="col-md-12">
          <h2 id="nvcc">Compiler: nvcc</h2>
          <p>This figure shows the internal structure of the various CUDA compilation phases. It is reprinted from <a href="http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/graphics/cuda-compilation-from-cu-to-cu-cpp-ii.png">http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/graphics/cuda-compilation-from-cu-to-cu-cpp-ii.png</a>.</p>
          <p><img class="img-responsive" src="http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/graphics/cuda-compilation-from-cu-to-cu-cpp-ii.png" alt="nvcc"></p>
          <p>The <code>cudafe++</code> front end preprocessor will be called multiple times.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~></b> echo "int main() { int dummy; }" > test.cu
<b class="text-info">hjli@hpc1:~></b> nvcc -arch=sm_35 test.cu
test.cu(1): warning: variable "dummy" was declared but never referenced

test.cu(1): warning: variable "dummy" was declared but never referenced

</pre>
          <p>The <code>-ptx</code> option generates <a href="http://docs.nvidia.com/cuda/parallel-thread-execution/index.html">ptx</a>. The <code>-cubin</code> option generates cubin. The <code>-fatbin</code> option generates fatbin.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~/cudart/vectorAdd></b> nvcc -arch=sm_35 -ptx vectorAdd.cu
<b class="text-info">hjli@hpc1:~/cudart/vectorAdd></b> nvcc -arch=sm_35 -cubin vectorAdd.ptx
<b class="text-info">hjli@hpc1:~/cudart/vectorAdd></b> nvcc -arch=sm_35 -fatbin vectorAdd.cubin
<b class="text-info">hjli@hpc1:~/cudart/vectorAdd></b> ls -l vectorAdd.*
-rw-rw-r--. 1 hjli hjli   1893 Oct 18 03:17 vectorAdd.ptx
-rw-rw-r--. 1 hjli hjli   1868 Oct 18 03:18 vectorAdd.cubin
-rw-rw-r--. 1 hjli hjli   2680 Oct 18 03:20 vectorAdd.fatbin
</pre>
          <p><code>ptxas</code> assemblies ptx into object. The option <code>-v</code> prints code generation statistics.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~/cudart/vectorAdd></b> ptxas -arch=sm_35 -v vectorAdd.ptx
ptxas info    : 0 bytes gmem
ptxas info    : Compiling entry function '_Z9vectorAddPKfS0_Pfi' for 'sm_35'
ptxas info    : Function properties for _Z9vectorAddPKfS0_Pfi
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 10 registers, 348 bytes cmem[0]
</pre>
          <p>The <a href="http://docs.nvidia.com/cuda/cuda-driver-api/">CUDA Driver API</a> can JIT (Just-In-Time) compile ptx for forward compatibility. It is much analogue to <a href="http://www.khronos.org/opencl">OpenCL</a>. Refer to <a href="https://github.com/HongjianLi/cuda">https://github.com/HongjianLi/cuda</a> and <a href="https://github.com/HongjianLi/opencl">https://github.com/HongjianLi/opencl</a> for samples.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~></b> git clone https://github.com/HongjianLi/cuda.git
<b class="text-info">hjli@hpc1:~></b> git clone https://github.com/HongjianLi/opencl.git
</pre>
          <p>Refer to <a href="http://on-demand.gputechconf.com/gtc/2013/webinar/cuda-toolkit-as-build-tool.pdf">Introduction to the CUDA Toolkit as an Application Build Tool</a> and <a href="http://on-demand.gputechconf.com/gtc-express/2012/presentations/gpu-object-linking.pdf">GPU Object Linking: Usage and Benefits</a> for more detail.</p>
          <h2 id="cuda-gdb">Debugger: cuda-gdb & cuda-gdbserver</h2>
          <p>Compile an application with the options <code>-g</code> and <code>-G</code>. They embed debug information for the host code and the device code, respectively.</p>
<pre id="vectorAdd/vectorAdd.gdb">
<b class="text-info">hjli@hpc1:~/cudart/vectorAdd></b> nvcc -arch=sm_35 -g -G vectorAdd.cu
<b class="text-info">hjli@hpc1:~/cudart/vectorAdd></b> cuda-gdb ./vectorAdd
</pre>
          <p>Remote debugging is also possible.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~/cudart/vectorAdd></b> cuda-gdbserver :3001 ./vectorAdd
Process ./vectorAdd created; pid = 48918
Listening on port 3001
</pre>
<pre class="sh">
<b class="text-info">hjli@pc90124:~/cudart/vectorAdd></b> cuda-gdb ./vectorAdd
NVIDIA (R) CUDA Debugger
5.5 release
Portions Copyright (C) 2007-2013 NVIDIA Corporation
GNU gdb (GDB) 7.2
Copyright (C) 2010 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type "show copying"
and "show warranty" for details.
This GDB was configured as "x86_64-unknown-linux-gnu".
For bug reporting instructions, please see:
&lt;http://www.gnu.org/software/gdb/bugs/&gt;...
Reading symbols from /home/hjli/cudart/vectorAdd/vectorAdd...done.
(cuda-gdb) target remote hpc1:3001
</pre>
          <p>Refer to <a href="http://on-demand.gputechconf.com/gtc/2013/presentations/S3037-S3038-Debugging-CUDA-Apps-Linux-Mac.pdf">Debugging CUDA Applications on Linux and Mac</a> for more detail.</p>
          <h2 id="nvprof">Profilers: nvprof & nvvp</h2>
          <p><code>nvprof</code> is a command-line profiler. <code>nvvp</code> is a visual profiler.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~/cudart/asyncEngine></b> nvprof -o %h-%p.nvprof ./asyncEngine
<b class="text-info">hjli@hpc1:~/cudart/asyncEngine></b> nvprof -i hpc1-47304.nvprof --print-api-trace
<b class="text-info">hjli@hpc1:~/cudart/asyncEngine></b> nvprof -i hpc1-47304.nvprof --print-gpu-trace
======== Profiling result:
   Start  Duration            Grid Size      Block Size     Regs*    SSMem*    DSMem*      Size  Throughput           Device   Context    Stream  Name
     0ns  1.0068ms                    -               -         -         -         -  6.2915MB  6.2493GB/s   Tesla K20m (0)         1         8  [CUDA memcpy HtoD]
1.0097ms  1.0061ms                    -               -         -         -         -  6.2915MB  6.2530GB/s   Tesla K20m (0)         1         8  [CUDA memcpy HtoD]
2.3197ms  1.0087ms           (6144 1 1)       (256 1 1)        10        0B        0B         -           -   Tesla K20m (0)         1         8  vectorAdd(float const *, float const *, float*, int) [260]
3.1087ms  1.0721ms                    -               -         -         -         -  6.2915MB  5.8686GB/s   Tesla K20m (0)         1         9  [CUDA memcpy HtoD]
3.3364ms  1.1404ms                    -               -         -         -         -  6.2915MB  5.5170GB/s   Tesla K20m (0)         1         8  [CUDA memcpy DtoH]
4.1925ms  1.0492ms                    -               -         -         -         -  6.2915MB  5.9963GB/s   Tesla K20m (0)         1         9  [CUDA memcpy HtoD]
5.5256ms  1.0195ms           (6144 1 1)       (256 1 1)        10        0B        0B         -           -   Tesla K20m (0)         1         9  vectorAdd(float const *, float const *, float*, int) [272]
6.2483ms  1.0644ms                    -               -         -         -         -  6.2915MB  5.9109GB/s   Tesla K20m (0)         1        10  [CUDA memcpy HtoD]
6.5525ms  1.1311ms                    -               -         -         -         -  6.2915MB  5.5621GB/s   Tesla K20m (0)         1         9  [CUDA memcpy DtoH]
7.3243ms  1.0433ms                    -               -         -         -         -  6.2915MB  6.0304GB/s   Tesla K20m (0)         1        10  [CUDA memcpy HtoD]
8.6571ms  1.0206ms           (6144 1 1)       (256 1 1)        10        0B        0B         -           -   Tesla K20m (0)         1        10  vectorAdd(float const *, float const *, float*, int) [284]
9.4258ms  1.0687ms                    -               -         -         -         -  6.2915MB  5.8870GB/s   Tesla K20m (0)         1        11  [CUDA memcpy HtoD]
9.6892ms  1.1271ms                    -               -         -         -         -  6.2915MB  5.5821GB/s   Tesla K20m (0)         1        10  [CUDA memcpy DtoH]
10.500ms  1.0390ms                    -               -         -         -         -  6.2915MB  6.0553GB/s   Tesla K20m (0)         1        11  [CUDA memcpy HtoD]
11.825ms  1.0113ms           (6144 1 1)       (256 1 1)        10        0B        0B         -           -   Tesla K20m (0)         1        11  vectorAdd(float const *, float const *, float*, int) [296]
12.582ms  1.0635ms                    -               -         -         -         -  6.2915MB  5.9157GB/s   Tesla K20m (0)         1        12  [CUDA memcpy HtoD]
12.844ms  1.1336ms                    -               -         -         -         -  6.2915MB  5.5500GB/s   Tesla K20m (0)         1        11  [CUDA memcpy DtoH]
13.658ms  1.0523ms                    -               -         -         -         -  6.2915MB  5.9790GB/s   Tesla K20m (0)         1        12  [CUDA memcpy HtoD]
14.996ms  1.0189ms           (6144 1 1)       (256 1 1)        10        0B        0B         -           -   Tesla K20m (0)         1        12  vectorAdd(float const *, float const *, float*, int) [308]
15.768ms  1.0589ms                    -               -         -         -         -  6.2915MB  5.9413GB/s   Tesla K20m (0)         1        13  [CUDA memcpy HtoD]
16.025ms  1.1395ms                    -               -         -         -         -  6.2915MB  5.5215GB/s   Tesla K20m (0)         1        12  [CUDA memcpy DtoH]
16.832ms  1.0561ms                    -               -         -         -         -  6.2915MB  5.9575GB/s   Tesla K20m (0)         1        13  [CUDA memcpy HtoD]
18.188ms  1.0209ms           (6144 1 1)       (256 1 1)        10        0B        0B         -           -   Tesla K20m (0)         1        13  vectorAdd(float const *, float const *, float*, int) [320]
18.985ms  1.0678ms                    -               -         -         -         -  6.2915MB  5.8921GB/s   Tesla K20m (0)         1        14  [CUDA memcpy HtoD]
19.218ms  1.1262ms                    -               -         -         -         -  6.2915MB  5.5863GB/s   Tesla K20m (0)         1        13  [CUDA memcpy DtoH]
20.062ms  1.0418ms                    -               -         -         -         -  6.2915MB  6.0391GB/s   Tesla K20m (0)         1        14  [CUDA memcpy HtoD]
21.375ms  1.0101ms           (6144 1 1)       (256 1 1)        10        0B        0B         -           -   Tesla K20m (0)         1        14  vectorAdd(float const *, float const *, float*, int) [332]
22.029ms  1.1190ms                    -               -         -         -         -  6.2915MB  5.6225GB/s   Tesla K20m (0)         1        15  [CUDA memcpy HtoD]
22.393ms  1.1847ms                    -               -         -         -         -  6.2915MB  5.3106GB/s   Tesla K20m (0)         1        14  [CUDA memcpy DtoH]
23.154ms  1.0563ms                    -               -         -         -         -  6.2915MB  5.9560GB/s   Tesla K20m (0)         1        15  [CUDA memcpy HtoD]
24.470ms  1.0200ms           (6144 1 1)       (256 1 1)        10        0B        0B         -           -   Tesla K20m (0)         1        15  vectorAdd(float const *, float const *, float*, int) [344]
25.493ms  942.27us                    -               -         -         -         -  6.2915MB  6.6769GB/s   Tesla K20m (0)         1        15  [CUDA memcpy DtoH]

Regs: Number of registers used per CUDA thread.
SSMem: Static shared memory allocated per CUDA block.
DSMem: Dynamic shared memory allocated per CUDA block.
</pre>
          <p><code>nvvp</code> can import *.nvprof and visualize the timeline data. File -> Import -> Nvprof -> Timeline data file -> Finish. Zoom in the main part.</p>
          <p><img class="img-responsive" src="hpc1-47304.png" alt="nvprof"></p>
          <p>Refer to <a href="http://on-demand.gputechconf.com/gtc/2013/webinar/gtc-express-guided-analysis-nvidia-visual-profiler.pdf">Guided Performance Analysis with NVIDIA Visual Profiler</a>, <a href="http://on-demand.gputechconf.com/gtc/2012/presentations/S0419B-GTC2012-Profiling-Profiling-Tools.pdf">Optimizing Application Performance with CUDA Profiling Tools</a> and <a href="http://on-demand.gputechconf.com/gtc/2013/presentations/S3046-Performance-Optimization-Strategies-for-GPU-Accelerated-Apps.pdf">Performance Optimization Strategies For GPU-Accelerated Applications</a> for more detail.</p>
          <h2 id="nsight">IDE: Nsight</h2>
          <p><a href="https://developer.nvidia.com/nvidia-nsight-visual-studio-edition">Visual Studio Edition for Windows</a> and <a href="https://developer.nvidia.com/nsight-eclipse-edition">Eclipse Edition for Linux</a>.</p>
          <p>Refer to <a href="http://on-demand.gputechconf.com/gtc/2013/presentations/S3011-CUDA-Optimization-With-Nsight-VSE.pdf">CUDA Optimization with NVIDIA Nsight Visual Studio Edition 3.0</a>, <a href="http://on-demand.gputechconf.com/gtc/2013/presentations/S3478-Debugging-CUDA-Kernel-Code.pdf">Debugging CUDA Kernel Code with NVIDIA Nsight Visual Studio Edition</a> and <a href="http://on-demand.gputechconf.com/gtc-express/2012/presentations/nsight-eclipse-edition.pdf">Nsight Eclipse Edition: High Productivity IDE for CUDA Development on Linux & MacOS</a> for more detail.</p>
          <h2 id="CUDA_Occupancy_Calculator.xls">CUDA_Occupancy_Calculator.xls</h2>
          <p>It calculates the theoretical percentage for which kernels occupy a device. The higher the occupancy, the higher the GPU utilization. The Excel file is located in <code>/usr/local/cuda/tools</code>.</p>
          <h2 id="nvidia-smi">Monitor: nvidia-smi</h2>
          <p><a href="https://developer.nvidia.com/nvidia-system-management-interface">nvidia-smi</a> allows administrators to query GPU device state and with the appropriate privileges, permits administrators to modify GPU device state.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~></b> nvidia-smi
Tue Sep 10 10:39:25 2013       
+------------------------------------------------------+                       
| NVIDIA-SMI 5.319.37   Driver Version: 319.37         |                       
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K20m          Off  | 0000:08:00.0     Off |                    0 |
| N/A   35C    P0    42W / 225W |       11MB /  4799MB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla K20m          Off  | 0000:24:00.0     Off |                    0 |
| N/A   41C    P0    44W / 225W |       11MB /  4799MB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla K20m          Off  | 0000:27:00.0     Off |                    0 |
| N/A   35C    P0    39W / 225W |       11MB /  4799MB |     78%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Compute processes:                                               GPU Memory |
|  GPU       PID  Process name                                     Usage      |
|=============================================================================|
|  No running compute processes found                                         |
+-----------------------------------------------------------------------------+
</pre> 
          <p><a href="https://developer.nvidia.com/nvidia-management-library-nvml">NVML</a> is a C-based API for monitoring and managing various states of the NVIDIA GPU devices. It provides a direct access to the queries and commands exposed via <a href="https://developer.nvidia.com/nvidia-system-management-interface">nvidia-smi</a>. They both are parts of the <a href="https://developer.nvidia.com/tesla-deployment-kit">Tesla Deployment Kit</a>. Refer to <a href="http://on-demand.gputechconf.com/gtc/2013/presentations/S3044-GPUs-in-Cluster-Environments.pdf">Monitoring and Managing NVIDIA GPUs in Cluster Environments</a> and <a href="http://on-demand.gputechconf.com/gtc-express/2012/presentations/deploying-managing-gpu-clusters.pdf">Best Practices for Deploying and Managing GPU Clusters</a> for more detail.</p>
          <h2 id="cuda-memcheck">Sanitizer: cuda-memcheck</h2>
          <p><a href="http://docs.nvidia.com/cuda/cuda-memcheck/">CUDA-MEMCHECK</a> is a functional correctness checking suite. The demo code here suffers from unaligned access and out-of-bound access.</p>
          <pre id="memcheckDemo/memcheckDemo.cu"></pre>
          <p>Compile the demo code with option <code>-Xcompiler -rdynamic</code>, and invoke <code>cuda-memcheck</code>.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~/cudart/memcheckDemo></b> cuda-memcheck ./memcheckDemo
========= CUDA-MEMCHECK
Mallocing memory
Running unaligned_kernel
Ran unaligned_kernel: no error
========= Invalid __global__ write of size 4
=========     at 0x00000028 in unaligned_kernel(void)
=========     by thread (0,0,0) in block (0,0,0)
=========     Address 0x2300200001 is misaligned
=========     Saved host backtrace up to driver entry point at kernel launch time
=========     Host Frame:/usr/lib64/libcuda.so (cuLaunchKernel + 0x331) [0xcd5d1]
=========     Host Frame:./memcheckDemo [0x1bfb8]
=========     Host Frame:./memcheckDemo [0x3b483]
=========     Host Frame:./memcheckDemo (_ZN71_GLOBAL__N__47_tmpxft_0000c21b_00000000_6_memcheckDemo_cpp1_ii_a1aea87b10cudaLaunchIcEE9cudaErrorPT_ + 0x18) [0x325e]
=========     Host Frame:./memcheckDemo (_Z35__device_stub__Z16unaligned_kernelvv + 0x19) [0x30c3]
=========     Host Frame:./memcheckDemo (_Z16unaligned_kernelv + 0x9) [0x30ce]
=========     Host Frame:./memcheckDemo (_Z13run_unalignedv + 0x75) [0x2f22]
=========     Host Frame:./memcheckDemo (main + 0x28) [0x304f]
=========     Host Frame:/lib64/libc.so.6 (__libc_start_main + 0xfd) [0x1ecdd]
=========     Host Frame:./memcheckDemo [0x2c79]
=========
========= Program hit error 4 on CUDA API call to cudaThreadSynchronize 
=========     Saved host backtrace up to driver entry point at error
=========     Host Frame:/usr/lib64/libcuda.so [0x26d660]
=========     Host Frame:./memcheckDemo [0x39f46]
=========     Host Frame:./memcheckDemo (_Z13run_unalignedv + 0x98) [0x2f45]
=========     Host Frame:./memcheckDemo (main + 0x28) [0x304f]
=========     Host Frame:/lib64/libc.so.6 (__libc_start_main + 0xfd) [0x1ecdd]
=========     Host Frame:./memcheckDemo [0x2c79]
=========
Sync: unspecified launch failure
Running out_of_bounds_kernel
========= Program hit error 4 on CUDA API call to cudaLaunch 
=========     Saved host backtrace up to driver entry point at error
=========     Host Frame:/usr/lib64/libcuda.so [0x26d660]
=========     Host Frame:./memcheckDemo [0x3b4be]
=========     Host Frame:./memcheckDemo (_ZN71_GLOBAL__N__47_tmpxft_0000c21b_00000000_6_memcheckDemo_cpp1_ii_a1aea87b10cudaLaunchIcEE9cudaErrorPT_ + 0x18) [0x325e]
=========     Host Frame:./memcheckDemo (_Z39__device_stub__Z20out_of_bounds_kernelvv + 0x19) [0x30e9]
=========     Host Frame:./memcheckDemo (_Z20out_of_bounds_kernelv + 0x9) [0x30f4]
=========     Host Frame:./memcheckDemo (_Z17run_out_of_boundsv + 0x75) [0x2fdf]
=========     Host Frame:./memcheckDemo (main + 0x2d) [0x3054]
=========     Host Frame:/lib64/libc.so.6 (__libc_start_main + 0xfd) [0x1ecdd]
=========     Host Frame:./memcheckDemo [0x2c79]
=========
========= Program hit error 4 on CUDA API call to cudaGetLastError 
=========     Saved host backtrace up to driver entry point at error
Ran out_of_bounds_kernel: unspecified launch failure
=========     Host Frame:/usr/lib64/libcuda.so [0x26d660]
=========     Host Frame:./memcheckDemo [0x39723]
=========     Host Frame:./memcheckDemo (_Z17run_out_of_boundsv + 0x7a) [0x2fe4]
=========     Host Frame:./memcheckDemo (main + 0x2d) [0x3054]
=========     Host Frame:/lib64/libc.so.6 (__libc_start_main + 0xfd) [0x1ecdd]
=========     Host Frame:./memcheckDemo [0x2c79]
=========
Sync: unspecified launch failure
========= Program hit error 4 on CUDA API call to cudaThreadSynchronize 
=========     Saved host backtrace up to driver entry point at error
=========     Host Frame:/usr/lib64/libcuda.so [0x26d660]
=========     Host Frame:./memcheckDemo [0x39f46]
=========     Host Frame:./memcheckDemo (_Z17run_out_of_boundsv + 0x98) [0x3002]
=========     Host Frame:./memcheckDemo (main + 0x2d) [0x3054]
=========     Host Frame:/lib64/libc.so.6 (__libc_start_main + 0xfd) [0x1ecdd]
=========     Host Frame:./memcheckDemo [0x2c79]
=========
========= Program hit error 17 on CUDA API call to cudaFree 
=========     Saved host backtrace up to driver entry point at error
=========     Host Frame:/usr/lib64/libcuda.so [0x26d660]
=========     Host Frame:./memcheckDemo [0x43186]
=========     Host Frame:./memcheckDemo (main + 0x3e) [0x3065]
=========     Host Frame:/lib64/libc.so.6 (__libc_start_main + 0xfd) [0x1ecdd]
=========     Host Frame:./memcheckDemo [0x2c79]
=========
========= ERROR SUMMARY: 6 errors
</pre>
          <p>Specify the option <code>--leak-check full</code> to print information about the allocations that have not been freed at the time the CUDA context is destroyed.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~/cudart/memcheckDemo></b> cuda-memcheck --leak-check full ./memcheckDemo
</pre>
          <p>Specify the option <code>--tool racecheck</code> to help identify memory access race conditions in CUDA applications that use shared memory.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~/cudart/memcheckDemo></b> cuda-memcheck --tool racecheck ./memcheckDemo
</pre>
          <h2 id="nvdisasm">Disassemblers: cuobjdump & nvdisasm</h2>
          <p><a href="http://docs.nvidia.com/cuda/cuda-binary-utilities/index.html#cuobjdump">cuobjdump</a> extracts information from CUDA binary files (both standalone and those embedded in host binaries) and presents them in human readable format.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~/cudart/vectorAdd></b> cuobjdump ./vectorAdd.fatbin

Fatbin elf code:
================
arch = sm_35
code version = [1,7]
producer = cuda
host = linux
compile_size = 64bit
identifier = vectorAdd.cu

Fatbin ptx code:
================
arch = sm_35
code version = [3,2]
producer = cuda
host = linux
compile_size = 64bit
compressed
identifier = vectorAdd.cu
</pre>
          <p><a href="http://docs.nvidia.com/cuda/cuda-binary-utilities/index.html#nvdisasm">nvdisasm</a> extracts information from standalone cubin files and presents them in human readable format.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~/cudart/vectorAdd></b> nvdisasm ./vectorAdd.cubin
	.headerflags	@"EF_CUDA_TEXMODE_UNIFIED EF_CUDA_64BIT_ADDRESS EF_CUDA_SM35 EF_CUDA_PTX_SM(EF_CUDA_SM35) "


//--------------------- .nv.info                  --------------------------
	.section	.nv.info,"",@"SHT_CUDA_INFO "
	.align	4


	// ---- nvinfo : EIATTR_MIN_STACK_SIZE
	.align		4
	/*0000*/ 	.byte	0x04, 0x12
	/*0002*/ 	.short	(.L_4 - .L_3)
	.align		4
.L_3:
	/*0004*/ 	.word	index@(_Z9vectorAddPKfS0_Pfi)
	/*0008*/ 	.word	0x00000000


	// ---- nvinfo : EIATTR_FRAME_SIZE
	.align		4
.L_4:
	/*000c*/ 	.byte	0x04, 0x11
	/*000e*/ 	.short	(.L_6 - .L_5)
	.align		4
.L_5:
	/*0010*/ 	.word	index@(_Z9vectorAddPKfS0_Pfi)
	/*0014*/ 	.word	0x00000000
.L_6:


//--------------------- .nv.info._Z9vectorAddPKfS0_Pfi --------------------------
	.section	.nv.info._Z9vectorAddPKfS0_Pfi,"",@"SHT_CUDA_INFO "
	.align	4


	// ---- nvinfo : EIATTR_PARAM_CBANK
	.align		4
	/*0000*/ 	.byte	0x04, 0x0a
	/*0002*/ 	.short	(.L_8 - .L_7)
	.align		4
.L_7:
	/*0004*/ 	.word	index@(.nv.constant0._Z9vectorAddPKfS0_Pfi)
	/*0008*/ 	.short	0x0140
	/*000a*/ 	.short	0x001c


	// ---- nvinfo : EIATTR_CBANK_PARAM_SIZE
	.align		4
.L_8:
	/*000c*/ 	.byte	0x03, 0x19
	/*000e*/ 	.short	0x001c


	// ---- nvinfo : EIATTR_KPARAM_INFO
	.align		4
	/*0010*/ 	.byte	0x04, 0x17
	/*0012*/ 	.short	(.L_10 - .L_9)
.L_9:
	/*0014*/ 	.word	0x00000000
	/*0018*/ 	.short	0x0003
	/*001a*/ 	.short	0x0018
	/*001c*/ 	.byte	0x00, 0xf0, 0x11, 0x00


	// ---- nvinfo : EIATTR_KPARAM_INFO
	.align		4
.L_10:
	/*0020*/ 	.byte	0x04, 0x17
	/*0022*/ 	.short	(.L_12 - .L_11)
.L_11:
	/*0024*/ 	.word	0x00000000
	/*0028*/ 	.short	0x0002
	/*002a*/ 	.short	0x0010
	/*002c*/ 	.byte	0x00, 0xf0, 0x21, 0x00


	// ---- nvinfo : EIATTR_KPARAM_INFO
	.align		4
.L_12:
	/*0030*/ 	.byte	0x04, 0x17
	/*0032*/ 	.short	(.L_14 - .L_13)
.L_13:
	/*0034*/ 	.word	0x00000000
	/*0038*/ 	.short	0x0001
	/*003a*/ 	.short	0x0008
	/*003c*/ 	.byte	0x00, 0xf0, 0x21, 0x00


	// ---- nvinfo : EIATTR_KPARAM_INFO
	.align		4
.L_14:
	/*0040*/ 	.byte	0x04, 0x17
	/*0042*/ 	.short	(.L_16 - .L_15)
.L_15:
	/*0044*/ 	.word	0x00000000
	/*0048*/ 	.short	0x0000
	/*004a*/ 	.short	0x0000
	/*004c*/ 	.byte	0x00, 0xf0, 0x21, 0x00


	// ---- nvinfo : EIATTR_CRS_STACK_SIZE
	.align		4
.L_16:
	/*0050*/ 	.byte	0x04, 0x1e
	/*0052*/ 	.short	(.L_18 - .L_17)
.L_17:
	/*0054*/ 	.word	0x00000010


	// ---- nvinfo : EIATTR_S2RCTAID_INSTR_OFFSETS
	.align		4
.L_18:
	/*0058*/ 	.byte	0x04, 0x1d
	/*005a*/ 	.short	(.L_20 - .L_19)


	//   ....[0]....
.L_19:
	/*005c*/ 	.word	0x00000010


	// ---- nvinfo : EIATTR_EXIT_INSTR_OFFSETS
	.align		4
.L_20:
	/*0060*/ 	.byte	0x04, 0x1c
	/*0062*/ 	.short	(.L_22 - .L_21)


	//   ....[0]....
.L_21:
	/*0064*/ 	.word	0x000000a8
.L_22:


//--------------------- .nv.constant0._Z9vectorAddPKfS0_Pfi --------------------------
	.section	.nv.constant0._Z9vectorAddPKfS0_Pfi,"a",@progbits
	.align	4
.nv.constant0._Z9vectorAddPKfS0_Pfi:
	.zero		348


//--------------------- .text._Z9vectorAddPKfS0_Pfi --------------------------
	.section	.text._Z9vectorAddPKfS0_Pfi,"ax",@progbits
	.sectioninfo	@"SHI_REGISTERS=10 "
	.align	4
	.global		_Z9vectorAddPKfS0_Pfi
	.type		_Z9vectorAddPKfS0_Pfi,@function
	.size		_Z9vectorAddPKfS0_Pfi,(.L_23 - _Z9vectorAddPKfS0_Pfi)
	.other		_Z9vectorAddPKfS0_Pfi,@"STO_CUDA_ENTRY STV_DEFAULT "
_Z9vectorAddPKfS0_Pfi:
.text._Z9vectorAddPKfS0_Pfi:
        /*0008*/                MOV R1, c[0x0][0x44];
        /*0010*/                S2R R0, SR_CTAID.X;
        /*0018*/                S2R R3, SR_TID.X;
        /*0020*/                IMAD R0, R0, c[0x0][0x28], R3;
        /*0028*/                ISETP.GE.AND P0, PT, R0, c[0x0][0x158], PT;
        /*0030*/            @P0 BRA.U `(.L_1);
        /*0038*/           @!P0 MOV32I R5, 0x4;
        /*0048*/           @!P0 IMAD R6.CC, R0, R5, c[0x0][0x140];
        /*0050*/           @!P0 IMAD.HI.X R7, R0, R5, c[0x0][0x144];
        /*0058*/           @!P0 IMAD R8.CC, R0, R5, c[0x0][0x148];
        /*0060*/           @!P0 LD.E R3, [R6];
        /*0068*/           @!P0 IMAD.HI.X R9, R0, R5, c[0x0][0x14c];
        /*0070*/           @!P0 LD.E R2, [R8];
        /*0078*/           @!P0 IMAD R4.CC, R0, R5, c[0x0][0x150];
        /*0088*/           @!P0 IMAD.HI.X R5, R0, R5, c[0x0][0x154];
        /*0090*/           @!P0 FADD R0, R3, R2;
        /*0098*/           @!P0 ST.E [R4], R0;
.L_1:
        /*00a0*/                MOV RZ, RZ;
        /*00a8*/                EXIT ;
.L_2:
        /*00b0*/                BRA `(.L_2);
.L_23:
</pre>
          <p>Generate a PNG image of the control flow of vectorAdd.cubin with <a href="http://docs.nvidia.com/cuda/cuda-binary-utilities/index.html#nvdisasm">nvdisasm</a> and <a href="http://www.graphviz.org">Graphviz</a>.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~/cudart/vectorAdd></b> nvdisasm -cfg ./vectorAdd.cubin | dot -Tpng -ovectorAdd.png
</pre>
          <p><img class="img-responsive" src="vectorAdd.png" alt="vectorAdd.png"></p>
        </div>
      </div>
    </section>
  </div>
  <footer role="contentinfo">
    <div class="container">
      <p><a href="http://www.cuhk.edu.hk"><img src="/cuhk.jpg" alt="CUHK logo"></a>&copy; 2012-2013 Chinese University of Hong Kong. Platform designed by <a href="http://www.cse.cuhk.edu.hk/~hjli">Hongjian Li</a>. Code licensed under <a href="http://www.apache.org/licenses/LICENSE-2.0">Apache License 2.0</a>. Documentation licensed under <a href="http://creativecommons.org/licenses/by/3.0">CC BY 3.0</a>.<a href="http://validator.w3.org/check?uri=referer"><img src="/HTML5_Badge_512.png" alt="HTML5 logo"></a></p>
    </div>
  </footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
  <script src="/bootstrap.min.js"></script>
  <script src="/jquery.lazyload.min.js"></script>
  <script src="jquery.snippet.min.js"></script>
  <script src="sh_sh.min.js"></script>
  <script src="sh_makefile.min.js"></script>
  <script src="index.js"></script>
</body>
</html>
