<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="author" content="Hongjian Li">
  <meta name="description" content="hands-on introduction to CUDA programming">
  <title>Hands-on Introduction to CUDA Programming</title>
  <link rel="stylesheet" href="/bootstrap.min.css">
  <link rel="stylesheet" href="/jquery.snippet.min.css">
  <link rel="stylesheet" href="/index.css">
  <link rel="stylesheet" href="index.css">
  <link rel="shortcut icon" href="/favicon.ico">
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="apple-touch-icon-precomposed" sizes="114x114" href="/apple-touch-icon-114-precomposed.png">
  <link rel="apple-touch-icon-precomposed" sizes="72x72" href="/apple-touch-icon-72-precomposed.png">
  <link rel="apple-touch-icon-precomposed" href="/apple-touch-icon-57-precomposed.png">
  <script>
    var _gaq=[['_setAccount','UA-20604862-1'],['_trackPageview']];
    (function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];
    g.src=('https:'==location.protocol?'//ssl':'//www')+'.google-analytics.com/ga.js';
    s.parentNode.insertBefore(g,s)}(document,'script'));
  </script>
</head>
<body>
  <a class="sr-only" href="#content">Skip navigation</a>
  <a href="//github.com/HongjianLi/istar" class="ribbon"></a>
  <header class="navbar navbar-inverse navbar-static-top" role="banner">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
      </div>
      <nav class="collapse navbar-collapse" role="navigation">
        <ul class="nav navbar-nav">
          <li>
            <a href="/"><img src="/logo.png" alt="istar logo">istar: software as a service</a>
          </li>
          <li>
            <a href="/idock"><img src="/idock/logo.png" alt="idock logo">idock: protein-ligand docking</a>
          </li>
          <li>
            <a href="/iview"><img src="/iview/logo.png" alt="iview logo">iview: interactive WebGL visualizer</a>
          </li>
          <li>
            <a href="/igrep"><img src="/igrep/logo.png" alt="igrep logo">igrep: DNA sequence matching</a>
          </li>
          <li class="active">
            <a href="/icuda"><img src="/icuda/logo.png" alt="icuda logo">icuda: introduction to CUDA</a>
          </li>
        </ul>
      </nav>
    </div>
  </header>
  <div class="jumbotron" id="content" role="main">
    <div class="container">
      <h1><img src="logo.png" alt="logo" class="logo">icuda</h1>
      <p>hands-on introduction to CUDA programming</p>
    </div>
  </div>
  <div class="container section">
    <section>
      <div class="page-header">
        <h1>Outline</h1>
      </div>
      <div class="row">
        <div class="col-md-12">
          <p>This is a hands-on seminar series on pragmatic CUDA programming. It emphasizes <b>examples</b>, <b>libraries</b> and <b>tools</b>. This seminar series consists of 3 seminars.</p>
          <h2>Contents</h2>
          <p>Seminar 1: This seminar gives a high-level overview of GPU hardware and CUDA programming philosophy. It covers four samples including <a href="#vectorAdd">vectorAdd</a>, <a href="#zeroCopy">zeroCopy</a>, <a href="bandwidthTest">bandwidthTest</a> and <a href="#checkError">checkError</a>, and two tools including <a href="#nvcc">nvcc</a> and <a href="#cuda-gdb">cuda-gdb</a>.</p>
          <p>Seminar 2: This seminar illustrates how to exploit single-device parallelism and GPU-specific hardware components. It covers four samples including <a href="#matrixMul">matrixMul</a>, <a href="#atomicAdd">atomicAdd</a>, <a href="#asyncEngine">asyncEngine</a> and <a href="#hyperQ">hyperQ</a>, and three tools including <a href="#nvprof">nvprof</a>, <a href="#nsight">nsight</a> and <a href="#CUDA_Occupancy_Calculator.xls">CUDA_Occupancy_Calculator.xls</a>.</p>
          <p>Seminar 3: This seminar illustrates how to exploit multi-device parallelism and how to utilize libraries. It covers six samples including <a href="#deviceQuery">deviceQuery</a>, <a href="#multiDevice">multiDevice</a>, <a href="#openmp">openmp</a>, <a href="#mpi">mpi</a>, <a href="#cublas">cublas</a> and <a href="#thrust">thrust</a>, and four tools including <a href="#nvidia-smi">nvidia-smi</a>, <a href="#cuda-memcheck">cuda-memcheck</a>, <a href="#nvdisasm">cuobjdump & nvdisasm</a>.</p>
          <h2>Time & venue</h2>
          <p>Seminar 1: 07 Nov 2013, 2:30pm - 4:00pm, SHB 123</p>
          <p>Seminar 2: 14 Nov 2013, 2:30pm - 4:00pm, SHB 123</p>
          <p>Seminar 3: 21 Nov 2013, 2:30pm - 4:00pm, SHB 123</p>
          <h2>Forms</h2>
          <p><a href="GPGPU-Account.pdf">GPGPU System ACCOUNT REQUEST FORM</a></p>
          <h2>References</h2>
          <p><a href="https://developer.nvidia.com/category/zone/cuda-zone">CUDA zone</a></p>
          <p><a href="http://docs.nvidia.com/cuda">CUDA toolkit documentation</a></p>
          <p><a href="http://courses.engr.illinois.edu/ece408">UIUC course ECE408/CS483: Applied Parallel Programming</a></p>
        </div>
      </div>
    </section>
    <section>
      <div class="page-header">
        <h1>Samples</h1>
      </div>
      <div class="row">
        <div class="col-md-12">
          <p>All the samples are free and open source under Apache License 2.0. They are available at <a href="https://github.com/HongjianLi/cudart">https://github.com/HongjianLi/cudart</a>.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~></b> git clone https://github.com/HongjianLi/cudart.git
</pre>
          <p>All the samples use the <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/">CUDA Runtime API</a>.</p>
          <h2>Makefile</h2>
          <p>A sample Makefile looks like this. Instead of <code>gcc</code>, it uses <code>nvcc</code> as CUDA compiler. The option <code>-arch</code> specifies the name of the class of nVidia GPU architectures for which the cuda input files must be compiled. Our K20m GPU architecture is <code>sm_35</code>. Refer to <a href="http://on-demand.gputechconf.com/gtc-express/2012/presentations/inside-tesla-kepler-k20-family.pdf">Inside the Kepler Tesla K20 Family</a> for more detail.</p>
<pre class="makefile">
CC=nvcc -arch=sm_35

vectorAdd: vectorAdd.o
	$(CC) -o $@ $^

vectorAdd.o: vectorAdd.cu
	$(CC) -o $@ $< -c

clean:
	rm -f vectorAdd vectorAdd.o
</pre>
          <h2 id="vectorAdd">vectorAdd</h2>
          <p>This sample adds two vectors of floats.</p>
<pre class="cpp">
#include &lt;stdio.h&gt;

__global__ void vectorAdd(const float *a, const float *b, float *c, int numElements)
{
	int i = blockDim.x * blockIdx.x + threadIdx.x;
	if (i < numElements)
	{
		c[i] = a[i] + b[i];
	}
}

int main(int argc, char *argv[])
{
	int numElements = 5e+4;

	// Allocate vectors a, b and c in host memory.
	size_t numBytes = sizeof(float) * numElements;
	float *h_a = (float *)malloc(numBytes);
	float *h_b = (float *)malloc(numBytes);
	float *h_c = (float *)malloc(numBytes);

	// Initialize vectors a and b.
	for (int i = 0; i < numElements; ++i)
	{
		h_a[i] = rand() / (float)RAND_MAX;
		h_b[i] = rand() / (float)RAND_MAX;
	}

	// Allocate vectors a, b and c in device memory.
	float *d_a;
	float *d_b;
	float *d_c;
	cudaMalloc((void **)&d_a, numBytes);
	cudaMalloc((void **)&d_b, numBytes);
	cudaMalloc((void **)&d_c, numBytes);

	// Copy vectors a and b from host memory to device memory synchronously.
	cudaMemcpy(d_a, h_a, numBytes, cudaMemcpyHostToDevice);
	cudaMemcpy(d_b, h_b, numBytes, cudaMemcpyHostToDevice);

	// Determine the number of threads per block and the number of blocks per grid.
	int numThreadsPerBlock = 256;
	int numBlocksPerGrid = (numElements + numThreadsPerBlock - 1) / numThreadsPerBlock;

	// Invoke the kernel on device asynchronously.
	vectorAdd&lt;&lt;&lt;numBlocksPerGrid, numThreadsPerBlock&gt;&gt;&gt;(d_a, d_b, d_c, numElements);

	// Copy vector c from device memory to host memory synchronously.
	cudaMemcpy(h_c, d_c, numBytes, cudaMemcpyDeviceToHost);

	// Validate the result.
	for (int i = 0; i < numElements; ++i)
	{
		float actual = h_c[i];
		float expected = h_a[i] + h_b[i];
		if (fabs(actual - expected) > 1e-7)
		{
			printf("h_c[%d] = %f, expected = %f\n", i, actual, expected);
			break;
		}
	}

	// Cleanup.
	cudaFree(d_c);
	cudaFree(d_b);
	cudaFree(d_a);
	cudaDeviceReset();
	free(h_c);
	free(h_b);
	free(h_a);
}
</pre>
          <p>Try different values of <code>numElements</code> and <code>numThreadsPerBlock</code>.</p>
          <h2 id="zeroCopy">zeroCopy</h2>
          <p>This sample maps device pointers to pinned host memory so that kernels can directly read from and write to pinned host memory.</p>
          <p>On <a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#data-transfer-between-host-and-device">integrated systems</a> where device memory and host memory are physically the same, the mapping mechanism saves superfluous copies from host to device and from device to host.</p>
          <p>On discrete systems where device memory and host memory are physically different, the mapping mechanism saves explicit copies from host to device and from device to host.</p>
<pre class="cpp">
#include &lt;stdio.h&gt;

__global__ void vectorAdd(const float* a, const float* b, float* c)
{
	int i = blockDim.x * blockIdx.x + threadIdx.x;
	c[i] = a[i] + b[i];
}

int main(int argc, char *argv[])
{
	// Initialize the number of threads per block and the number of blocks per grid.
	const unsigned int numThreadsPerBlock = 256;
	const unsigned int numBlocksPerGrid = 1024;
	const unsigned int numThreadsPerGrid = numThreadsPerBlock * numBlocksPerGrid;

	// Set the flag in order to allocate pinned host memory that is accessible to the device.
	cudaSetDeviceFlags(cudaDeviceMapHost);

	// Allocate pinned vectors a, b and c in host memory with the cudaHostAllocMapped flag so that they can be accessed by the device.
	float* h_a;
	float* h_b;
	float* h_c;
	cudaHostAlloc((void**)&h_a, sizeof(float) * numThreadsPerGrid, cudaHostAllocMapped);
	cudaHostAlloc((void**)&h_b, sizeof(float) * numThreadsPerGrid, cudaHostAllocMapped);
	cudaHostAlloc((void**)&h_c, sizeof(float) * numThreadsPerGrid, cudaHostAllocMapped);

	// Initialize vectors a and b.
	for (int i = 0; i < numThreadsPerGrid; ++i)
	{
		h_a[i] = rand() / (float)RAND_MAX;
		h_b[i] = rand() / (float)RAND_MAX;
	}

	// Get the mapped pointers for the device.
	float* d_a;
	float* d_b;
	float* d_c;
	cudaHostGetDevicePointer(&d_a, h_a, 0);
	cudaHostGetDevicePointer(&d_b, h_b, 0);
	cudaHostGetDevicePointer(&d_c, h_c, 0);

	// Invoke the kernel on device asynchronously.
	vectorAdd&lt;&lt;&lt;numBlocksPerGrid, numThreadsPerBlock&gt;&gt;&gt;(d_a, d_b, d_c);

	// Wait for the device to finish.
	cudaDeviceSynchronize();

	// Validate the result.
	for (int i = 0; i < numThreadsPerGrid; ++i)
	{
		float actual = h_c[i];
		float expected = h_a[i] + h_b[i];
		if (fabs(actual - expected) > 1e-7)
		{
			printf("h_c[%d] = %f, expected = %f\n", i, actual, expected);
			break;
		}
	}

	// Cleanup.
	cudaFreeHost(h_c);
	cudaFreeHost(h_b);
	cudaFreeHost(h_a);
	cudaDeviceReset();
}
</pre>
          <p>Try passing the flag <code>cudaDeviceScheduleAuto</code> to <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g18074e885b4d89f5a0fe1beab589e0c8">cudaSetDeviceFlags</a>.</p>
          <p>Try passing the flag <code>cudaHostAllocDefault</code> to <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g15a3871f15f8c38f5b7190946845758c">cudaHostAlloc</a> to allocate the memory as non-mapped.</p>
          <h2 id="bandwidthTest">bandwidthTest</h2>
          <p>This sample measures host-to-device and device-to-host bandwidth via PCIe for pageable and pinned memory of four transfer sizes of 3KB, 15KB, 15MB and 100MB, and outputs them in CSV format.</p>
<pre class="cpp">
#include &lt;stdio.h&gt;
#include &lt;sys/time.h&gt;

// This function returns the current timestamp in seconds.
double shrDeltaT()
{
	static struct timeval old_time;
	struct timeval new_time;
	gettimeofday(&new_time, NULL);
	const double DeltaT = ((double)new_time.tv_sec + 1.0e-6 * (double)new_time.tv_usec) - ((double)old_time.tv_sec + 1.0e-6 * (double)old_time.tv_usec);
	old_time.tv_sec  = new_time.tv_sec;
	old_time.tv_usec = new_time.tv_usec;
	return DeltaT;
}

int main(int argc, char* argv[])
{
	// Initialize 4 transfer sizes, i.e. 3KB, 15KB, 15MB and 100MB.
	const int n = 4;
	const size_t sizes[n] = { 3 << 10, 15 << 10, 15 << 20, 100 << 20 };

	// Initialize the number of transfer iterations, i.e. 60K, 60K, 300 and 30 iterations, respectively.
	const int iterations[n] = { 60000, 60000, 300, 30 };

	// Print header in CSV format.
	printf("size (B),memory,direction,bandwidth (MB/s)\n");

	// Loop through the 4 transfer sizes.
	for (int s = 0; s < n; ++s)
	{
		// Calculate the total transfer size.
		const size_t size = sizes[s];
		const int iteration = iterations[s];
		const double totalSizeInMB = (double)size * iteration / (1 << 20);
		double time;

		// Allocate d_p in device memory.
		void* h_p;
		void* d_p;
		cudaMalloc(&d_p, size);

		// Allocate pageable h_p in host memory.
		h_p = malloc(size);

		// Test host-to-device bandwidth.
		shrDeltaT();
		for (int i = 0; i < iteration; ++i)
		{
			cudaMemcpy(d_p, h_p, size, cudaMemcpyHostToDevice);
		}
		time = shrDeltaT();
		printf("%lu,%s,%s,%.0f\n", size, "pageable", "HtoD", totalSizeInMB / time);

		// Test device-to-host bandwidth.
		shrDeltaT();
		for (int i = 0; i < iteration; ++i)
		{
			cudaMemcpy(h_p, d_p, size, cudaMemcpyDeviceToHost);
		}
		time = shrDeltaT();
		printf("%lu,%s,%s,%.0f\n", size, "pageable", "DtoH", totalSizeInMB / time);

		// Deallocate pageable h_p in host memory.
		free(h_p);

		// Allocate pinned h_p in host memory.
        cudaHostAlloc(&h_p, size, cudaHostAllocDefault);

		// Test host-to-device bandwidth.
		shrDeltaT();
		for (int i = 0; i < iteration; ++i)
		{
			cudaMemcpyAsync(d_p, h_p, size, cudaMemcpyHostToDevice);
		}
		cudaDeviceSynchronize();
		time = shrDeltaT();
		printf("%lu,%s,%s,%.0f\n", size, "pinned", "HtoD", totalSizeInMB / time);

		// Test device-to-host bandwidth.
		shrDeltaT();
		for (int i = 0; i < iteration; ++i)
		{
			cudaMemcpyAsync(h_p, d_p, size, cudaMemcpyDeviceToHost);
		}
		cudaDeviceSynchronize();
		time = shrDeltaT();
		printf("%lu,%s,%s,%.0f\n", size, "pinned", "DtoH", totalSizeInMB / time);

		// Deallocate pinned h_p in host memory.
		cudaFreeHost(h_p);

		// Deallocate d_p in device memory.
		cudaFree(d_p);
	}

	// Cleanup.
	cudaDeviceReset();
}
</pre>
          <p>Try different values of <code>sizes</code>.</p>
          <p>Try <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g48efa06b81cc031b2aa6fdc2e9930741">cudaMemcpy</a> instead of <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1gf2810a94ec08fe3c7ff9d93a33af7255">cudaMemcpyAsync</a> on pinned memory.</p>
          <p>Try <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g36b9fe28f547f28d23742e8c7cd18141">cudaHostRegister</a> to register an existing host memory range as pinned memory, and <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1gc07b1312c60ca36c118e2ed71b192afe">cudaHostUnregister</a> to unregister it after use.</p>
          <p>Try passing the flag <code>cudaHostAllocWriteCombined</code> to <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g15a3871f15f8c38f5b7190946845758c">cudaHostAlloc</a> to allocate the memory as write-combined (WC).</p>
          <h2 id="checkError">checkError</h2>
          <p>This sample checks the return value of every runtime API. It requires <code>-I /usr/local/cuda/samples/common/inc</code> in the Makefile.</p>
<pre class="cpp">
#include &lt;helper_cuda.h&gt;

__global__ void iota(float *a)
{
	int i = blockDim.x * blockIdx.x + threadIdx.x;
	a[i] = i;
}

int main(int argc, char *argv[])
{
	int numElements = 1e+8;

	// Allocate vector a in device memory.
	float *d_a;
	checkCudaErrors(cudaMalloc((void **)&d_a, sizeof(float) * numElements));

	// Determine the number of threads per block and the number of blocks per grid.
	int numThreadsPerBlock = 256;
	int numBlocksPerGrid = (numElements + numThreadsPerBlock - 1) / numThreadsPerBlock;

	// Invoke the kernel on device asynchronously.
	iota&lt;&lt;&lt;numBlocksPerGrid, numThreadsPerBlock&gt;&gt;&gt;(d_a);

	// Cleanup.
	checkCudaErrors(cudaFree(d_a));
	checkCudaErrors(cudaDeviceReset());
}
</pre>
          <p>Try setting <code>numElements</code> to 1e+10, and observe <code>cudaErrorMemoryAllocation</code>.</p>
          <p>Try setting <code>numThreadsPerBlock</code> to 2560, and observe <code>cudaErrorInvalidConfiguration</code>.</p>
          <p>Try accessing <code>a[i*2]</code> in the kernel, and observe <code>cudaErrorLaunchFailure</code>. Note the error line number at line 26 because kernel execution is asynchronous.</p>
          <p>The <code>checkCudaErrors</code> macro, as listed below, is defined in <code>/usr/local/cuda/samples/common/inc/helper_cuda.h</code>. In case a CUDA host call returns an error, it will output the proper CUDA error string to stderr, calls <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1gdcc2c6f914eb9461565b12648faa5e28">cudaDeviceReset</a> to destroy all allocations and reset all state on the current device in the current process, and exits the program. Try to define your own <code>checkCudaErrors</code> macro for your particular applications.</p>
<pre class="cpp">
#ifdef __DRIVER_TYPES_H__
#ifndef DEVICE_RESET
#define DEVICE_RESET cudaDeviceReset();
#endif
#else
#ifndef DEVICE_RESET
#define DEVICE_RESET 
#endif
#endif

template< typename T >
void check(T result, char const *const func, const char *const file, int const line)
{
    if (result)
    {
        fprintf(stderr, "CUDA error at %s:%d code=%d(%s) \"%s\" \n",
                file, line, static_cast<unsigned int>(result), _cudaGetErrorEnum(result), func);
		DEVICE_RESET
        // Make sure we call CUDA Device Reset before exiting
        exit(EXIT_FAILURE);
    }
}

#ifdef __DRIVER_TYPES_H__
// This will output the proper CUDA error strings in the event that a CUDA host call returns an error
#define checkCudaErrors(val)           check ( (val), #val, __FILE__, __LINE__ )
</pre>
          <h2 id="matrixMul">matrixMul</h2>
          <p>This sample uses shared memory to accelerate matrix multiplication.</p>
<pre class="cpp">
#include &lt;stdio.h&gt;

template &lt;int BLOCK_SIZE&gt; __global__ void matrixMul(float *C, float *A, float *B, int wA, int wB)
{
	int bx = blockIdx.x;
	int by = blockIdx.y;
	int tx = threadIdx.x;
	int ty = threadIdx.y;
	int aBegin = wA * BLOCK_SIZE * by;
	int aEnd   = aBegin + wA - 1;
	int aStep  = BLOCK_SIZE;
	int bBegin = BLOCK_SIZE * bx;
	int bStep  = BLOCK_SIZE * wB;
	float Csub = 0;
	for (int a = aBegin, b = bBegin; a <= aEnd; a += aStep, b += bStep)
	{
		// Allocate blocks of data in shared memory.
		__shared__ float As[BLOCK_SIZE][BLOCK_SIZE];
		__shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];

		// Copy a block of data from global memory to shared memory.
		As[ty][tx] = A[a + wA * ty + tx];
		Bs[ty][tx] = B[b + wB * ty + tx];

		// Waits until all threads in the thread block have reached this point and all global and shared memory accesses made by these threads prior to __syncthreads() are visible to all threads in the block.
		__syncthreads();

// Unrolling is possible because BLOCK_SIZE is known at compile time.
#pragma unroll
		for (int k = 0; k < BLOCK_SIZE; ++k)
		{
			Csub += As[ty][k] * Bs[k][tx];
		}
		__syncthreads();
	}

	// Save the result from shared memory to global memory.
	int c = wB * BLOCK_SIZE * by + BLOCK_SIZE * bx;
	C[c + wB * ty + tx] = Csub;
}

int main(int argc, char *argv[])
{
	// Initialize constants.
	const int block_size = 32;
	dim3 dimsA(5 * 2 * block_size, 5 * 2 * block_size, 1);
	dim3 dimsB(5 * 4 * block_size, 5 * 2 * block_size, 1);
	dim3 dimsC(dimsB.x, dimsA.y, 1);
	unsigned int size_A = dimsA.x * dimsA.y;
	unsigned int size_B = dimsB.x * dimsB.y;
	unsigned int size_C = dimsC.x * dimsC.y;
	unsigned int mem_size_A = sizeof(float) * size_A;
	unsigned int mem_size_B = sizeof(float) * size_B;
	unsigned int mem_size_C = sizeof(float) * size_C;

	// Allocate matrices a, b and c in host memory.
	float *h_A = (float *)malloc(mem_size_A);
	float *h_B = (float *)malloc(mem_size_B);
	float *h_C = (float *)malloc(mem_size_C);

	// Initialize matrices a and b.
	for (int i = 0; i < size_A; ++i)
	{
		h_A[i] = 1.0f;
	}
	const float valB = 0.01f;
	for (int i = 0; i < size_B; ++i)
	{
		h_B[i] = valB;
	}

	// Allocate matrices a, b and c in device memory.
	float *d_A, *d_B, *d_C;
	cudaMalloc((void **)&d_A, mem_size_A);
	cudaMalloc((void **)&d_B, mem_size_B);
	cudaMalloc((void **)&d_C, mem_size_C);

	// Copy matrices a and b from host memory to device memory.
	cudaMemcpy(d_A, h_A, mem_size_A, cudaMemcpyHostToDevice);
	cudaMemcpy(d_B, h_B, mem_size_B, cudaMemcpyHostToDevice);

	// Determine the number of threads per block and the number of blocks per grid.
	dim3 threadsPerBlock(block_size, block_size);
	dim3 blocksPerGrid(dimsB.x / threadsPerBlock.x, dimsA.y / threadsPerBlock.y);

	// Invoke the kernel on device asynchronously.
	matrixMul&lt;block_size&gt;&lt;&lt;&lt;blocksPerGrid, threadsPerBlock&gt;&gt;&gt;(d_C, d_A, d_B, dimsA.x, dimsB.x);

	// Wait for the device to finish.
	cudaDeviceSynchronize();

	// Create events to record timing data.
	cudaEvent_t start, stop;
	cudaEventCreate(&start);
	cudaEventCreate(&stop);

	// Record an event in stream 0 before kernel invocations.
	cudaEventRecord(start, 0);

	// Invoke the kernel for a number of iterations.
	int numIterations = 300;
	for (int i = 0; i < numIterations; ++i)
	{
		matrixMul&lt;block_size&gt;&lt;&lt;&lt;blocksPerGrid, threadsPerBlock&gt;&gt;&gt(d_C, d_A, d_B, dimsA.x, dimsB.x);
	}

	// Record an event in stream 0 after kernel invocations.
	cudaEventRecord(stop, 0);

	// Wait for the event to complete.
	cudaEventSynchronize(stop);

	// Compute the elapsed time between two events.
	float elapsed;
	cudaEventElapsedTime(&elapsed, start, stop);

	// Compute and print the GLOPS/s performance metric.
	printf("%.2f GFLOP/s\n", (2.0f * dimsA.x * dimsA.y * dimsB.x * numIterations * 1e-9f) / (elapsed / 1000.0f));

	// Copy matrix c from device memory to host memory synchronously.
	cudaMemcpy(h_C, d_C, mem_size_C, cudaMemcpyDeviceToHost);

	// Validate the result.
	for (int i = 0; i < dimsC.x * dimsC.y; ++i)
	{
		float actual = h_C[i];
		float expected = dimsA.x * valB;
		if (fabs(actual - expected) / fabs(actual) / dimsA.x > 1e-7)
		{
			printf("h_C[%d] = %f, expected = %f\n", i, actual, expected);
		}
	}

	// Cleanup.
	cudaFree(d_C);
	cudaFree(d_B);
	cudaFree(d_A);
	cudaDeviceReset();
	free(h_C);
	free(h_B);
	free(h_A);
}
</pre>
          <p>Try using <code>double</code> instead of <code>float</code>.</p>
          <p>Try using <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1gac27b566beee1aa9175373bb9e29b8d1">cudaDeviceSetCacheConfig</a> or <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EXECUTION.html#group__CUDART__EXECUTION_1g4f35d04be20a41c5df96613a748eecc1">cudaFuncSetCacheConfig</a> to set the sizes of L1 cache and shared memory out of 64KB.</p>
          <p>Try using <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g1a4789fb687cc36dccc98f25c96f0cd8">cudaDeviceSetSharedMemConfig</a> or <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EXECUTION.html#group__CUDART__EXECUTION_1gcb2e02dc6c75e4f705dfd2252748fe99">cudaFuncSetSharedMemConfig</a> to set shared memory bank width to be eight bytes natively.</p>
          <p>Try using <a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared">dynamic shared memory</a> which can have a dynamic size at run time.</p>
          <p>Try using <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g80d689bc903792f906e49be4a0b6d8db">cudaMallocPitch</a> to allocate a 2D array and <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g17f3a55e8c9aef5f90b67cdf22851375">cudaMemcpy2D</a> to copy a 2D array.</p>
          <p>Try using <a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#execution-configuration">memory fence functions</a> instead of <a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#synchronization-functions">synchronization functions</a>.</p>
          <p>Try rewriting the previous <a href="#bandwidthTest">bandwidthTest</a> sample using <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html#group__CUDART__EVENT_1ge31b1b1558db52579c9e23c5782af93e">cudaEventRecord</a> and <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html#group__CUDART__EVENT_1g14c387cc57ce2e328f6669854e6020a5">cudaEventElapsedTime</a> instead of <code>shrDeltaT</code>.</p>
          <h2 id="atomicAdd">atomicAdd</h2>
          <p>This sample uses <a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#atomic-functions">atomic functions</a>, <a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#assertion">assertions</a> and <a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#formatted-output">printf</a>.</p>
<pre class="cpp">
#include &lt;stdio.h&gt;
#include &lt;assert.h&gt;

__constant__ int inc;
__device__ int sum;

__global__ void atomicAdd()
{
	int s = atomicAdd(&sum, inc);
	assert((s - 1) % inc == 0);
	if (threadIdx.x == 0)
	{
		printf("blockIdx.x = %d, sum = %d\n", blockIdx.x, s);
	}
}

int main(int argc, char *argv[])
{
	// Initialize inc and sum.
	int h_inc = 3;
	int h_sum = 1;

	// Copy inc and sum from host memory to device memory synchronously.
	cudaMemcpyToSymbol(inc, &h_inc, sizeof(int));
	cudaMemcpyToSymbol(sum, &h_sum, sizeof(int));

	// Invoke the kernel on device asynchronously.
	atomicAdd&lt;&lt;&lt;2, 2&gt;&gt;&gt;();

	// Copy sum from device memory to host memory synchronously.
	cudaMemcpyFromSymbol(&h_sum, sum, sizeof(int));

	// Print the result.
	printf("sum = %d\n", h_sum);

	// Cleanup.
	cudaDeviceReset();
}
</pre>
          <p>Try different numbers of threads per block and different numbers of blocks per grid.</p>
          <p>Try atomic functions on shared memory, and compare the performance to atomic functions on global memory.</p>
          <p>Refer to <a href="http://on-demand.gputechconf.com/gtc/2013/presentations/S3101-Atomic-Memory-Operations.pdf">Understanding and Using Atomic Memory Operations</a> for more detail.</p>
          <h2 id="asyncEngine">asyncEngine</h2>
          <p>This sample uses asynchronous engines to overlap data transfer and kernel execution.</p>
<pre class="cpp">
#include &lt;stdio.h&gt;

__global__ void vectorAdd(const float *a, const float *b, float *c, int numElements)
{
	int i = blockDim.x * blockIdx.x + threadIdx.x;
	if (i < numElements)
	{
		c[i] = a[i] + b[i];
	}
	for (const clock_t threshold = clock() + 1e+4; clock() < threshold;);
}

int main(int argc, char *argv[])
{
	int numElements = 3 << 22;

	// Allocate vectors a, b and c in host memory.
	size_t numBytes = sizeof(float) * numElements;
	float *h_a;
	float *h_b;
	float *h_c;
	cudaMallocHost((void **)&h_a, numBytes);
	cudaMallocHost((void **)&h_b, numBytes);
	cudaMallocHost((void **)&h_c, numBytes);

	// Initialize vectors a and b.
	for (int i = 0; i < numElements; ++i)
	{
		h_a[i] = rand() / (float)RAND_MAX;
		h_b[i] = rand() / (float)RAND_MAX;
	}

	// Initialize a number of CUDA streams.
	int numStreams = 8;
    cudaStream_t *streams = (cudaStream_t *)malloc(sizeof(cudaStream_t) * numStreams);
    for (int i = 0; i < numStreams; ++i)
    {
        cudaStreamCreate(&streams[i]);
    }

	// Compute the average number of elements per device and the number of spare elements.
	int avgElementsPerStream = numElements / numStreams;
	int sprElements = numElements - avgElementsPerStream * numStreams;

	float **d_a = (float **)malloc(sizeof(float *) * numStreams);
	float **d_b = (float **)malloc(sizeof(float *) * numStreams);
	float **d_c = (float **)malloc(sizeof(float *) * numStreams);

	for (int i = 0, offset = 0; i < numStreams; ++i)
	{
		// Determine the number of elements to be processed by the current device.
		int numElementsCurrentStream = avgElementsPerStream + (i < sprElements);

		// Allocate vectors a, b and c in device memory.
		size_t numBytesCurrentStream = sizeof(int) * numElementsCurrentStream;
		cudaMalloc((void **)&d_a[i], numBytesCurrentStream);
		cudaMalloc((void **)&d_b[i], numBytesCurrentStream);
		cudaMalloc((void **)&d_c[i], numBytesCurrentStream);

		// Copy vectors a and b from host memory to device memory asynchronously.
		cudaMemcpyAsync(d_a[i], h_a + offset, numBytesCurrentStream, cudaMemcpyHostToDevice, streams[i]);
		cudaMemcpyAsync(d_b[i], h_b + offset, numBytesCurrentStream, cudaMemcpyHostToDevice, streams[i]);

		// Determine the number of threads per block and the number of blocks per grid.
		unsigned int numThreadsPerBlock = 256;
		unsigned int numBlocksPerGrid = (numElementsCurrentStream + numThreadsPerBlock - 1) / numThreadsPerBlock;

		// Invoke the kernel on device asynchronously.
		// The 1st argument is of type dim3 and specifies the number of blocks per grid.
		// The 2nd argument is of type dim3 and specifies the number of threads per block.
		// The 3rd argument is of type size_t and specifiesthe number of bytes in shared memory that is dynamically allocated per block for this call in addition to the statically allocated memory. It is an optional argument which defaults to 0.
		// The 4th argument is of type cudaStream_t and specifies the associated stream. It is an optional argument which defaults to 0.
		vectorAdd&lt;&lt;&lt;numBlocksPerGrid, numThreadsPerBlock, 0, streams[i]&gt;&gt;&gt;(d_a[i], d_b[i], d_c[i], numElementsCurrentStream);

		// Copy vector c from device memory to host memory asynchronously.
		cudaMemcpyAsync(h_c + offset, d_c[i], numBytesCurrentStream, cudaMemcpyDeviceToHost, streams[i]);

		// Increase offset to point to the next portion of data.
		offset += numElementsCurrentStream;
	}

	// Wait for the device to finish.
	cudaDeviceSynchronize();

	// Validate the result.
	for (int i = 0; i < numElements; ++i)
	{
		float actual = h_c[i];
		float expected = h_a[i] + h_b[i];
		if (fabs(actual - expected) > 1e-7)
		{
			printf("h_c[%d] = %f, expected = %f\n", i, actual, expected);
			break;
		}
	}

	// Cleanup.
	for (int i = 0; i < numStreams; ++i)
	{
		cudaFree(d_c[i]);
		cudaFree(d_b[i]);
		cudaFree(d_a[i]);
	}
	free(d_c);
	free(d_b);
	free(d_a);
    for (int i = 0; i < numStreams; ++i)
    {
        cudaStreamDestroy(streams[i]);
    }
    free(streams);
	cudaFreeHost(h_c);
	cudaFreeHost(h_b);
	cudaFreeHost(h_a);
	cudaDeviceReset();
}
</pre>
          <h2 id="hyperQ">hyperQ</h2>
          <p>This sample uses multiple streams to overlap multiple kernel execution, known as the <a href="http://docs.nvidia.com/cuda/samples/6_Advanced/simpleHyperQ/doc/HyperQ.pdf">HyperQ</a> technology.</p>
<pre class="cpp">
#include &lt;stdio.h&gt;

__global__ void spin(clock_t numClocks)
{
	for (const clock_t threshold = clock() + numClocks; clock() < threshold;);
}

int main(int argc, char *argv[])
{
	// Initialize constants.
	const int numMilliseconds = 10;
	const int numKernels = 2;
	const int numStreams = 32;

	// Get the major and minor compute capability version numbers.
	int major, minor;
	cudaDeviceGetAttribute(&major, cudaDevAttrComputeCapabilityMajor, 0);
	cudaDeviceGetAttribute(&minor, cudaDevAttrComputeCapabilityMinor, 0);

	// Get the peak clock frequency in KHz of device 0.
	int clockRate;
	cudaDeviceGetAttribute(&clockRate, cudaDevAttrClockRate, 0);

	// Calculate the number of clocks within a certain period.
	clock_t numClocks = clockRate * numMilliseconds;

	// Create streams to enqueue kernels.
	cudaStream_t *streams = (cudaStream_t*)malloc(sizeof(cudaStream_t) * numStreams);
	for (int s = 0; s < numStreams; ++s)
	{
		cudaStreamCreate(&streams[s]);
	}

	// Create events to record timing data.
	cudaEvent_t beg, end;
	cudaEventCreate(&beg);
	cudaEventCreate(&end);

	// Record an event in stream 0 before kernel invocations.
	cudaEventRecord(beg, 0);

	// Enqueue kernels to streams.
	for (int s = 0; s < numStreams; ++s)
	{
		for (int k = 0; k < numKernels; ++k)
		{
			spin&lt;&lt;&lt;1, 1, 0, streams[s]&gt;&gt;&gt;(numClocks);
		}
	}

	// Record an event in stream 0 after kernel invocations.
	cudaEventRecord(end, 0);

	// Wait for the event to complete.
	cudaEventSynchronize(end);

	// Compute the elapsed time between two events.
	float elapsed;
	cudaEventElapsedTime(&elapsed, beg, end);

	// Print the result.
	printf("%d streams, each %d kernels, each %d ms\n", numStreams, numKernels, numMilliseconds);
	printf("       SM <= 1.3:%4d ms\n", numMilliseconds * numKernels * numStreams);
	printf("2.0 <= SM <= 3.0:%4d ms\n", numMilliseconds * (1 + (numKernels - 1) * numStreams));
	printf("3.5 <= SM       :%4d ms\n", numMilliseconds * numKernels);
	printf("       SM == %d.%d:%4d ms\n", major, minor, (int)elapsed);

	// Cleanup.
	cudaEventDestroy(end);
	cudaEventDestroy(beg);
	for (int s = 0; s < numStreams; ++s)
	{
		cudaStreamDestroy(streams[s]);
	}
	free(streams);
	cudaDeviceReset();
}
</pre>
          <p>Try different values of <code>numMilliseconds</code>, <code>numKernels</code> and <code>numStreams</code>.</p>
          <p>Try different numbers of threads per block, different numbers of blocks per grid, and different sizes of dynamic shared memory.</p>
          <p>Try double buffering.</p>
          <h2 id="deviceQuery">deviceQuery</h2>
          <p>This sample enumerates some properties of the CUDA devices present in the system.</p>
<pre class="cpp">
#include &lt;stdio.h&gt;

int main(int argc, char *argv[])
{
	// Get the number of devices.
	int deviceCount;
	cudaGetDeviceCount(&deviceCount);
	printf("deviceCount: %d\n", deviceCount);

	// Get the properties of device 0.
	cudaDeviceProp deviceProp;
	cudaGetDeviceProperties(&deviceProp, 0);

	// Print some properties.
	printf("name: %s\n", deviceProp.name);
	printf("major: %d\n", deviceProp.major);
	printf("minor: %d\n", deviceProp.minor);
	printf("multiProcessorCount: %d\n", deviceProp.multiProcessorCount);
	printf("totalGlobalMem: %lu B = %lu MB\n", deviceProp.totalGlobalMem, deviceProp.totalGlobalMem / 1048576);
	printf("sharedMemPerBlock: %lu B = %lu KB\n", deviceProp.sharedMemPerBlock, deviceProp.sharedMemPerBlock / 1024);
	printf("totalConstMem: %lu B = %lu KB\n", deviceProp.totalConstMem, deviceProp.totalConstMem / 1024);
	printf("regsPerBlock: %d\n", deviceProp.regsPerBlock);
	printf("ECCEnabled: %d\n", deviceProp.ECCEnabled);
	printf("kernelExecTimeoutEnabled: %d\n", deviceProp.kernelExecTimeoutEnabled);
	printf("clockRate: %d KHz = %d MHz\n", deviceProp.clockRate, deviceProp.clockRate / 1000);
	printf("memoryClockRate: %d KHz = %d MHz\n", deviceProp.memoryClockRate, deviceProp.memoryClockRate / 1000);
	printf("memoryBusWidth: %d bits\n", deviceProp.memoryBusWidth);
	printf("l2CacheSize: %d B = %d KB\n", deviceProp.l2CacheSize, deviceProp.l2CacheSize / 1024);
	printf("warpSize: %d\n", deviceProp.warpSize);
	printf("maxThreadsPerMultiProcessor: %d\n", deviceProp.maxThreadsPerMultiProcessor);
	printf("maxThreadsPerBlock: %d\n", deviceProp.maxThreadsPerBlock);
	printf("maxThreadsDim[0]: %d\n", deviceProp.maxThreadsDim[0]);
	printf("maxThreadsDim[1]: %d\n", deviceProp.maxThreadsDim[1]);
	printf("maxThreadsDim[2]: %d\n", deviceProp.maxThreadsDim[2]);
	printf("maxGridSize[0]: %d\n", deviceProp.maxGridSize[0]);
	printf("maxGridSize[1]: %d\n", deviceProp.maxGridSize[1]);
	printf("maxGridSize[2]: %d\n", deviceProp.maxGridSize[2]);
	printf("deviceOverlap: %d\n", deviceProp.deviceOverlap);
	printf("asyncEngineCount: %d\n", deviceProp.asyncEngineCount);
	printf("integrated: %d\n", deviceProp.integrated);
	printf("canMapHostMemory: %d\n", deviceProp.canMapHostMemory);
	printf("concurrentKernels: %d\n", deviceProp.concurrentKernels);
	printf("tccDriver: %d\n", deviceProp.tccDriver);
	printf("unifiedAddressing: %d\n", deviceProp.unifiedAddressing);
	printf("pciBusID: %d\n", deviceProp.pciBusID);
	printf("pciDeviceID: %d\n", deviceProp.pciDeviceID);
	printf("computeMode: %d\n", deviceProp.computeMode);
	if (deviceProp.computeMode == cudaComputeModeDefault) printf("computeMode: %s\n", "Default (multiple host threads can use ::cudaSetDevice() with device simultaneously)");
	if (deviceProp.computeMode == cudaComputeModeExclusive) printf("computeMode: %s\n", "Exclusive (only one host thread in one process is able to use ::cudaSetDevice() with this device)");
	if (deviceProp.computeMode == cudaComputeModeProhibited) printf("computeMode: %s\n", "Prohibited (no host thread can use ::cudaSetDevice() with this device)");
	if (deviceProp.computeMode == cudaComputeModeExclusiveProcess) printf("computeMode: %s\n", "Exclusive Process (many threads in one process is able to use ::cudaSetDevice() with this device)");
}
</pre>
          <p>Try enumerating all the properties returned by <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g929dcd9a191e17b7498e7ccaa3d16350">cudaGetDeviceProperties</a> of all the devices.</p>
          <h2 id="multiDevice">multiDevice</h2>
          <p>This sample uses <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g418c299b069c4803bfb7cab4943da383">cudaSetDevice</a> within a single thread to utilize multiple GPUs.</p>
<pre class="cpp">
#include &lt;stdio.h&gt;

__global__ void vectorAdd(const float *a, const float *b, float *c, int numElements)
{
	int i = blockDim.x * blockIdx.x + threadIdx.x;
	if (i < numElements)
	{
		c[i] = a[i] + b[i];
	}
	for (const clock_t threshold = clock() + 1e+4; clock() < threshold;);
}

int main(int argc, char *argv[])
{
	int numElements = 3 << 22;

	// Allocate vectors a, b and c in host memory.
	size_t numBytes = sizeof(float) * numElements;
	float *h_a;
	float *h_b;
	float *h_c;
	cudaMallocHost((void **)&h_a, numBytes);
	cudaMallocHost((void **)&h_b, numBytes);
	cudaMallocHost((void **)&h_c, numBytes);

	// Initialize vectors a and b.
	for (int i = 0; i < numElements; ++i)
	{
		h_a[i] = rand() / (float)RAND_MAX;
		h_b[i] = rand() / (float)RAND_MAX;
	}

	// Get the number of CUDA devices.
	int numDevices;
	cudaGetDeviceCount(&numDevices);

	// Compute the average number of elements per device and the number of spare elements.
	int avgElementsPerDevice = numElements / numDevices;
	int sprElements = numElements - avgElementsPerDevice * numDevices;

	float **d_a = (float **)malloc(sizeof(float *) * numDevices);
	float **d_b = (float **)malloc(sizeof(float *) * numDevices);
	float **d_c = (float **)malloc(sizeof(float *) * numDevices);

	for (int i = 0, offset = 0; i < numDevices; ++i)
	{
		// Determine the number of elements to be processed by the current device.
		int numElementsCurrentDevice = avgElementsPerDevice + (i < sprElements);

		// Set device to be used for GPU executions.
		cudaSetDevice(i);

		// Allocate vectors a, b and c in device memory.
		size_t numBytesCurrentDevice = sizeof(int) * numElementsCurrentDevice;
		cudaMalloc((void **)&d_a[i], numBytesCurrentDevice);
		cudaMalloc((void **)&d_b[i], numBytesCurrentDevice);
		cudaMalloc((void **)&d_c[i], numBytesCurrentDevice);

		// Copy vectors a and b from host memory to device memory asynchronously.
		cudaMemcpyAsync(d_a[i], h_a + offset, numBytesCurrentDevice, cudaMemcpyHostToDevice);
		cudaMemcpyAsync(d_b[i], h_b + offset, numBytesCurrentDevice, cudaMemcpyHostToDevice);

		// Determine the number of threads per block and the number of blocks per grid.
		unsigned int numThreadsPerBlock = 256;
		unsigned int numBlocksPerGrid = (numElementsCurrentDevice + numThreadsPerBlock - 1) / numThreadsPerBlock;

		// Invoke the kernel on device asynchronously.
		vectorAdd&lt;&lt;&lt;numBlocksPerGrid, numThreadsPerBlock&gt;&gt;&gt;(d_a[i], d_b[i], d_c[i], numElementsCurrentDevice);

		// Copy vector c from device memory to host memory asynchronously.
		cudaMemcpyAsync(h_c + offset, d_c[i], numBytesCurrentDevice, cudaMemcpyDeviceToHost);

		// Increase offset to point to the next portion of data.
		offset += numElementsCurrentDevice;
	}

	// Wait for the devices to finish.
	for (int i = 0; i < numDevices; ++i)
	{
		cudaSetDevice(i);
		cudaDeviceSynchronize();
	}

	// Validate the result.
	for (int i = 0; i < numElements; ++i)
	{
		float actual = h_c[i];
		float expected = h_a[i] + h_b[i];
		if (fabs(actual - expected) > 1e-7)
		{
			printf("h_c[%d] = %f, expected = %f\n", i, actual, expected);
			break;
		}
	}

	// Cleanup.
	for (int i = 0; i < numDevices; ++i)
	{
		cudaSetDevice(i);
		cudaFree(d_c[i]);
		cudaFree(d_b[i]);
		cudaFree(d_a[i]);
	}
	free(d_c);
	free(d_b);
	free(d_a);
	cudaFreeHost(h_c);
	cudaFreeHost(h_b);
	cudaFreeHost(h_a);
	for (int i = 0; i < numDevices; ++i)
	{
		cudaSetDevice(i);
		cudaDeviceReset();
	}
}
</pre>
          <p>Try using <code>valgrind</code> to detect memory leak in the following 3 cases of cleanup.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~/cudart/multiDevice></b> valgrind ./multiDevice
</pre>
          <p>Case 1: <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1gdcc2c6f914eb9461565b12648faa5e28">cudaDeviceReset</a> is called on every device.</p>
<pre class="cpp">
	for (int i = 0; i < numDevices; ++i)
	{
		cudaSetDevice(i);
		cudaDeviceReset();
	}
</pre>
<pre class="sh">
==53804== HEAP SUMMARY:
==53804==     in use at exit: 343,139 bytes in 119 blocks
==53804==   total heap usage: 21,298 allocs, 21,179 frees, 15,813,174 bytes allocated
==53804== 
==53804== LEAK SUMMARY:
==53804==    definitely lost: 0 bytes in 0 blocks
==53804==    indirectly lost: 0 bytes in 0 blocks
==53804==      possibly lost: 2,992 bytes in 22 blocks
==53804==    still reachable: 340,147 bytes in 97 blocks
==53804==         suppressed: 0 bytes in 0 blocks
</pre>
          <p>Case 2: <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1gdcc2c6f914eb9461565b12648faa5e28">cudaDeviceReset</a> is called on one device.</p>
<pre class="cpp">
//	for (int i = 0; i < numDevices; ++i)
//	{
//		cudaSetDevice(i);
		cudaDeviceReset();
//	}
</pre>
<pre class="sh">
==53860== HEAP SUMMARY:
==53860==     in use at exit: 7,031,081 bytes in 11,771 blocks
==53860==   total heap usage: 21,282 allocs, 9,511 frees, 15,812,934 bytes allocated
==53860== 
==53860== LEAK SUMMARY:
==53860==    definitely lost: 0 bytes in 0 blocks
==53860==    indirectly lost: 0 bytes in 0 blocks
==53860==      possibly lost: 51,984 bytes in 380 blocks
==53860==    still reachable: 6,979,097 bytes in 11,391 blocks
==53860==         suppressed: 0 bytes in 0 blocks
</pre>
          <p>Case 3: <a href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1gdcc2c6f914eb9461565b12648faa5e28">cudaDeviceReset</a> is not called at all.</p>
<pre class="cpp">
//	for (int i = 0; i < numDevices; ++i)
//	{
//		cudaSetDevice(i);
//		cudaDeviceReset();
//	}
</pre>
<pre class="sh">
==53911== HEAP SUMMARY:
==53911==     in use at exit: 10,393,620 bytes in 17,742 blocks
==53911==   total heap usage: 21,274 allocs, 3,532 frees, 15,812,814 bytes allocated
==53911== 
==53911== LEAK SUMMARY:
==53911==    definitely lost: 0 bytes in 0 blocks
==53911==    indirectly lost: 0 bytes in 0 blocks
==53911==      possibly lost: 78,656 bytes in 575 blocks
==53911==    still reachable: 10,314,964 bytes in 17,167 blocks
==53911==         suppressed: 0 bytes in 0 blocks
</pre>
          <p>Refer to <a href="http://on-demand.gputechconf.com/gtc/2013/presentations/S3465-Multi-GPU-Programming.pdf">Multi-GPU Programming</a> for more detail.</p>
          <h2 id="openmp">openmp</h2>
          <p>This sample uses OpenMP to create multiple CPU threads to utilize multiple GPUs.</p>
<pre class="cpp">
#include &lt;stdio.h&gt;
#include &lt;omp.h&gt;

__global__ void scalarAdd(int *data, int inc)
{
	int i = blockIdx.x * blockDim.x + threadIdx.x;
	data[i] += inc;
}

int main(int argc, char *argv[])
{
	// Get the number of CUDA devices.
	int numDevices;
	cudaGetDeviceCount(&numDevices);

	// Allocate and initialize data.
	size_t numElements = 8192 * numDevices;
	size_t numBytes = sizeof(int) * numElements;
	int *data = (int *)malloc(numBytes);
	memset(data, 0, numBytes);
	int inc = 7;

	// Create as many CPU threads as there are CUDA devices. Each CPU thread controls a different device, processing its portion of the data.
	omp_set_num_threads(numDevices);

	// All variables declared inside an "omp parallel" scope are local to each CPU thread.
	#pragma omp parallel
	{
		// Get the number of CPU threads and the thread number of the current CPU thread.
		// 0 <= threadNum <= numThreads - 1
		int numThreads = omp_get_num_threads();
		int threadNum = omp_get_thread_num();

		// Set device to be used for GPU executions.
		int deviceNum = threadNum % numDevices;
		cudaSetDevice(deviceNum);

		// Calculate the number of elements per CPU thread and the number of bytes per CPU thread.
		size_t numElementsPerThread = numElements / numThreads;
		size_t numBytesPerThread = sizeof(int) * numElementsPerThread;

		// Calculate the offset to the original data for the current CPU thread.
		int *h = data + numElementsPerThread * threadNum;

		// Allocate device memory to temporarily hold the portion of the data of the current CPU thread.
		int *d;
		cudaMalloc((void **)&d, numBytesPerThread);

		// Copy the portion of the data of the current CPU thread from host memory to device memory.
		cudaMemcpy(d, h, numBytesPerThread, cudaMemcpyHostToDevice);

		// Invoke the kernel for the current portion of the data.
		scalarAdd&lt;&lt;&lt;numElementsPerThread / 128, 128&gt;&gt;&gt;(d, inc);

		// Copy the portion of the data of the current CPU thread from device memory to host memory.
		cudaMemcpy(h, d, numBytesPerThread, cudaMemcpyDeviceToHost);

		// Deallocate the temporary device memory.
		cudaFree(d);
	}

	for (int i = 0; i < numElements; ++i)
	{
		int actual = data[i];
		int expected = 0 + inc;
		if (actual != expected)
		{
			printf("data[%d] = %d, expected = %d\n", i, actual, expected);
			break;
		}
	}

	// Cleanup.
	for (int i = 0; i < numDevices; ++i)
	{
		cudaSetDevice(i);
		cudaDeviceReset();
	}
	free(data);
}
</pre>
          <p>In the Makefile, pass the compiler option <code>-Xcompiler -fopenmp</code> to <code>nvcc</code> in both compilation and linking.</p>
<pre class="makefile">
CC=nvcc -arch=sm_35 -Xcompiler -fopenmp

openmp: openmp.o
	$(CC) -o $@ $^

openmp.o: openmp.cu
	$(CC) -o $@ $< -c -I ~/cuda-5.5/samples/common/inc

clean:
	rm -f openmp openmp.o
</pre>
          <p>Try <code>omp_set_num_threads(numDevices*2);</code> to create twice as many CPU threads as there are CUDA devices. In this case two threads will be allocating resources and launching kernels on the same device.</p>
          <p>Refer to <a href="http://on-demand.gputechconf.com/gtc/2013/webinar/gtc-express-webinar-openacc-vs-openmp.pdf">OpenACC 2.0 vs OpenMP 4.0 Programming Comparison</a> for more detail.</p>
          <h2 id="mpi">mpi</h2>
          <p>This sample uses MPI to create multiple CPU processes to utilize multiple GPUs.</p>
<pre class="cpp">
#include &lt;stdio.h&gt;
#include &lt;mpi.h&gt;

__global__ void square(int *d)
{
	int i = blockDim.x * blockIdx.x + threadIdx.x;
	d[i] = d[i] * d[i];
}

int main(int argc, char *argv[])
{
	// Initialize MPI.
	MPI_Init(&argc, &argv);

	// Get the node count and node rank.
	int commSize, commRank;
	MPI_Comm_size(MPI_COMM_WORLD, &commSize);
	MPI_Comm_rank(MPI_COMM_WORLD, &commRank);

	// Get the number of CUDA devices.
	int numDevices;
	cudaGetDeviceCount(&numDevices);

	// Initialize constants.
	int numThreadsPerBlock = 256;
	int numBlocksPerGrid = 10000;
	int dataSizePerNode = numThreadsPerBlock * numBlocksPerGrid;
	int dataSize = dataSizePerNode * commSize;

	// Generate some random numbers on the root node.
	int *data;
	if (commRank == 0)
	{
		data = new int[dataSize];
		for (int i = 0; i < dataSize; ++i)
		{
			data[i] = rand() % 10;
		}
	}

	// Allocate a buffer on the current node.
	int *dataPerNode = new int[dataSizePerNode];

	// Dispatch a portion of the input data to each node.
	MPI_Scatter(data, dataSizePerNode, MPI_INT, dataPerNode, dataSizePerNode, MPI_INT, 0, MPI_COMM_WORLD);

	// Compute the square of each element on device.
	int *d;
	cudaSetDevice(commRank % numDevices);
	cudaMalloc((void **)&d, sizeof(int) * dataSizePerNode);
	cudaMemcpy(d, dataPerNode, sizeof(int) * dataSizePerNode, cudaMemcpyHostToDevice);
	square&lt;&lt;&lt;numBlocksPerGrid, numThreadsPerBlock&gt;&gt;&gt;(d);
	cudaMemcpy(dataPerNode, d, sizeof(int) * dataSizePerNode, cudaMemcpyDeviceToHost);
	cudaFree(d);

	// Compute the sum of the current node.
	int sum = 0;
	for (int i = 0; i < dataSizePerNode; ++i)
	{
		sum += dataPerNode[i];
	}

	// Compute the sum of all nodes.
	int actual;
	MPI_Reduce(&sum, &actual, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);

	// Validate the result.
	if (commRank == 0)
	{
		int expected = 0;
		for (int i = 0; i < dataSize; ++i)
		{
			expected += data[i] * data[i];
		}
		if (actual != expected)
		{
			printf("actual = %d, expected = %d\n", actual, expected);
		}
		delete[] data;
	}

	// Cleanup.
	delete[] dataPerNode;
	cudaDeviceReset();
	MPI_Finalize();
}
</pre>
          <p>In the Makefile, pass the compiler option <code>-L /usr/lib/openmpi -lmpi -lmpi_cxx</code> to <code>nvcc</code> in linking.</p>
<pre class="makefile">
CC=nvcc -arch=sm_35

mpi: mpi.o
	$(CC) -o $@ $^ -L /usr/lib/openmpi -lmpi -lmpi_cxx

mpi.o: mpi.cu
	$(CC) -o $@ $< -c

clean:
	rm -f mpi mpi.o
</pre>
          <p>Run the executable with <code>mpirun</code>. Specify the number of processes to run with the option <code>-n</code>.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~/cudart/mpi></b> mpirun -n 4 ./mpi
</pre>
          <p>Try using <code>float</code> instead of <code>int</code>.</p>
          <p>Try writing a reduce kernel to compute the sum of the current node.</p>
          <p>Refer to <a href="http://on-demand.gputechconf.com/gtc/2013/presentations/S3047-Intro-CUDA-Aware-MPI-NVIDIA-GPUDirect.pdf">Introduction to CUDA-aware MPI and NVIDIA GPUDirect</a>, <a href="http://on-demand.gputechconf.com/gtc/2013/presentations/S3266-GPUDirect-RDMA-Green-Multi-GPU-Architectures.pdf">GPUDirect RDMA and Green Multi-GPU Architectures</a>, <a href="http://on-demand.gputechconf.com/gtc/2013/presentations/S3316-MVAPICH2-High-Performance-MPI-Library.pdf">MVAPICH2: A High Performance MPI Library for NVIDIA GPU Clusters with InfiniBand </a> and <a href="http://on-demand.gputechconf.com/gtc/2013/webinar/gtc-express-gpudirect-rdma.pdf">Accelerating High Performance Computing with GPUDirect RDMA</a> for more detail.</p>
          <h2 id="cublas">cublas</h2>
          <p>This sample uses <a href="http://docs.nvidia.com/cuda/cublas/">CUBLAS</a>, a CUDA implementation of BLAS (Basic Linear Algebra Subprograms), for matrix multiplication.</p>
<pre class="cpp">
#include &lt;stdio.h&gt;
#include &lt;cublas_v2.h&gt;

int main(int argc, char *argv[])
{
	// Initialize constants.
	int block_size = 32;
	int wA = 2 * block_size * 5;
	int hA = 4 * block_size * 5;
	int wB = 2 * block_size * 5;
	int hB = 4 * block_size * 5;
	int wC = 2 * block_size * 5;
	int hC = 4 * block_size * 5;
	unsigned int size_A = wA * hA;
	unsigned int size_B = wB * hB;
	unsigned int size_C = wC * hC;
	unsigned int mem_size_A = sizeof(float) * size_A;
	unsigned int mem_size_B = sizeof(float) * size_B;
	unsigned int mem_size_C = sizeof(float) * size_C;

	// Allocates matrices a, b and c in host memory.
	float *h_A = (float *)malloc(mem_size_A);
	float *h_B = (float *)malloc(mem_size_B);
	float *h_C = (float *)malloc(mem_size_C);

	// Initialize matrices a and b.
	srand(2006);
	for (int i = 0; i < size_A; ++i)
	{
		h_A[i] = rand() / (float)RAND_MAX;
	}
	for (int i = 0; i < size_B; ++i)
	{
		h_B[i] = rand() / (float)RAND_MAX;
	}

	// Allocate matrices a, b and c in device memory.
	float *d_A, *d_B, *d_C;
	cudaMalloc((void **)&d_A, mem_size_A);
	cudaMalloc((void **)&d_B, mem_size_B);
	cudaMalloc((void **)&d_C, mem_size_C);

	// Copy matrices a and b from host memory to device memory.
	cudaMemcpy(d_A, h_A, mem_size_A, cudaMemcpyHostToDevice);
	cudaMemcpy(d_B, h_B, mem_size_B, cudaMemcpyHostToDevice);

	// Determine the number of threads per block and the number of blocks per grid.
	dim3 numThreadsPerBlock(block_size, block_size);
	dim3 numBlocksPerGrid(wC / numThreadsPerBlock.x, hC / numThreadsPerBlock.y);

	// Initialize a cublas handle.
	cublasHandle_t handle;
	cublasCreate(&handle);

	// CUBLAS is column primary.
	const float alpha = 1.0f;
	const float beta  = 0.0f;
	cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, wB, hA, wA, &alpha, d_B, wB, d_A, wA, &beta, d_C, wA);

	// Measure the performance of cublasSgemm over a number of iterations.
	cudaEvent_t start, stop;
	cudaEventCreate(&start);
	cudaEventCreate(&stop);
	cudaEventRecord(start, 0);
	int numIterations = 30;
	for (int i = 0; i < numIterations; ++i)
	{
		cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, wB, hA, wA, &alpha, d_B, wB, d_A, wA, &beta, d_C, wA);
	}
	cudaEventRecord(stop, 0);
	cudaEventSynchronize(stop);
	float elapsed;
	cudaEventElapsedTime(&elapsed, start, stop);

	// Compute and print the GLOPS/s performance metric.
	printf("%.2f GFLOP/s\n", (2.0f * wA * hA * wB * numIterations * 1e-9f) / (elapsed / 1000.0f));

	// Copy matrix c from device memory to host memory.
	cudaMemcpy(h_C, d_C, mem_size_C, cudaMemcpyDeviceToHost);

	// Compute reference solution.
	float *ref = (float *)malloc(mem_size_C);
	for (int i = 0; i < hA; ++i)
	{
		for (int j = 0; j < wB; ++j)
		{
			float sum = 0;
			for (int k = 0; k < wA; ++k)
			{
				sum += h_A[i * wA + k] * h_B[k * wB + j];
			}
			ref[i * wB + j] = sum;
		}
	}

	// Validate the result.
	for (int i = 0; i < size_C; ++i)
	{
		float actual = h_C[i];
		float expected = ref[i];
		if (fabs(actual - expected) / fabs(actual) / wA > 1e-7)
		{
			printf("h_C[%d] = %f, expected = %f\n", i, actual, expected);
			break;
		}
	}

	// Cleanup.
	free(ref);
	cublasDestroy(handle);
	cudaFree(d_C);
	cudaFree(d_B);
	cudaFree(d_A);
	cudaDeviceReset();
	free(h_C);
	free(h_B);
	free(h_A);
}
</pre>
          <p><a href="http://docs.nvidia.com/cuda/cufft/">CUFFT</a> is a CUDA implementation of FFT (Fast Fourier Transform).</p>
          <p><a href="http://docs.nvidia.com/cuda/curand/">CURAND</a> is a CUDA implementation of PRNG (PseudoRandom Number Generator) and QRNG (QuasiRandom Number Generator).</p>
          <p><a href="http://docs.nvidia.com/cuda/cusparse/">CUSPARSE</a> is a CUDA implementation of basic linear algebra subroutines for sparse matrices.</p>
          <p><a href="https://developer.nvidia.com/magma">MAGMA</a> is a third-party CUDA implementation of next generation LA (Linear Algebra) by the team that developed LAPACK and ScaLAPACK.</p>
          <p><a href="https://developer.nvidia.com/paralution">PARALUTION</a> is a third-party CUDA implementation of sparse iterative methods.</p>
          <p>More GPU-accelerated libraries can be found at <a href="https://developer.nvidia.com/gpu-accelerated-libraries">https://developer.nvidia.com/gpu-accelerated-libraries</a>.</p>
          <p>Refer to <a href="http://on-demand.gputechconf.com/gtc/2013/presentations/S3461-CUDA-Accelerated-Compute-Libraries.pdf">CUDA Accelerated Compute Libraries</a> and <a href="http://on-demand.gputechconf.com/gtc-express/2013/presentations/cuda--5.0-math-libraries-performance.pdf">CUDA 5 Math Library Performance Overview</a> for more detail.</p>
          <h2 id="thrust">thrust</h2>
          <p>This sample uses <a href="http://docs.nvidia.com/cuda/thrust/">thrust</a>, a CUDA implementation of STL (Standard Template Library), for vector reduction.</p>
<pre class="cpp">
#include &lt;iostream&gt;
#include &lt;thrust/host_vector.h&gt;
#include &lt;thrust/device_vector.h&gt;
#include &lt;thrust/generate.h&gt;
#include &lt;thrust/reduce.h&gt;
#include &lt;thrust/random.h&gt;

int my_rand()
{
	static thrust::default_random_engine rng;
	static thrust::uniform_int_distribution<int> dist(0, 999);
	return dist(rng);
}

int main(int argc, char* argv[])
{
	// Allocate a vector in host memory.
	thrust::host_vector<int> h(1e+7);

	// Generate random numbers on the host.
	thrust::generate(h.begin(), h.end(), my_rand);

	// Copy the vector from host memory to device memory.
	thrust::device_vector<int> d = h;
 
	// Compute the sum on the device.
	int actual = thrust::reduce(d.begin(), d.end(), 0, thrust::plus<int>());

	// Compute the sum on the host.
	int expected = 0;
	for (int i = 0; i < h.size(); ++i)
	{
		expected += h[i];
	}

	// Validate the result.
	if (actual != expected)
	{
		std::cout << "actual = " << actual << ", expected = " << expected << std::endl;
	}
}
</pre>
          <p><a href="http://www.accelereyes.com/products/arrayfire">ArrayFire</a> is a fast software library for GPU computing with an easy-to-use API. Refer to <a href="http://on-demand.gputechconf.com/gtc/2013/webinar/gtc-express-arrayfire-for-defense-intelligence-applications.pdf">ArrayFire: A Productive GPU Software Library for Defense and Intelligence Applications</a> for more detail.</p>
        </div>
      </div>
    </section>
    <section>
      <div class="page-header">
        <h1>Tools</h1>
      </div>
      <div class="row">
        <div class="col-md-12">
          <h2 id="nvcc">Compiler: nvcc</h2>
          <p>This figure shows the internal structure of the various CUDA compilation phases. It is reprinted from <a href="http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/graphics/cuda-compilation-from-cu-to-cu-cpp-ii.png">http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/graphics/cuda-compilation-from-cu-to-cu-cpp-ii.png</a>.</p>
          <p><img class="img-responsive" src="http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/graphics/cuda-compilation-from-cu-to-cu-cpp-ii.png" alt="nvcc"></p>
          <p>The <code>cudafe++</code> front end preprocessor will be called multiple times.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~></b> echo "int main() { int dummy; }" > test.cu
<b class="text-info">hjli@hpc1:~></b> nvcc -arch=sm_35 test.cu
test.cu(1): warning: variable "dummy" was declared but never referenced

test.cu(1): warning: variable "dummy" was declared but never referenced

</pre>
          <p>The <code>-ptx</code> option generates <a href="http://docs.nvidia.com/cuda/parallel-thread-execution/index.html">ptx</a>. The <code>-cubin</code> option generates cubin. The <code>-fatbin</code> option generates fatbin.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~/cudart/vectorAdd></b> nvcc -arch=sm_35 -ptx vectorAdd.cu
<b class="text-info">hjli@hpc1:~/cudart/vectorAdd></b> nvcc -arch=sm_35 -cubin vectorAdd.ptx
<b class="text-info">hjli@hpc1:~/cudart/vectorAdd></b> nvcc -arch=sm_35 -fatbin vectorAdd.cubin
<b class="text-info">hjli@hpc1:~/cudart/vectorAdd></b> ls -l vectorAdd.*
-rw-rw-r--. 1 hjli hjli   1893 Oct 18 03:17 vectorAdd.ptx
-rw-rw-r--. 1 hjli hjli   1868 Oct 18 03:18 vectorAdd.cubin
-rw-rw-r--. 1 hjli hjli   2680 Oct 18 03:20 vectorAdd.fatbin
</pre>
          <p><code>ptxas</code> assemblies ptx into object. The option <code>-v</code> prints code generation statistics.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~/cudart/vectorAdd></b> ptxas -arch=sm_35 -v vectorAdd.ptx
ptxas info    : 0 bytes gmem
ptxas info    : Compiling entry function '_Z9vectorAddPKfS0_Pfi' for 'sm_35'
ptxas info    : Function properties for _Z9vectorAddPKfS0_Pfi
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 10 registers, 348 bytes cmem[0]
</pre>
          <p>The <a href="http://docs.nvidia.com/cuda/cuda-driver-api/">CUDA Driver API</a> can JIT (Just-In-Time) compile ptx for forward compatibility. It is much analogue to <a href="http://www.khronos.org/opencl">OpenCL</a>. Refer to <a href="https://github.com/HongjianLi/cuda">https://github.com/HongjianLi/cuda</a> and <a href="https://github.com/HongjianLi/opencl">https://github.com/HongjianLi/opencl</a> for samples.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~></b> git clone https://github.com/HongjianLi/cuda.git
<b class="text-info">hjli@hpc1:~></b> git clone https://github.com/HongjianLi/opencl.git
</pre>
          <p>Refer to <a href="http://on-demand.gputechconf.com/gtc/2013/webinar/cuda-toolkit-as-build-tool.pdf">Introduction to the CUDA Toolkit as an Application Build Tool</a> and <a href="http://on-demand.gputechconf.com/gtc-express/2012/presentations/gpu-object-linking.pdf">GPU Object Linking: Usage and Benefits</a> for more detail.</p>
          <h2 id="cuda-gdb">Debugger: cuda-gdb & cuda-gdbserver</h2>
          <p>Compile an application with the options <code>-g</code> and <code>-G</code>. They embed debug information for the host code and the device code, respectively.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~/cudart/vectorAdd></b> nvcc -arch=sm_35 -g -G vectorAdd.cu
<b class="text-info">hjli@hpc1:~/cudart/vectorAdd></b> cuda-gdb ./vectorAdd
NVIDIA (R) CUDA Debugger
5.5 release
Portions Copyright (C) 2007-2013 NVIDIA Corporation
GNU gdb (GDB) 7.2
Copyright (C) 2010 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type "show copying"
and "show warranty" for details.
This GDB was configured as "x86_64-unknown-linux-gnu".
For bug reporting instructions, please see:
&lt;http://www.gnu.org/software/gdb/bugs/&gt;...
Reading symbols from /home/hjli/cudart/vectorAdd/vectorAdd...done.
(cuda-gdb) b vectorAdd
Breakpoint 1 at 0x402dd9: file vectorAdd.cu, line 4.
(cuda-gdb) r
Starting program: ~/cudart/vectorAdd/vectorAdd 
[Thread debugging using libthread_db enabled]
[New Thread 0x7ffff6ca4700 (LWP 54366)]
[Context Create of context 0x67f380 on Device 0]
[Launch of CUDA Kernel 0 (vectorAdd<<<(196,1,1),(256,1,1)>>>) on Device 0]
[Switching focus to CUDA kernel 0, grid 1, block (0,0,0), thread (0,0,0), device 0, sm 12, warp 0, lane 0]

Breakpoint 1, vectorAdd(const float * @generic, const float * @generic, float * @generic, int)<<<(196,1,1),(256,1,1)>>> (a=0x2300300000, b=0x2300330e00, 
    c=0x2300361c00, numElements=50000) at vectorAdd.cu:5
5		int i = blockDim.x * blockIdx.x + threadIdx.x;
(cuda-gdb) p numElements
$1 = 50000
(cuda-gdb) n
6		if (i < numElements)
(cuda-gdb) p i
$2 = 0
(cuda-gdb) cuda block 1 thread 3
[Switching focus to CUDA kernel 0, grid 1, block (1,0,0), thread (3,0,0), device 0, sm 11, warp 0, lane 3]
5		int i = blockDim.x * blockIdx.x + threadIdx.x;
(cuda-gdb) n
6		if (i < numElements)
(cuda-gdb) p i
$3 = 259
(cuda-gdb) n
8			c[i] = a[i] + b[i];
(cuda-gdb) n
10	}
(cuda-gdb) n
[Switching focus to CUDA kernel 0, grid 1, block (0,0,0), thread (0,0,0), device 0, sm 12, warp 0, lane 0]
6		if (i < numElements)
(cuda-gdb) help cuda
Print or select the CUDA focus.

List of cuda subcommands:

cuda block -- Print or select the current CUDA block
cuda device -- Print or select the current CUDA device
cuda grid -- Print or select the current CUDA grid
cuda kernel -- Print or select the current CUDA kernel
cuda lane -- Print or select the current CUDA lane
cuda sm -- Print or select the current CUDA SM
cuda thread -- Print or select the current CUDA thread
cuda warp -- Print or select the current CUDA warp

Type "help cuda" followed by cuda subcommand name for full documentation.
Type "apropos word" to search for commands related to "word".
Command name abbreviations are allowed if unambiguous.
(cuda-gdb) help info cuda
Print informations about the current CUDA activities. Available options:
         devices : information about all the devices
             sms : information about all the SMs in the current device
           warps : information about all the warps in the current SM
           lanes : information about all the lanes in the current warp
         kernels : information about all the active kernels
        contexts : information about all the contexts
          blocks : information about all the active blocks in the current kernel
         threads : information about all the active threads in the current kernel
    launch trace : information about the parent kernels of the kernel in focus
 launch children : information about the kernels launched by the kernels in focus

(cuda-gdb) help set cuda
Generic command for setting gdb cuda variables

List of set cuda subcommands:

set cuda api_failures -- Set the api_failures to ignore/stop/hide on CUDA driver API call errors
set cuda break_on_launch -- Automatically set a breakpoint at the entrance of kernels
set cuda coalescing -- Turn on/off coalescing of the CUDA commands output
set cuda context_events -- Turn on/off context events (push/pop/create/destroy) output messages
set cuda defer_kernel_launch_notifications -- Turn on/off deferral of kernel launch messages
set cuda disassemble_from -- Choose whether to disassemble from the device memory (slow) or the ELF image (fast)
set cuda gpu_busy_check -- Turn on/off GPU busy check the next time the inferior application is run
set cuda hide_internal_frame -- Set hiding of the internal CUDA frames when printing the call stack
set cuda kernel_events -- Turn on/off kernel events (launch/termination) output messages
set cuda launch_blocking -- Turn on/off CUDA kernel launch blocking (effective starting from the next run)
set cuda memcheck -- Turn on/off CUDA Memory Checker next time the inferior application is run
set cuda notify -- Thread to notify about CUDA events when no other known candidate
set cuda software_preemption -- Turn on/off CUDA software preemption debugging the next time the inferior application is run
set cuda thread_selection -- Set the automatic thread selection policy to use when the current thread cannot be selected

Type "help set cuda" followed by set cuda subcommand name for full 
documentation.
Type "apropos word" to search for commands related to "word".
Command name abbreviations are allowed if unambiguous.
(cuda-gdb) del 1
(cuda-gdb) c
Continuing.
[Termination of CUDA Kernel 0 (vectorAdd<<<(196,1,1),(256,1,1)>>>) on Device 0]
[Context Destroy of context 0x67f380 on Device 0]
[Thread 0x7ffff6ca4700 (LWP 54406) exited]

Program exited normally.
</pre>
          <p>Remote debugging is also possible.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~/cudart/vectorAdd></b> cuda-gdbserver :3001 ./vectorAdd
Process ./vectorAdd created; pid = 48918
Listening on port 3001
</pre>
<pre class="sh">
<b class="text-info">hjli@pc90124:~/cudart/vectorAdd></b> cuda-gdb ./vectorAdd
NVIDIA (R) CUDA Debugger
5.5 release
Portions Copyright (C) 2007-2013 NVIDIA Corporation
GNU gdb (GDB) 7.2
Copyright (C) 2010 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type "show copying"
and "show warranty" for details.
This GDB was configured as "x86_64-unknown-linux-gnu".
For bug reporting instructions, please see:
&lt;http://www.gnu.org/software/gdb/bugs/&gt;...
Reading symbols from /home/hjli/cudart/vectorAdd/vectorAdd...done.
(cuda-gdb) target remote tiger1:3001
</pre>
          <p>Refer to <a href="http://on-demand.gputechconf.com/gtc/2013/presentations/S3037-S3038-Debugging-CUDA-Apps-Linux-Mac.pdf">Debugging CUDA Applications on Linux and Mac</a> for more detail.</p>
          <h2 id="nvprof">Profilers: nvprof & nvvp</h2>
          <p><code>nvprof</code> is a command-line profiler. <code>nvvp</code> is a visual profiler.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~/cudart/asyncEngine></b> nvprof -o %h-%p.nvprof ./asyncEngine
<b class="text-info">hjli@hpc1:~/cudart/asyncEngine></b> nvprof -i hpc1-47304.nvprof --print-api-trace
<b class="text-info">hjli@hpc1:~/cudart/asyncEngine></b> nvprof -i hpc1-47304.nvprof --print-gpu-trace
======== Profiling result:
   Start  Duration            Grid Size      Block Size     Regs*    SSMem*    DSMem*      Size  Throughput           Device   Context    Stream  Name
     0ns  1.0068ms                    -               -         -         -         -  6.2915MB  6.2493GB/s   Tesla K20m (0)         1         8  [CUDA memcpy HtoD]
1.0097ms  1.0061ms                    -               -         -         -         -  6.2915MB  6.2530GB/s   Tesla K20m (0)         1         8  [CUDA memcpy HtoD]
2.3197ms  1.0087ms           (6144 1 1)       (256 1 1)        10        0B        0B         -           -   Tesla K20m (0)         1         8  vectorAdd(float const *, float const *, float*, int) [260]
3.1087ms  1.0721ms                    -               -         -         -         -  6.2915MB  5.8686GB/s   Tesla K20m (0)         1         9  [CUDA memcpy HtoD]
3.3364ms  1.1404ms                    -               -         -         -         -  6.2915MB  5.5170GB/s   Tesla K20m (0)         1         8  [CUDA memcpy DtoH]
4.1925ms  1.0492ms                    -               -         -         -         -  6.2915MB  5.9963GB/s   Tesla K20m (0)         1         9  [CUDA memcpy HtoD]
5.5256ms  1.0195ms           (6144 1 1)       (256 1 1)        10        0B        0B         -           -   Tesla K20m (0)         1         9  vectorAdd(float const *, float const *, float*, int) [272]
6.2483ms  1.0644ms                    -               -         -         -         -  6.2915MB  5.9109GB/s   Tesla K20m (0)         1        10  [CUDA memcpy HtoD]
6.5525ms  1.1311ms                    -               -         -         -         -  6.2915MB  5.5621GB/s   Tesla K20m (0)         1         9  [CUDA memcpy DtoH]
7.3243ms  1.0433ms                    -               -         -         -         -  6.2915MB  6.0304GB/s   Tesla K20m (0)         1        10  [CUDA memcpy HtoD]
8.6571ms  1.0206ms           (6144 1 1)       (256 1 1)        10        0B        0B         -           -   Tesla K20m (0)         1        10  vectorAdd(float const *, float const *, float*, int) [284]
9.4258ms  1.0687ms                    -               -         -         -         -  6.2915MB  5.8870GB/s   Tesla K20m (0)         1        11  [CUDA memcpy HtoD]
9.6892ms  1.1271ms                    -               -         -         -         -  6.2915MB  5.5821GB/s   Tesla K20m (0)         1        10  [CUDA memcpy DtoH]
10.500ms  1.0390ms                    -               -         -         -         -  6.2915MB  6.0553GB/s   Tesla K20m (0)         1        11  [CUDA memcpy HtoD]
11.825ms  1.0113ms           (6144 1 1)       (256 1 1)        10        0B        0B         -           -   Tesla K20m (0)         1        11  vectorAdd(float const *, float const *, float*, int) [296]
12.582ms  1.0635ms                    -               -         -         -         -  6.2915MB  5.9157GB/s   Tesla K20m (0)         1        12  [CUDA memcpy HtoD]
12.844ms  1.1336ms                    -               -         -         -         -  6.2915MB  5.5500GB/s   Tesla K20m (0)         1        11  [CUDA memcpy DtoH]
13.658ms  1.0523ms                    -               -         -         -         -  6.2915MB  5.9790GB/s   Tesla K20m (0)         1        12  [CUDA memcpy HtoD]
14.996ms  1.0189ms           (6144 1 1)       (256 1 1)        10        0B        0B         -           -   Tesla K20m (0)         1        12  vectorAdd(float const *, float const *, float*, int) [308]
15.768ms  1.0589ms                    -               -         -         -         -  6.2915MB  5.9413GB/s   Tesla K20m (0)         1        13  [CUDA memcpy HtoD]
16.025ms  1.1395ms                    -               -         -         -         -  6.2915MB  5.5215GB/s   Tesla K20m (0)         1        12  [CUDA memcpy DtoH]
16.832ms  1.0561ms                    -               -         -         -         -  6.2915MB  5.9575GB/s   Tesla K20m (0)         1        13  [CUDA memcpy HtoD]
18.188ms  1.0209ms           (6144 1 1)       (256 1 1)        10        0B        0B         -           -   Tesla K20m (0)         1        13  vectorAdd(float const *, float const *, float*, int) [320]
18.985ms  1.0678ms                    -               -         -         -         -  6.2915MB  5.8921GB/s   Tesla K20m (0)         1        14  [CUDA memcpy HtoD]
19.218ms  1.1262ms                    -               -         -         -         -  6.2915MB  5.5863GB/s   Tesla K20m (0)         1        13  [CUDA memcpy DtoH]
20.062ms  1.0418ms                    -               -         -         -         -  6.2915MB  6.0391GB/s   Tesla K20m (0)         1        14  [CUDA memcpy HtoD]
21.375ms  1.0101ms           (6144 1 1)       (256 1 1)        10        0B        0B         -           -   Tesla K20m (0)         1        14  vectorAdd(float const *, float const *, float*, int) [332]
22.029ms  1.1190ms                    -               -         -         -         -  6.2915MB  5.6225GB/s   Tesla K20m (0)         1        15  [CUDA memcpy HtoD]
22.393ms  1.1847ms                    -               -         -         -         -  6.2915MB  5.3106GB/s   Tesla K20m (0)         1        14  [CUDA memcpy DtoH]
23.154ms  1.0563ms                    -               -         -         -         -  6.2915MB  5.9560GB/s   Tesla K20m (0)         1        15  [CUDA memcpy HtoD]
24.470ms  1.0200ms           (6144 1 1)       (256 1 1)        10        0B        0B         -           -   Tesla K20m (0)         1        15  vectorAdd(float const *, float const *, float*, int) [344]
25.493ms  942.27us                    -               -         -         -         -  6.2915MB  6.6769GB/s   Tesla K20m (0)         1        15  [CUDA memcpy DtoH]

Regs: Number of registers used per CUDA thread.
SSMem: Static shared memory allocated per CUDA block.
DSMem: Dynamic shared memory allocated per CUDA block.
</pre>
          <p><code>nvvp</code> can import *.nvprof and visualize the timeline data. File -> Import -> Nvprof -> Timeline data file -> Finish. Zoom in the main part.</p>
          <p><img class="img-responsive" src="hpc1-47304.png" alt="nvprof"></p>
          <p>Refer to <a href="http://on-demand.gputechconf.com/gtc/2013/webinar/gtc-express-guided-analysis-nvidia-visual-profiler.pdf">Guided Performance Analysis with NVIDIA Visual Profiler</a>, <a href="http://on-demand.gputechconf.com/gtc/2012/presentations/S0419B-GTC2012-Profiling-Profiling-Tools.pdf">Optimizing Application Performance with CUDA Profiling Tools</a> and <a href="http://on-demand.gputechconf.com/gtc/2013/presentations/S3046-Performance-Optimization-Strategies-for-GPU-Accelerated-Apps.pdf">Performance Optimization Strategies For GPU-Accelerated Applications</a> for more detail.</p>
          <h2 id="nsight">IDE: Nsight</h2>
          <p><a href="https://developer.nvidia.com/nvidia-nsight-visual-studio-edition">Visual Studio Edition for Windows</a> and <a href="https://developer.nvidia.com/nsight-eclipse-edition">Eclipse Edition for Linux</a>.</p>
          <p>Refer to <a href="http://on-demand.gputechconf.com/gtc/2013/presentations/S3011-CUDA-Optimization-With-Nsight-VSE.pdf">CUDA Optimization with NVIDIA Nsight Visual Studio Edition 3.0</a>, <a href="http://on-demand.gputechconf.com/gtc/2013/presentations/S3478-Debugging-CUDA-Kernel-Code.pdf">Debugging CUDA Kernel Code with NVIDIA Nsight Visual Studio Edition</a> and <a href="http://on-demand.gputechconf.com/gtc-express/2012/presentations/nsight-eclipse-edition.pdf">Nsight Eclipse Edition: High Productivity IDE for CUDA Development on Linux & MacOS</a> for more detail.</a></p>
          <h2 id="CUDA_Occupancy_Calculator.xls">CUDA_Occupancy_Calculator.xls</h2>
          <p>It calculates the theoretical percentage for which kernels occupy a device. The higher the occupancy, the higher the GPU utilization. The Excel file is located in <code>/usr/local/cuda/tools</code>.</p>
          <h2 id="nvidia-smi">Monitor: nvidia-smi</h2>
          <p><a href="https://developer.nvidia.com/nvidia-system-management-interface">nvidia-smi</a> allows administrators to query GPU device state and with the appropriate privileges, permits administrators to modify GPU device state.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~></b> nvidia-smi
Tue Sep 10 10:39:25 2013       
+------------------------------------------------------+                       
| NVIDIA-SMI 5.319.37   Driver Version: 319.37         |                       
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K20m          Off  | 0000:08:00.0     Off |                    0 |
| N/A   35C    P0    42W / 225W |       11MB /  4799MB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla K20m          Off  | 0000:24:00.0     Off |                    0 |
| N/A   41C    P0    44W / 225W |       11MB /  4799MB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla K20m          Off  | 0000:27:00.0     Off |                    0 |
| N/A   35C    P0    39W / 225W |       11MB /  4799MB |     78%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Compute processes:                                               GPU Memory |
|  GPU       PID  Process name                                     Usage      |
|=============================================================================|
|  No running compute processes found                                         |
+-----------------------------------------------------------------------------+
</pre> 
          <p><a href="https://developer.nvidia.com/nvidia-management-library-nvml">NVML</a> is a C-based API for monitoring and managing various states of the NVIDIA GPU devices. It provides a direct access to the queries and commands exposed via <a href="https://developer.nvidia.com/nvidia-system-management-interface">nvidia-smi</a>. They both are parts of the <a href="https://developer.nvidia.com/tesla-deployment-kit">Tesla Deployment Kit</a>. Refer to <a href="http://on-demand.gputechconf.com/gtc/2013/presentations/S3044-GPUs-in-Cluster-Environments.pdf">Monitoring and Managing NVIDIA GPUs in Cluster Environments</a> and <a href="http://on-demand.gputechconf.com/gtc-express/2012/presentations/deploying-managing-gpu-clusters.pdf">Best Practices for Deploying and Managing GPU Clusters</a> for more detail.</p>
          <h2 id="cuda-memcheck">Sanitizer: cuda-memcheck</h2>
          <p><a href="http://docs.nvidia.com/cuda/cuda-memcheck/">CUDA-MEMCHECK</a> is a functional correctness checking suite. The demo code here suffers from unaligned access and out-of-bound access.</p>
<pre class="cpp">
#include &lt;stdio.h&gt;

__device__ int x;

__global__ void unaligned_kernel()
{
	*(int*) ((char*)&x + 1) = 42;
}

__device__ void out_of_bounds_function()
{
	*(int*) 0x87654320 = 42;
}

__global__ void out_of_bounds_kernel()
{
	out_of_bounds_function();
}

void run_unaligned(void)
{
	printf("Running unaligned_kernel\n");
	unaligned_kernel&lt;&lt;&lt;1,1&gt;&gt;&gt;();
	printf("Ran unaligned_kernel: %s\n",
	cudaGetErrorString(cudaGetLastError()));
	printf("Sync: %s\n", cudaGetErrorString(cudaThreadSynchronize()));
}

void run_out_of_bounds(void)
{
	printf("Running out_of_bounds_kernel\n");
	out_of_bounds_kernel&lt;&lt;&lt;1,1&gt;&gt;&gt;();
	printf("Ran out_of_bounds_kernel: %s\n",
	cudaGetErrorString(cudaGetLastError()));
	printf("Sync: %s\n", cudaGetErrorString(cudaThreadSynchronize()));
}

int main()
{
	int *devMem;
	printf("Mallocing memory\n");
	cudaMalloc((void**)&devMem, 1024);
	run_unaligned();
	run_out_of_bounds();
	cudaDeviceReset();
	cudaFree(devMem);
}
</pre>
          <p>Compile the demo code with option <code>-Xcompiler -rdynamic</code>, and invoke <code>cuda-memcheck</code>.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~/cudart/memcheckDemo></b> cuda-memcheck ./memcheckDemo
========= CUDA-MEMCHECK
Mallocing memory
Running unaligned_kernel
Ran unaligned_kernel: no error
========= Invalid __global__ write of size 4
=========     at 0x00000028 in unaligned_kernel(void)
=========     by thread (0,0,0) in block (0,0,0)
=========     Address 0x2300200001 is misaligned
=========     Saved host backtrace up to driver entry point at kernel launch time
=========     Host Frame:/usr/lib64/libcuda.so (cuLaunchKernel + 0x331) [0xcd5d1]
=========     Host Frame:./memcheckDemo [0x1bfb8]
=========     Host Frame:./memcheckDemo [0x3b483]
=========     Host Frame:./memcheckDemo (_ZN71_GLOBAL__N__47_tmpxft_0000c21b_00000000_6_memcheckDemo_cpp1_ii_a1aea87b10cudaLaunchIcEE9cudaErrorPT_ + 0x18) [0x325e]
=========     Host Frame:./memcheckDemo (_Z35__device_stub__Z16unaligned_kernelvv + 0x19) [0x30c3]
=========     Host Frame:./memcheckDemo (_Z16unaligned_kernelv + 0x9) [0x30ce]
=========     Host Frame:./memcheckDemo (_Z13run_unalignedv + 0x75) [0x2f22]
=========     Host Frame:./memcheckDemo (main + 0x28) [0x304f]
=========     Host Frame:/lib64/libc.so.6 (__libc_start_main + 0xfd) [0x1ecdd]
=========     Host Frame:./memcheckDemo [0x2c79]
=========
========= Program hit error 4 on CUDA API call to cudaThreadSynchronize 
=========     Saved host backtrace up to driver entry point at error
=========     Host Frame:/usr/lib64/libcuda.so [0x26d660]
=========     Host Frame:./memcheckDemo [0x39f46]
=========     Host Frame:./memcheckDemo (_Z13run_unalignedv + 0x98) [0x2f45]
=========     Host Frame:./memcheckDemo (main + 0x28) [0x304f]
=========     Host Frame:/lib64/libc.so.6 (__libc_start_main + 0xfd) [0x1ecdd]
=========     Host Frame:./memcheckDemo [0x2c79]
=========
Sync: unspecified launch failure
Running out_of_bounds_kernel
========= Program hit error 4 on CUDA API call to cudaLaunch 
=========     Saved host backtrace up to driver entry point at error
=========     Host Frame:/usr/lib64/libcuda.so [0x26d660]
=========     Host Frame:./memcheckDemo [0x3b4be]
=========     Host Frame:./memcheckDemo (_ZN71_GLOBAL__N__47_tmpxft_0000c21b_00000000_6_memcheckDemo_cpp1_ii_a1aea87b10cudaLaunchIcEE9cudaErrorPT_ + 0x18) [0x325e]
=========     Host Frame:./memcheckDemo (_Z39__device_stub__Z20out_of_bounds_kernelvv + 0x19) [0x30e9]
=========     Host Frame:./memcheckDemo (_Z20out_of_bounds_kernelv + 0x9) [0x30f4]
=========     Host Frame:./memcheckDemo (_Z17run_out_of_boundsv + 0x75) [0x2fdf]
=========     Host Frame:./memcheckDemo (main + 0x2d) [0x3054]
=========     Host Frame:/lib64/libc.so.6 (__libc_start_main + 0xfd) [0x1ecdd]
=========     Host Frame:./memcheckDemo [0x2c79]
=========
========= Program hit error 4 on CUDA API call to cudaGetLastError 
=========     Saved host backtrace up to driver entry point at error
Ran out_of_bounds_kernel: unspecified launch failure
=========     Host Frame:/usr/lib64/libcuda.so [0x26d660]
=========     Host Frame:./memcheckDemo [0x39723]
=========     Host Frame:./memcheckDemo (_Z17run_out_of_boundsv + 0x7a) [0x2fe4]
=========     Host Frame:./memcheckDemo (main + 0x2d) [0x3054]
=========     Host Frame:/lib64/libc.so.6 (__libc_start_main + 0xfd) [0x1ecdd]
=========     Host Frame:./memcheckDemo [0x2c79]
=========
Sync: unspecified launch failure
========= Program hit error 4 on CUDA API call to cudaThreadSynchronize 
=========     Saved host backtrace up to driver entry point at error
=========     Host Frame:/usr/lib64/libcuda.so [0x26d660]
=========     Host Frame:./memcheckDemo [0x39f46]
=========     Host Frame:./memcheckDemo (_Z17run_out_of_boundsv + 0x98) [0x3002]
=========     Host Frame:./memcheckDemo (main + 0x2d) [0x3054]
=========     Host Frame:/lib64/libc.so.6 (__libc_start_main + 0xfd) [0x1ecdd]
=========     Host Frame:./memcheckDemo [0x2c79]
=========
========= Program hit error 17 on CUDA API call to cudaFree 
=========     Saved host backtrace up to driver entry point at error
=========     Host Frame:/usr/lib64/libcuda.so [0x26d660]
=========     Host Frame:./memcheckDemo [0x43186]
=========     Host Frame:./memcheckDemo (main + 0x3e) [0x3065]
=========     Host Frame:/lib64/libc.so.6 (__libc_start_main + 0xfd) [0x1ecdd]
=========     Host Frame:./memcheckDemo [0x2c79]
=========
========= ERROR SUMMARY: 6 errors
</pre>
          <p>Specify the option <code>--leak-check full</code> to print information about the allocations that have not been freed at the time the CUDA context is destroyed.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~/cudart/memcheckDemo></b> cuda-memcheck --leak-check full ./memcheckDemo
</pre>
          <p>Specify the option <code>--tool racecheck</code> to help identify memory access race conditions in CUDA applications that use shared memory.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~/cudart/memcheckDemo></b> cuda-memcheck --tool racecheck ./memcheckDemo
</pre>
          <h2 id="nvdisasm">Disassemblers: cuobjdump & nvdisasm</h2>
          <p><a href="http://docs.nvidia.com/cuda/cuda-binary-utilities/index.html#cuobjdump">cuobjdump</a> extracts information from CUDA binary files (both standalone and those embedded in host binaries) and presents them in human readable format.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~/cudart/vectorAdd></b> cuobjdump ./vectorAdd.fatbin

Fatbin elf code:
================
arch = sm_35
code version = [1,7]
producer = cuda
host = linux
compile_size = 64bit
identifier = vectorAdd.cu

Fatbin ptx code:
================
arch = sm_35
code version = [3,2]
producer = cuda
host = linux
compile_size = 64bit
compressed
identifier = vectorAdd.cu
</pre>
          <p><a href="http://docs.nvidia.com/cuda/cuda-binary-utilities/index.html#nvdisasm">nvdisasm</a> extracts information from standalone cubin files and presents them in human readable format.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~/cudart/vectorAdd></b> nvdisasm ./vectorAdd.cubin
	.headerflags	@"EF_CUDA_TEXMODE_UNIFIED EF_CUDA_64BIT_ADDRESS EF_CUDA_SM35 EF_CUDA_PTX_SM(EF_CUDA_SM35) "


//--------------------- .nv.info                  --------------------------
	.section	.nv.info,"",@"SHT_CUDA_INFO "
	.align	4


	// ---- nvinfo : EIATTR_MIN_STACK_SIZE
	.align		4
	/*0000*/ 	.byte	0x04, 0x12
	/*0002*/ 	.short	(.L_4 - .L_3)
	.align		4
.L_3:
	/*0004*/ 	.word	index@(_Z9vectorAddPKfS0_Pfi)
	/*0008*/ 	.word	0x00000000


	// ---- nvinfo : EIATTR_FRAME_SIZE
	.align		4
.L_4:
	/*000c*/ 	.byte	0x04, 0x11
	/*000e*/ 	.short	(.L_6 - .L_5)
	.align		4
.L_5:
	/*0010*/ 	.word	index@(_Z9vectorAddPKfS0_Pfi)
	/*0014*/ 	.word	0x00000000
.L_6:


//--------------------- .nv.info._Z9vectorAddPKfS0_Pfi --------------------------
	.section	.nv.info._Z9vectorAddPKfS0_Pfi,"",@"SHT_CUDA_INFO "
	.align	4


	// ---- nvinfo : EIATTR_PARAM_CBANK
	.align		4
	/*0000*/ 	.byte	0x04, 0x0a
	/*0002*/ 	.short	(.L_8 - .L_7)
	.align		4
.L_7:
	/*0004*/ 	.word	index@(.nv.constant0._Z9vectorAddPKfS0_Pfi)
	/*0008*/ 	.short	0x0140
	/*000a*/ 	.short	0x001c


	// ---- nvinfo : EIATTR_CBANK_PARAM_SIZE
	.align		4
.L_8:
	/*000c*/ 	.byte	0x03, 0x19
	/*000e*/ 	.short	0x001c


	// ---- nvinfo : EIATTR_KPARAM_INFO
	.align		4
	/*0010*/ 	.byte	0x04, 0x17
	/*0012*/ 	.short	(.L_10 - .L_9)
.L_9:
	/*0014*/ 	.word	0x00000000
	/*0018*/ 	.short	0x0003
	/*001a*/ 	.short	0x0018
	/*001c*/ 	.byte	0x00, 0xf0, 0x11, 0x00


	// ---- nvinfo : EIATTR_KPARAM_INFO
	.align		4
.L_10:
	/*0020*/ 	.byte	0x04, 0x17
	/*0022*/ 	.short	(.L_12 - .L_11)
.L_11:
	/*0024*/ 	.word	0x00000000
	/*0028*/ 	.short	0x0002
	/*002a*/ 	.short	0x0010
	/*002c*/ 	.byte	0x00, 0xf0, 0x21, 0x00


	// ---- nvinfo : EIATTR_KPARAM_INFO
	.align		4
.L_12:
	/*0030*/ 	.byte	0x04, 0x17
	/*0032*/ 	.short	(.L_14 - .L_13)
.L_13:
	/*0034*/ 	.word	0x00000000
	/*0038*/ 	.short	0x0001
	/*003a*/ 	.short	0x0008
	/*003c*/ 	.byte	0x00, 0xf0, 0x21, 0x00


	// ---- nvinfo : EIATTR_KPARAM_INFO
	.align		4
.L_14:
	/*0040*/ 	.byte	0x04, 0x17
	/*0042*/ 	.short	(.L_16 - .L_15)
.L_15:
	/*0044*/ 	.word	0x00000000
	/*0048*/ 	.short	0x0000
	/*004a*/ 	.short	0x0000
	/*004c*/ 	.byte	0x00, 0xf0, 0x21, 0x00


	// ---- nvinfo : EIATTR_CRS_STACK_SIZE
	.align		4
.L_16:
	/*0050*/ 	.byte	0x04, 0x1e
	/*0052*/ 	.short	(.L_18 - .L_17)
.L_17:
	/*0054*/ 	.word	0x00000010


	// ---- nvinfo : EIATTR_S2RCTAID_INSTR_OFFSETS
	.align		4
.L_18:
	/*0058*/ 	.byte	0x04, 0x1d
	/*005a*/ 	.short	(.L_20 - .L_19)


	//   ....[0]....
.L_19:
	/*005c*/ 	.word	0x00000010


	// ---- nvinfo : EIATTR_EXIT_INSTR_OFFSETS
	.align		4
.L_20:
	/*0060*/ 	.byte	0x04, 0x1c
	/*0062*/ 	.short	(.L_22 - .L_21)


	//   ....[0]....
.L_21:
	/*0064*/ 	.word	0x000000a8
.L_22:


//--------------------- .nv.constant0._Z9vectorAddPKfS0_Pfi --------------------------
	.section	.nv.constant0._Z9vectorAddPKfS0_Pfi,"a",@progbits
	.align	4
.nv.constant0._Z9vectorAddPKfS0_Pfi:
	.zero		348


//--------------------- .text._Z9vectorAddPKfS0_Pfi --------------------------
	.section	.text._Z9vectorAddPKfS0_Pfi,"ax",@progbits
	.sectioninfo	@"SHI_REGISTERS=10 "
	.align	4
	.global		_Z9vectorAddPKfS0_Pfi
	.type		_Z9vectorAddPKfS0_Pfi,@function
	.size		_Z9vectorAddPKfS0_Pfi,(.L_23 - _Z9vectorAddPKfS0_Pfi)
	.other		_Z9vectorAddPKfS0_Pfi,@"STO_CUDA_ENTRY STV_DEFAULT "
_Z9vectorAddPKfS0_Pfi:
.text._Z9vectorAddPKfS0_Pfi:
        /*0008*/                MOV R1, c[0x0][0x44];
        /*0010*/                S2R R0, SR_CTAID.X;
        /*0018*/                S2R R3, SR_TID.X;
        /*0020*/                IMAD R0, R0, c[0x0][0x28], R3;
        /*0028*/                ISETP.GE.AND P0, PT, R0, c[0x0][0x158], PT;
        /*0030*/            @P0 BRA.U `(.L_1);
        /*0038*/           @!P0 MOV32I R5, 0x4;
        /*0048*/           @!P0 IMAD R6.CC, R0, R5, c[0x0][0x140];
        /*0050*/           @!P0 IMAD.HI.X R7, R0, R5, c[0x0][0x144];
        /*0058*/           @!P0 IMAD R8.CC, R0, R5, c[0x0][0x148];
        /*0060*/           @!P0 LD.E R3, [R6];
        /*0068*/           @!P0 IMAD.HI.X R9, R0, R5, c[0x0][0x14c];
        /*0070*/           @!P0 LD.E R2, [R8];
        /*0078*/           @!P0 IMAD R4.CC, R0, R5, c[0x0][0x150];
        /*0088*/           @!P0 IMAD.HI.X R5, R0, R5, c[0x0][0x154];
        /*0090*/           @!P0 FADD R0, R3, R2;
        /*0098*/           @!P0 ST.E [R4], R0;
.L_1:
        /*00a0*/                MOV RZ, RZ;
        /*00a8*/                EXIT ;
.L_2:
        /*00b0*/                BRA `(.L_2);
.L_23:
</pre>
          <p>Generate a PNG image of the control flow of vectorAdd.cubin with <a href="http://docs.nvidia.com/cuda/cuda-binary-utilities/index.html#nvdisasm">nvdisasm</a> and <a href="http://www.graphviz.org">Graphviz</a>.</p>
<pre class="sh">
<b class="text-info">hjli@hpc1:~/cudart/vectorAdd></b> nvdisasm -cfg ./vectorAdd.cubin | dot -Tpng -ovectorAdd.png
</pre>
          <p><img class="img-responsive" src="vectorAdd.png" alt="vectorAdd.png"></p>
        </div>
      </div>
    </section>
  </div>
  <footer role="contentinfo">
    <div class="container">
      <p><a href="http://www.cuhk.edu.hk"><img src="/cuhk.jpg" alt="CUHK logo"></a>&copy; 2012-2013 Chinese University of Hong Kong. Platform designed by <a href="http://www.cse.cuhk.edu.hk/~hjli">Hongjian Li</a>. Code licensed under <a href="http://www.apache.org/licenses/LICENSE-2.0">Apache License 2.0</a>. Documentation licensed under <a href="http://creativecommons.org/licenses/by/3.0">CC BY 3.0</a>.<a href="http://validator.w3.org/check?uri=referer"><img src="/HTML5_Badge_512.png" alt="HTML5 logo"></a></p>
    </div>
  </footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
  <script src="/bootstrap.min.js"></script>
  <script src="/jquery.lazyload.min.js"></script>
  <script src="/jquery.snippet.min.js"></script>
  <script src="/sh_sh.min.js"></script>
  <script src="sh_makefile.min.js"></script>
  <script src="index.js"></script>
</body>
</html>
